# app.py

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
from datetime import datetime, timedelta
import time
import asyncio
import openai
import re
from opencc import OpenCC
from googleapiclient.discovery import build

# =========================
# 0. å·¥å…·å‡½æ•¸ï¼šé—œéµå­—ã€èªè¨€æª¢æ¸¬
# =========================

def generate_search_queries(movie_title: str):
    """
    ç”Ÿæˆæ›´å¯¬é¬†ä¸”å¤šæ¨£åŒ–çš„é—œéµå­—çµ„åˆï¼š
    - å»æ‰éæ–¼åš´æ ¼çš„å¼•è™Ÿé™åˆ¶
    - ä¸­è‹±æ··åˆï¼Œæ¶µè“‹ã€Œå½±è©•/è©•è«–/è©•åƒ¹/è§£æ/åˆ†æ/ç„¡é›·/æœ‰é›·/é å‘Š/èŠ±çµ®/åæ‡‰/review/reaction/ending explainedã€
    - ä»ä¿ç•™å°‘é‡å¸¶å¼•è™Ÿçš„ç²¾ç¢ºåŒ¹é…ï¼Œä½œç‚ºè£œå……
    """
    zh_terms = [
        "å½±è©•", "è©•è«–", "è©•åƒ¹", "é»è©•", "è§£æ", "åˆ†æ", "è§€å¾Œæ„Ÿ",
        "ç„¡é›·", "æœ‰é›·", "è¨è«–", "å¥½å””å¥½ç‡", "é å‘Š", "èŠ±çµ®", "ç‰‡æ®µ", "é¦–æ˜ ", "å¹•å¾Œ"
    ]
    en_terms = [
        "review", "reaction", "ending explained", "analysis", "explained",
        "behind the scenes", "bts", "premiere", "interview", "press conference"
    ]

    # å¯¬é¬†ï¼ˆç„¡å¼•è™Ÿï¼‰
    loose = [f"{movie_title}"]
    loose += [f"{movie_title} {t}" for t in zh_terms]
    loose += [f"{movie_title} {t}" for t in en_terms]

    # å°‘é‡ç²¾ç¢ºï¼ˆå¸¶å¼•è™Ÿï¼‰
    tight = [
        f"\"{movie_title}\"",
        f"\"{movie_title}\" å½±è©•",
        f"\"{movie_title}\" è©•è«–",
        f"\"{movie_title}\" è§£æ",
        f"\"{movie_title}\" review",
        f"\"{movie_title}\" reaction",
    ]

    # å»é‡ä¿åº
    seen = set()
    queries = []
    for q in loose + tight:
        if q not in seen:
            queries.append(q)
            seen.add(q)
    return queries


def count_chars(text: str):
    """
    è¨ˆç®—å„é¡å­—ç¬¦æ•¸é‡ï¼šä¸­æ—¥éŸ“æ¼¢å­—ã€å‡åã€æ‹‰ä¸ã€æ•¸å­—ç­‰
    """
    counts = {
        "cjk": 0,
        "hiragana": 0,
        "katakana": 0,
        "half_katakana": 0,
        "hangul": 0,
        "latin": 0,
        "digits": 0,
        "other": 0
    }
    for ch in text:
        code = ord(ch)
        if 0x4E00 <= code <= 0x9FFF or 0x3400 <= code <= 0x4DBF or 0xF900 <= code <= 0xFAFF:
            counts["cjk"] += 1
        elif 0x3040 <= code <= 0x309F:
            counts["hiragana"] += 1
        elif 0x30A0 <= code <= 0x30FF or 0x31F0 <= code <= 0x31FF:
            counts["katakana"] += 1
        elif 0xFF65 <= code <= 0xFF9F:
            counts["half_katakana"] += 1
        elif 0xAC00 <= code <= 0xD7AF:
            counts["hangul"] += 1
        elif (0x0041 <= code <= 0x005A) or (0x0061 <= code <= 0x007A):
            counts["latin"] += 1
        elif 0x0030 <= code <= 0x0039:
            counts["digits"] += 1
        else:
            counts["other"] += 1
    return counts


def diff_chars(a: str, b: str) -> int:
    """
    ä¼°ç®—å­—ç¬¦ç´šå·®ç•°æ•¸ï¼šzip å°é½Šå¾Œä¸ç­‰ + é•·åº¦å·®
    """
    m = min(len(a), len(b))
    base = sum(1 for i in range(m) if a[i] != b[i])
    return base + abs(len(a) - len(b))


def classify_zh_trad_simp(text: str, cc_t2s: OpenCC, cc_s2t: OpenCC):
    """
    ç°¡å–®çš„èªè¨€/æ›¸å¯«ç³»çµ±åˆ†é¡ï¼š
    - jaï¼šå«æœ‰è¼ƒé«˜æ¯”ä¾‹çš„å‡åï¼ˆå¹³/ç‰‡/åŠè§’ç‰‡å‡åï¼‰
    - zh-Hantï¼št2s è®ŠåŒ–é¡¯è‘—è€Œ s2t è®ŠåŒ–å¾ˆå°ï¼ˆåŸæ–‡æ›´æ¥è¿‘ç¹é«”ï¼‰
    - zh-Hansï¼šç›¸åï¼ˆåŸæ–‡æ›´æ¥è¿‘ç°¡é«”ï¼‰
    - zh-unknï¼šä¸­æ–‡ä½†é›£ä»¥å€åˆ†ï¼ˆå¤šç‚ºå…¬å…±æ¼¢å­—+æ¨™é»ï¼‰
    - otherï¼šåŸºæœ¬æ²’æœ‰ CJK
    """
    if not isinstance(text, str) or len(text.strip()) < 2:
        return "other"

    counts = count_chars(text)
    kana = counts["hiragana"] + counts["katakana"] + counts["half_katakana"]
    cjk = counts["cjk"]

    # æ—¥æ–‡å‰”é™¤ï¼šå‡åæ•¸ >= 2 ä¸”ç›¸å°å æ¯” > 10% è¦–ç‚ºæ—¥æ–‡
    if kana >= 2 and kana / max(1, (cjk + kana)) >= 0.10:
        return "ja"

    if cjk < 1:
        return "other"

    t2s = cc_t2s.convert(text)  # ç¹->ç°¡
    s2t = cc_s2t.convert(text)  # ç°¡->ç¹
    ct2s = diff_chars(text, t2s)
    cs2t = diff_chars(text, s2t)

    threshold = max(1, int(0.05 * cjk))  # cjk çš„ 5% ä½œç‚ºå·®ç•°é–¾å€¼

    if ct2s > cs2t + threshold:
        return "zh-Hant"
    elif cs2t > ct2s + threshold:
        return "zh-Hans"
    else:
        return "zh-unkn"


# =========================
# 1. YouTube æœå°‹ï¼ˆå¼·åŒ– + åˆ†é ï¼‰
# =========================

def search_youtube_videos(
    keywords,
    youtube_client,
    max_per_keyword,
    start_date,
    end_date,
    add_language_bias=True
):
    """
    - å°æ¯å€‹é—œéµå­—ç”¨ order=relevance èˆ‡ order=viewCount å…©ç¨®æ’åºæŠ“å–
    - åˆ†é ç›´åˆ°é”åˆ° max_per_keyword æˆ–ç„¡æ›´å¤šçµæœ
    - è¿”å›ï¼š
      - video_ids: å»é‡å¾Œçš„æ‰€æœ‰è¦–é » ID åˆ—è¡¨
      - video_meta: {video_id: {"title": ..., "channelTitle": ..., "publishedAt": ...}}
    """
    all_video_ids = set()
    video_meta = {}

    for query in keywords:
        collected_for_query = set()
        for order in ["relevance", "viewCount"]:
            try:
                request = youtube_client.search().list(
                    q=query,
                    part="id,snippet",
                    type="video",
                    maxResults=50,  # API ä¸Šé™
                    publishedAfter=f"{start_date}T00:00:00Z",
                    publishedBefore=f"{end_date}T23:59:59Z",
                    order=order,
                    safeSearch="none",
                    **({"relevanceLanguage": "zh-Hant"} if add_language_bias else {})
                )
                while request and len(collected_for_query) < max_per_keyword:
                    response = request.execute()
                    for item in response.get("items", []):
                        vid = item["id"]["videoId"]
                        if vid in collected_for_query:
                            continue
                        collected_for_query.add(vid)
                        all_video_ids.add(vid)
                        # ä¿ç•™ä¸€ä»½åŸºæœ¬å…ƒæ•¸æ“šï¼ˆæ¨™é¡Œ/é »é“/æ™‚é–“ï¼‰
                        if vid not in video_meta:
                            snip = item.get("snippet", {})
                            video_meta[vid] = {
                                "title": snip.get("title", ""),
                                "channelTitle": snip.get("channelTitle", ""),
                                "publishedAt": snip.get("publishedAt", "")
                            }
                    # ç¿»é 
                    request = youtube_client.search().list_next(request, response)
                    if len(collected_for_query) >= max_per_keyword:
                        break
                    time.sleep(0.2)  # æº«å’Œé™æµ
            except Exception as e:
                st.warning(f"æœå°‹é—œéµå­— '{query}'ï¼ˆorder={order}ï¼‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                continue
    return list(all_video_ids), video_meta


# =========================
# 2. æ‰¹é‡æŠ“å–ç•™è¨€ï¼ˆè£œå……ä¾†æºä¿¡æ¯ï¼‰
# =========================

def get_all_comments(video_ids, youtube_client, max_per_video, video_meta=None):
    """
    æŠ“å–æ¯å€‹è¦–é »çš„é ‚å±¤è©•è«–ï¼ˆcommentThreadsï¼‰ï¼Œç›´åˆ°é”åˆ°æ¯è¦–é »ä¸Šé™ã€‚
    ç‚ºæ¯æ¢è©•è«–æ·»åŠ ä¾†æºè¦–é »çš„æ¨™é¡Œèˆ‡è¶…éˆæ¥ã€‚
    """
    video_meta = video_meta or {}
    all_comments = []
    total_videos = len(video_ids)
    progress_bar = st.progress(0, text="æŠ“å– YouTube ç•™è¨€ä¸­...")

    for i, video_id in enumerate(video_ids):
        try:
            request = youtube_client.commentThreads().list(
                part="snippet",
                videoId=video_id,
                textFormat="plainText",
                order="time",
                maxResults=100
            )
            comments_fetched = 0
            while request and comments_fetched < max_per_video:
                response = request.execute()
                for item in response.get("items", []):
                    if comments_fetched >= max_per_video:
                        break
                    comment = item["snippet"]["topLevelComment"]["snippet"]
                    all_comments.append({
                        "video_id": video_id,
                        "video_title": video_meta.get(video_id, {}).get("title", ""),
                        "video_url": f"https://www.youtube.com/watch?v={video_id}",
                        "comment_text": comment.get("textDisplay", ""),
                        "published_at": comment.get("publishedAt", ""),
                        "like_count": comment.get("likeCount", 0)
                    })
                    comments_fetched += 1
                if comments_fetched >= max_per_video:
                    break
                request = youtube_client.commentThreads().list_next(request, response)
                time.sleep(0.2)
        except Exception:
            # æœ‰äº›è¦–é »å¯èƒ½é—œé–‰äº†è©•è«–æˆ–è¢«é™æ¬Š
            pass
        finally:
            progress_bar.progress(
                (i + 1) / max(1, total_videos),
                text=f"æŠ“å– YouTube ç•™è¨€ä¸­... ({i+1}/{total_videos} éƒ¨å½±ç‰‡)"
            )
    progress_bar.empty()
    return pd.DataFrame(all_comments)


# =========================
# 3. DeepSeek AI ç•°æ­¥æƒ…æ„Ÿåˆ†æï¼ˆé †åºå°é½Šï¼‰
# =========================

async def analyze_comment_deepseek_async(comment_text, deepseek_client, semaphore, max_retries=3):
    import json
    if not isinstance(comment_text, str) or len(comment_text.strip()) < 5:
        return {"sentiment": "Invalid", "topic": "N/A", "summary": "Comment too short or invalid."}

    system_prompt = (
        "You are a professional Hong Kong market sentiment analyst. "
        "Analyze the following movie comment and strictly return the result in JSON format. "
        "The JSON object must contain three keys: "
        "1. 'sentiment': Must be either 'Positive', 'Negative', or 'Neutral'. "
        "2. 'topic': The core topic of the comment, e.g., 'Plot', 'Acting', 'Action Design', "
        "'Visuals', 'Pace', or 'Overall'. If unable to determine, use 'N/A'. "
        "3. 'summary': A concise one-sentence summary of the comment's main point. "
        "Ensure the output is only the JSON object and nothing else."
    )

    async with semaphore:
        for attempt in range(max_retries):
            try:
                response = await deepseek_client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": comment_text}
                    ],
                    response_format={"type": "json_object"},
                    temperature=0.1,
                )
                data = response.choices[0].message.content
                analysis_result = json.loads(data)
                return analysis_result
            except Exception as e:
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                else:
                    return {"sentiment": "Error", "topic": "Error", "summary": f"API Error: {e}"}


async def run_all_analyses(df, deepseek_client):
    """
    as_completed + é€²åº¦æ¢ï¼Œä½†ç”¨ç´¢å¼•å›å¡«ï¼Œç¢ºä¿è¼¸å‡ºèˆ‡è¼¸å…¥é †åºä¸€ä¸€å°é½Š
    """
    semaphore = asyncio.Semaphore(50)
    tasks = []

    async def with_index(idx, text):
        res = await analyze_comment_deepseek_async(text, deepseek_client, semaphore)
        return idx, res

    for i, text in enumerate(df["comment_text"]):
        tasks.append(asyncio.create_task(with_index(i, text)))

    progress_bar = st.progress(0, text="AI æƒ…æ„Ÿåˆ†æä¸­...")
    results = [None] * len(tasks)
    for done_idx, coro in enumerate(asyncio.as_completed(tasks), start=1):
        idx, res = await coro
        results[idx] = res
        progress_bar.progress(done_idx / len(tasks), text=f"AI æƒ…æ„Ÿåˆ†æä¸­... ({done_idx}/{len(tasks)})")
    progress_bar.empty()
    return results


# =========================
# 4. ä¸»æµç¨‹ï¼ˆå¢å¼·ï¼šèªè¨€éæ¿¾ã€ä¾†æºå­—æ®µã€æœå°‹åŠ å¯¬ï¼‰
# =========================

def movie_comment_analysis(
    movie_title, start_date, end_date,
    yt_api_key, deepseek_api_key,
    max_videos_per_keyword=30, max_comments_per_video=50, sample_size=None,
    relax_trad_filter=True
):
    # é—œéµå­—ç”Ÿæˆï¼šå¯¬é¬†+ç²¾ç¢ºæ··åˆ
    SEARCH_KEYWORDS = generate_search_queries(movie_title)

    youtube_client = build("youtube", "v3", developerKey=yt_api_key)
    deepseek_client = openai.AsyncOpenAI(
        api_key=deepseek_api_key,
        base_url="https://api.deepseek.com/v1"
    )

    # æœå°‹ï¼ˆåˆ†é  + å…ƒæ•¸æ“šï¼‰
    video_ids, video_meta = search_youtube_videos(
        SEARCH_KEYWORDS, youtube_client, max_videos_per_keyword, start_date, end_date, add_language_bias=True
    )
    if not video_ids:
        return None, "æ‰¾ä¸åˆ°ç›¸é—œå½±ç‰‡ã€‚"

    # ç•™è¨€æŠ“å–ï¼ˆè£œè¦–é »æ¨™é¡Œèˆ‡éˆæ¥ï¼‰
    df_comments = get_all_comments(video_ids, youtube_client, max_comments_per_video, video_meta=video_meta)
    if df_comments.empty:
        return None, "æ‰¾ä¸åˆ°ä»»ä½•ç•™è¨€ã€‚"

    # èªè¨€éæ¿¾ï¼šé¡¯å¼å‰”é™¤æ—¥æ–‡ï¼Œä¿ç•™ç¹é«”ï¼ˆå¯é¸ä¿ç•™ã€Œç–‘ä¼¼ä¸­æ–‡ã€ï¼‰
    st.info(f"å·²æŠ“å– {len(df_comments)} å‰‡åŸå§‹ç•™è¨€ï¼Œç¾é–‹å§‹èªè¨€ç¯©é¸ï¼ˆç¹é«” + å‰”é™¤æ—¥æ–‡ï¼‰...")

    cc_t2s = OpenCC("t2s")  # ç¹->ç°¡
    cc_s2t = OpenCC("s2t")  # ç°¡->ç¹

    def lang_pred(text):
        return classify_zh_trad_simp(text, cc_t2s, cc_s2t)

    df_comments["lang_pred"] = df_comments["comment_text"].apply(lang_pred)

    if relax_trad_filter:
        # æ”¾å¯¬ï¼šä¿ç•™ zh-Hant + zh-unknï¼ˆç–‘ä¼¼ä¸­æ–‡ä½†é›£åˆ¤ç¹/ç°¡ï¼‰ï¼Œå‰”é™¤ ja/other/zh-Hans
        df_comments_filtered = df_comments[df_comments["lang_pred"].isin(["zh-Hant", "zh-unkn"])].reset_index(drop=True)
    else:
        # åš´æ ¼ï¼šåªä¿ç•™ zh-Hant
        df_comments_filtered = df_comments[df_comments["lang_pred"] == "zh-Hant"].reset_index(drop=True)

    # é¡¯å¼å‰”é™¤æ—¥æ–‡
    df_comments_filtered = df_comments_filtered[df_comments_filtered["lang_pred"] != "ja"]

    st.info(f"ç¯©é¸å¾Œå‰©ä¸‹ {len(df_comments_filtered)} å‰‡ç¬¦åˆæ¢ä»¶çš„ç•™è¨€ã€‚")
    if df_comments_filtered.empty:
        return None, "åœ¨æŠ“å–çš„ç•™è¨€ä¸­æ‰¾ä¸åˆ°ç¬¦åˆèªè¨€æ¢ä»¶çš„å…§å®¹ã€‚"

    # æ™‚å€è™•ç†èˆ‡æ—¥æœŸç¯„åœäºŒæ¬¡æ ¡é©—
    df_comments_filtered["published_at"] = pd.to_datetime(df_comments_filtered["published_at"], utc=True, errors="coerce")
    df_comments_filtered["published_at_hk"] = df_comments_filtered["published_at"].dt.tz_convert("Asia/Hong_Kong")

    start_dt = pd.to_datetime(start_date).tz_localize("Asia/Hong_Kong")
    end_dt = pd.to_datetime(end_date).tz_localize("Asia/Hong_Kong") + timedelta(days=1)
    mask_date = (df_comments_filtered["published_at_hk"] >= start_dt) & (df_comments_filtered["published_at_hk"] < end_dt)
    df_comments_filtered = df_comments_filtered.loc[mask_date].reset_index(drop=True)
    if df_comments_filtered.empty:
        return None, "åœ¨æŒ‡å®šæ—¥æœŸç¯„åœå…§æ²’æœ‰ç¬¦åˆèªè¨€æ¢ä»¶çš„ç•™è¨€ã€‚"

    # å–æ¨£æ§åˆ¶
    if sample_size and 0 < sample_size < len(df_comments_filtered):
        df_analyze = df_comments_filtered.sample(n=sample_size, random_state=42)
    else:
        df_analyze = df_comments_filtered

    st.info(f"æº–å‚™å° {len(df_analyze)} å‰‡ç•™è¨€é€²è¡Œé«˜é€Ÿä¸¦ç™¼åˆ†æ...")

    # ç•°æ­¥åˆ†æä¸¦å°é½Š
    analysis_results = asyncio.run(run_all_analyses(df_analyze, deepseek_client))
    analysis_df = pd.DataFrame(analysis_results)

    final_df = pd.concat([df_analyze.reset_index(drop=True), analysis_df], axis=1)
    final_df["published_at"] = pd.to_datetime(final_df["published_at"])

    return final_df, None


# =========================
# 5. Streamlit UI
# =========================

st.set_page_config(page_title="YouTube é›»å½±è©•è«– AI åˆ†æ", layout="wide")
st.title("ğŸ¬ YouTube é›»å½±è©•è«– AI æƒ…æ„Ÿåˆ†æ")

with st.expander("ä½¿ç”¨èªªæ˜"):
    st.markdown("""
    1.  è¼¸å…¥é›»å½±çš„ä¸­æ–‡å…¨åã€åˆ†ææ™‚é–“ç¯„åœåŠæ‰€éœ€çš„ API é‡‘é‘°ã€‚
    2.  è‡ªè¨‚æ¯å€‹é—œéµå­—æœå°‹çš„å½±ç‰‡æ•¸é‡ä¸Šé™ï¼ŒåŠæ¯éƒ¨å½±ç‰‡æŠ“å–çš„ç•™è¨€æ•¸é‡ä¸Šé™ã€‚
    3.  ç³»çµ±å°‡è‡ªå‹•æŠ“å– YouTube ç•™è¨€ï¼Œå‰”é™¤æ—¥æ–‡ï¼Œä¸¦ä»¥ç¹é«”ç‚ºä¸»è¦ç›®æ¨™èªè¨€é€²è¡Œ AI æƒ…æ„Ÿåˆ†æã€‚
        ä½ å¯é¸æ“‡æ˜¯å¦ã€Œæ”¾å¯¬ç¹é«”åˆ¤å®šã€ï¼Œä»¥å¢åŠ æ¨£æœ¬é‡ã€‚
    4.  åˆ†æå®Œæˆå¾Œï¼Œä¸‹æ–¹æœƒé¡¯ç¤ºæ•¸æ“šåœ–è¡¨åŠè©³ç´°çµæœçš„ä¸‹è¼‰æŒ‰éˆ•ã€‚
    """)

movie_title = st.text_input("é›»å½±åç¨± (å»ºè­°ä½¿ç”¨é¦™æ¸¯é€šç”¨çš„ä¸­æ–‡å…¨å)", value="ä¹é¾åŸå¯¨ä¹‹åœåŸ")
col1, col2 = st.columns(2)
with col1:
    start_date = st.date_input("é–‹å§‹æ—¥æœŸ", value=datetime.today() - timedelta(days=30))
with col2:
    end_date = st.date_input("çµæŸæ—¥æœŸ", value=datetime.today())
yt_api_key = st.text_input("YouTube API Key", type='password')
deepseek_api_key = st.text_input("DeepSeek API Key", type='password')

st.subheader("é€²éšè¨­å®š")
max_videos = st.slider("æ¯å€‹é—œéµå­—çš„æœ€å¤§å½±ç‰‡æœå°‹æ•¸", 5, 80, 30, help="å¢åŠ æ­¤æ•¸å€¼æœƒæ‰¾åˆ°æ›´å¤šå½±ç‰‡ï¼Œä½†æœƒå¢åŠ  YouTube API çš„é…é¡æ¶ˆè€—ã€‚")
max_comments = st.slider("æ¯éƒ¨å½±ç‰‡çš„æœ€å¤§ç•™è¨€æŠ“å–æ•¸", 10, 200, 80, help="æ•¸é‡è¶Šå¤šï¼Œåˆ†æçµæœè¶Šå…¨é¢ï¼Œä½† DeepSeek API æˆæœ¬è¶Šé«˜ã€‚")
sample_size = st.number_input("åˆ†æç•™è¨€æ•¸é‡ä¸Šé™ (0 ä»£è¡¨åˆ†æå…¨éƒ¨å·²æŠ“å–çš„ç•™è¨€)", 0, 5000, 500, help="ä¾‹å¦‚æŠ“å–äº† 2000 å‰‡ç•™è¨€ï¼Œé€™è£¡è¨­ 500 å°±åªæœƒåˆ†æå…¶ä¸­çš„ 500 å‰‡ã€‚")
relax_trad_filter = st.checkbox("æ”¾å¯¬ç¹é«”åˆ¤å®šï¼ˆå…è¨±ç–‘ä¼¼ä¸­æ–‡ä½†ç„¡æ³•åˆ¤åˆ¥ç¹ï¼ç°¡çš„ç•™è¨€ï¼‰", value=True)

if st.button("ğŸš€ é–‹å§‹åˆ†æ"):
    if not all([movie_title, yt_api_key, deepseek_api_key]):
        st.warning("è«‹å¡«å¯«é›»å½±åç¨±å’Œå…©å€‹ API é‡‘é‘°ã€‚")
    else:
        result_container = st.container()
        with st.spinner("AI é«˜é€Ÿåˆ†æä¸­... è«‹ç¨å€™..."):
            df_result, err = movie_comment_analysis(
                movie_title, str(start_date), str(end_date),
                yt_api_key, deepseek_api_key,
                max_videos, max_comments, sample_size,
                relax_trad_filter=relax_trad_filter
            )

        if err:
            st.error(err)
        else:
            st.success("åˆ†æå®Œæˆï¼")
            st.dataframe(df_result.head(20), use_container_width=True)

            st.header("ğŸ“Š å¯è¦–åŒ–åˆ†æçµæœ")

            # å…±ç”¨è¨­å®š
            sentiments_order = ['Positive', 'Negative', 'Neutral', 'Invalid', 'Error']
            colors_map = {
                'Positive': '#5cb85c', 'Negative': '#d9534f', 'Neutral': '#f0ad4e',
                'Invalid': '#cccccc', 'Error': '#888888'
            }

            # 1. æƒ…æ„Ÿåˆ†ä½ˆåœ“é¤…åœ–
            st.subheader("1. Sentiment Distribution (Pie)")
            sentiment_series = df_result['sentiment'].dropna().astype(str)
            sentiment_counts = sentiment_series.value_counts()
            ordered_labels = [label for label in sentiments_order if label in sentiment_counts.index]

            if not sentiment_counts.empty:
                fig1 = px.pie(
                    values=sentiment_counts[ordered_labels].values,
                    names=ordered_labels,
                    title='Overall Sentiment Distribution',
                    color=ordered_labels,
                    color_discrete_map=colors_map,
                    hole=0.0
                )
                st.plotly_chart(fig1, use_container_width=True)
            else:
                st.info("No sentiment data available for pie chart.")

            # 2. æ¯æ—¥æƒ…æ„Ÿè¶¨å‹¢åœ–
            st.subheader("2. Daily Sentiment Trend")

            if 'published_at_hk' in df_result.columns:
                df_result['date'] = df_result['published_at_hk'].dt.date
            else:
                df_result['date'] = pd.to_datetime(df_result['published_at'], utc=True).dt.tz_convert('Asia/Hong_Kong').dt.date

            daily = df_result.groupby(['date', 'sentiment']).size().unstack().fillna(0)
            daily = daily.reindex(columns=sentiments_order).dropna(axis=1, how='all')

            if not daily.empty:
                daily_long = daily.reset_index().melt(id_vars='date', var_name='sentiment', value_name='count')

                st.markdown("#### æ¯æ—¥æƒ…æ„Ÿè¶¨å‹¢ (æŠ˜ç·šåœ–)")
                fig_line = px.line(
                    daily_long, x='date', y='count', color='sentiment',
                    title='Daily Comment Volume Trend by Sentiment',
                    labels={'date': 'Date', 'count': 'Number of Comments', 'sentiment': 'Sentiment'},
                    color_discrete_map=colors_map,
                    category_orders={'sentiment': [col for col in sentiments_order if col in daily.columns]}
                )
                st.plotly_chart(fig_line, use_container_width=True)

                st.markdown("#### æ¯æ—¥ç•™è¨€ç¸½é‡åŠæƒ…æ„Ÿåˆ†ä½ˆ (å †ç–Šé•·æ¢åœ–)")
                fig_bar = px.bar(
                    daily_long, x='date', y='count', color='sentiment',
                    title='Daily Comment Volume by Sentiment (Stacked)',
                    labels={'date': 'Date', 'count': 'Number of Comments', 'sentiment': 'Sentiment'},
                    color_discrete_map=colors_map,
                    category_orders={'sentiment': [col for col in sentiments_order if col in daily.columns]},
                    barmode='stack'
                )
                st.plotly_chart(fig_bar, use_container_width=True)
            else:
                st.info("Not enough daily sentiment data to display the trend charts.")

            # 3. å„ä¸»é¡Œæƒ…æ„Ÿä½”æ¯”
            st.subheader("3. Sentiment Share by Topic")
            topic_sentiment = df_result.groupby(['topic', 'sentiment']).size().unstack().fillna(0)
            topic_sentiment = topic_sentiment.reindex(columns=sentiments_order).dropna(axis=1, how='all')

            if not topic_sentiment.empty:
                topic_sentiment = topic_sentiment[topic_sentiment.sum(axis=1) > 0]
                if not topic_sentiment.empty:
                    topic_sentiment_percent = topic_sentiment.div(topic_sentiment.sum(axis=1), axis=0).fillna(0) * 100
                    fig3 = px.bar(
                        topic_sentiment_percent.reset_index().melt(id_vars='topic', var_name='sentiment', value_name='pct'),
                        x='topic', y='pct', color='sentiment',
                        title='Sentiment Share by Topic',
                        labels={'topic': 'Topic', 'pct': 'Percentage (%)', 'sentiment': 'Sentiment'},
                        color_discrete_map=colors_map
                    )
                    st.plotly_chart(fig3, use_container_width=True)
                else:
                    st.info("No topic data with comments to display the chart.")
            else:
                st.info("Not enough topic sentiment data to display the stacked bar chart.")

            # 4. ä¸‹è¼‰åˆ†ææ˜ç´°ï¼ˆæ–°å¢ video_title / video_urlï¼‰
            st.subheader("4. ä¸‹è¼‰åˆ†ææ˜ç´°")
            csv = df_result.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                "ğŸ“¥ ä¸‹è¼‰å…¨éƒ¨åˆ†ææ˜ç´° (CSV)",
                csv,
                file_name=f"{movie_title}_analysis_details.csv",
                mime='text/csv'
            )
