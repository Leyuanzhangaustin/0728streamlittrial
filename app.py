import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from datetime import datetime, timedelta
import time
import asyncio
import openai

# ========== 1. YouTube Search (No changes) ==========
def search_youtube_videos(keywords, youtube_client, max_per_keyword, start_date, end_date):
    all_video_ids = set()
    for query in keywords:
        try:
            search_response = youtube_client.search().list(
                q=query,
                part='id,snippet',
                type='video',
                maxResults=max_per_keyword,
                publishedAfter=f"{start_date}T00:00:00Z",
                publishedBefore=f"{end_date}T23:59:59Z",
                relevanceLanguage='zh-Hant',
                regionCode='HK'
            ).execute()
            video_ids = [item['id']['videoId'] for item in search_response.get('items', [])]
            all_video_ids.update(video_ids)
            time.sleep(0.5)  # Keep a small delay to be nice to YouTube API
        except Exception as e:
            st.warning(f"æœå°‹é—œéµå­— '{query}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            continue
    return list(all_video_ids)

# ========== 2. Batch Fetch Comments (No changes) ==========
def get_all_comments(video_ids, youtube_client, max_per_video):
    all_comments = []
    for video_id in video_ids:
        try:
            request = youtube_client.commentThreads().list(
                part='snippet', videoId=video_id, textFormat='plainText', maxResults=100
            )
            comments_fetched = 0
            while request and comments_fetched < max_per_video:
                response = request.execute()
                for item in response['items']:
                    if comments_fetched >= max_per_video:
                        break
                    comment = item['snippet']['topLevelComment']['snippet']
                    all_comments.append({
                        'video_id': video_id,
                        'comment_text': comment['textDisplay'],
                        'published_at': comment['publishedAt'],
                        'like_count': comment['likeCount']
                    })
                    comments_fetched += 1
                if comments_fetched >= max_per_video:
                    break
                request = youtube_client.commentThreads().list_next(request, response)
        except Exception:
            continue
    return pd.DataFrame(all_comments)

# ========== 3. DeepSeek AI Sentiment Analysis (Async) ==========
async def analyze_comment_deepseek_async(comment_text, deepseek_client, semaphore, max_retries=3):
    import json
    if not isinstance(comment_text, str) or len(comment_text.strip()) < 5:
        return {"sentiment": "Invalid", "topic": "N/A", "summary": "Comment too short or invalid."}

    system_prompt = (
        "You are a professional Hong Kong market sentiment analyst. "
        "Analyze the following movie comment and strictly return the result in JSON format. "
        "The JSON object must contain three keys: "
        "1. 'sentiment': Must be either 'Positive', 'Negative', or 'Neutral'. "
        "2. 'topic': The core topic of the comment, e.g., 'Plot', 'Acting', 'Action Design', "
        "'Visuals', 'Pace', or 'Overall'. If unable to determine, use 'N/A'. "
        "3. 'summary': A concise one-sentence summary of the comment's main point. "
        "Ensure the output is only the JSON object and nothing else."
    )

    async with semaphore:
        for attempt in range(max_retries):
            try:
                response = await deepseek_client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": comment_text}
                    ],
                    response_format={"type": "json_object"},
                    temperature=0.1,
                )
                analysis_result = json.loads(response.choices[0].message.content)
                return analysis_result
            except Exception as e:
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                else:
                    return {"sentiment": "Error", "topic": "Error", "summary": f"API Error: {e}"}

# ========== 4. Main Process (Async Coordinator) ==========
async def run_all_analyses(df, deepseek_client):
    semaphore = asyncio.Semaphore(50)
    tasks = [
        analyze_comment_deepseek_async(comment_text, deepseek_client, semaphore)
        for comment_text in df['comment_text']
    ]

    from tqdm.asyncio import tqdm_asyncio
    results = await tqdm_asyncio.gather(*tasks, desc="AI Sentiment Analysis (Concurrent)")
    return results

def movie_comment_analysis(
    movie_title, start_date, end_date,
    yt_api_key, deepseek_api_key,
    max_videos_per_keyword=30, max_comments_per_video=50, sample_size=None
):
    SEARCH_KEYWORDS = [
        f'"{movie_title}" é å‘Š', f'"{movie_title}" review', f'"{movie_title}" å½±è©•',
        f'"{movie_title}" åˆ†æ', f'"{movie_title}" å¥½å””å¥½ç‡', f'"{movie_title}" è¨è«–',
        f'"{movie_title}" reaction'
    ]

    from googleapiclient.discovery import build
    youtube_client = build('youtube', 'v3', developerKey=yt_api_key)

    deepseek_client = openai.AsyncOpenAI(
        api_key=deepseek_api_key,
        base_url="https://api.deepseek.com/v1"
    )

    video_ids = search_youtube_videos(
        SEARCH_KEYWORDS, youtube_client, max_videos_per_keyword, start_date, end_date
    )
    if not video_ids:
        return None, "æ‰¾ä¸åˆ°ç›¸é—œå½±ç‰‡ã€‚"

    df_comments = get_all_comments(video_ids, youtube_client, max_comments_per_video)
    if df_comments.empty:
        return None, "æ‰¾ä¸åˆ°ä»»ä½•ç•™è¨€ã€‚"

    df_comments['published_at'] = pd.to_datetime(df_comments['published_at'], utc=True)
    df_comments['published_at_hk'] = df_comments['published_at'].dt.tz_convert('Asia/Hong_Kong')

    start = pd.to_datetime(start_date).tz_localize('Asia/Hong_Kong')
    end = pd.to_datetime(end_date).tz_localize('Asia/Hong_Kong') + timedelta(days=1)
    mask = (df_comments['published_at_hk'] >= start) & (df_comments['published_at_hk'] <= end)
    df_comments = df_comments.loc[mask].reset_index(drop=True)
    if df_comments.empty:
        return None, "åœ¨æŒ‡å®šæ—¥æœŸç¯„åœå…§æ²’æœ‰ç•™è¨€ã€‚"

    if sample_size and sample_size > 0 and sample_size < len(df_comments):
        df_analyze = df_comments.sample(n=sample_size, random_state=42)
    else:
        df_analyze = df_comments

    st.info(f"æº–å‚™å° {len(df_analyze)} å‰‡ç•™è¨€é€²è¡Œé«˜é€Ÿä¸¦ç™¼åˆ†æ...")
    analysis_results = asyncio.run(run_all_analyses(df_analyze, deepseek_client))

    analysis_df = pd.json_normalize(analysis_results)
    final_df = pd.concat([df_analyze.reset_index(drop=True), analysis_df], axis=1)
    final_df['published_at'] = pd.to_datetime(final_df['published_at'])
    return final_df, None

# ========== 5. Streamlit UI ==========
st.set_page_config(page_title="YouTube é›»å½±è©•è«– AI åˆ†æ", layout="wide")
st.title("ğŸ¬ YouTube é›»å½±è©•è«– AI æƒ…æ„Ÿåˆ†æ")

with st.expander("ä½¿ç”¨èªªæ˜"):
    st.markdown("""
    1.  è¼¸å…¥é›»å½±çš„**ä¸­æ–‡å…¨å**ã€åˆ†ææ™‚é–“ç¯„åœåŠæ‰€éœ€çš„ API é‡‘é‘°ã€‚
    2.  è‡ªè¨‚æ¯å€‹é—œéµå­—æœå°‹çš„å½±ç‰‡æ•¸é‡ä¸Šé™ï¼ŒåŠæ¯éƒ¨å½±ç‰‡æŠ“å–çš„ç•™è¨€æ•¸é‡ä¸Šé™ã€‚
    3.  é»æ“Šã€Œé–‹å§‹åˆ†æã€ï¼Œç³»çµ±å°‡è‡ªå‹•æŠ“å– YouTube ç•™è¨€ä¸¦é€²è¡Œ AI é«˜é€Ÿæƒ…æ„Ÿåˆ†æã€‚
    4.  åˆ†æå®Œæˆå¾Œï¼Œä¸‹æ–¹æœƒé¡¯ç¤ºæ•¸æ“šåœ–è¡¨åŠè©³ç´°çµæœçš„ä¸‹è¼‰æŒ‰éˆ•ã€‚
    """)

movie_title = st.text_input("é›»å½±åç¨± (å»ºè­°ä½¿ç”¨é¦™æ¸¯é€šç”¨çš„ä¸­æ–‡å…¨å)", value="ä¹é¾åŸå¯¨ä¹‹åœåŸ")
col1, col2 = st.columns(2)
with col1:
    start_date = st.date_input("é–‹å§‹æ—¥æœŸ", value=datetime.today() - timedelta(days=30))
with col2:
    end_date = st.date_input("çµæŸæ—¥æœŸ", value=datetime.today())
yt_api_key = st.text_input("YouTube API Key", type='password')
deepseek_api_key = st.text_input("DeepSeek API Key", type='password')

st.subheader("é€²éšè¨­å®š")
max_videos = st.slider("æ¯å€‹é—œéµå­—çš„æœ€å¤§å½±ç‰‡æœå°‹æ•¸", 5, 50, 10, help="å¢åŠ æ­¤æ•¸å€¼æœƒæ‰¾åˆ°æ›´å¤šå½±ç‰‡ï¼Œä½†æœƒå¢åŠ  YouTube API çš„é…é¡æ¶ˆè€—ã€‚")
max_comments = st.slider("æ¯éƒ¨å½±ç‰‡çš„æœ€å¤§ç•™è¨€æŠ“å–æ•¸", 10, 200, 50, help="åˆ†æçš„ä¸»è¦ä¾†æºï¼Œæ•¸é‡è¶Šå¤šï¼Œåˆ†æçµæœè¶Šå…¨é¢ï¼Œä½† DeepSeek API æˆæœ¬è¶Šé«˜ã€‚")
sample_size = st.number_input("åˆ†æç•™è¨€æ•¸é‡ä¸Šé™ (0 ä»£è¡¨åˆ†æå…¨éƒ¨å·²æŠ“å–çš„ç•™è¨€)", 0, 5000, 500, help="è¨­å®šä¸€å€‹ä¸Šé™ä»¥æ§åˆ¶åˆ†ææ™‚é–“å’Œæˆæœ¬ã€‚ä¾‹å¦‚ï¼Œå³ä½¿æŠ“å–äº† 2000 å‰‡ç•™è¨€ï¼Œé€™è£¡è¨­ 500 å°±åªæœƒåˆ†æå…¶ä¸­çš„ 500 å‰‡ã€‚")

if st.button("ğŸš€ é–‹å§‹åˆ†æ"):
    if not all([movie_title, yt_api_key, deepseek_api_key]):
        st.warning("è«‹å¡«å¯«é›»å½±åç¨±å’Œå…©å€‹ API é‡‘é‘°ã€‚")
    else:
        progress_placeholder = st.empty()

        with st.spinner("AI é«˜é€Ÿåˆ†æä¸­... (è™•ç† 500 å‰‡ç•™è¨€ç´„éœ€ 1-2 åˆ†é˜)"):
            df_result, err = movie_comment_analysis(
                movie_title, str(start_date), str(end_date),
                yt_api_key, deepseek_api_key,
                max_videos, max_comments, sample_size
            )

        if err:
            st.error(err)
        else:
            st.success("åˆ†æå®Œæˆï¼")
            st.dataframe(df_result.head(20))

            st.subheader("1. æƒ…æ„Ÿåˆ†ä½ˆåœ“é¤…åœ–")
            fig1, ax1 = plt.subplots(figsize=(5, 4))
            valmap = {
                "Positive": "æ­£é¢", "Negative": "è² é¢", "Neutral": "ä¸­æ€§",
                "Invalid": "ç„¡æ•ˆ", "Error": "éŒ¯èª¤"
            }
            df_result['sentiment_cn'] = df_result['sentiment'].map(lambda x: valmap.get(str(x), str(x)))
            s_counts = df_result['sentiment_cn'].value_counts()
            order = ['æ­£é¢', 'è² é¢', 'ä¸­æ€§', 'ç„¡æ•ˆ', 'éŒ¯èª¤']
            colors_map = {'æ­£é¢': '#5cb85c', 'è² é¢': '#d9534f', 'ä¸­æ€§': '#f0ad4e', 'ç„¡æ•ˆ': '#cccccc', 'éŒ¯èª¤': '#888888'}
            s_counts = s_counts.reindex(order).dropna()

            s_counts.plot.pie(
                autopct='%.1f%%', ax=ax1,
                colors=[colors_map[key] for key in s_counts.index],
                wedgeprops={'linewidth': 1.0, 'edgecolor': 'white'}
            )
            ax1.set_title('æ•´é«”æƒ…æ„Ÿåˆ†ä½ˆ', fontsize=16)
            ax1.set_ylabel('')
            st.pyplot(fig1, use_container_width=False)

            st.subheader("2. æ¯æ—¥æƒ…æ„Ÿè¶¨å‹¢ (å †ç–Šé•·æ¢åœ–)")
            df_result['date'] = df_result['published_at_hk'].dt.floor('D')
            daily = df_result.groupby(['date', 'sentiment_cn']).size().unstack().fillna(0)
            daily = daily.reindex(columns=order).dropna(axis=1, how='all')
            daily = daily.sort_index()
            daily.index = pd.to_datetime(daily.index)

            fig2, ax2 = plt.subplots(figsize=(12, 4))

            daily_plot_df = daily.reset_index()
            dates = daily_plot_df['date'].to_list()
            bottom = np.zeros(len(dates))

            for sentiment in daily.columns:
                values = daily_plot_df[sentiment].to_numpy()
                if np.any(values):
                    ax2.bar(
                        dates,
                        values,
                        bottom=bottom,
                        width=0.8,
                        color=colors_map.get(sentiment, '#999999'),
                        label=sentiment
                    )
                    bottom += values

            ax2.set_title('æ¯æ—¥æƒ…æ„Ÿè¶¨å‹¢', fontsize=16)
            ax2.set_xlabel('æ—¥æœŸ')
            ax2.set_ylabel('ç•™è¨€æ•¸é‡')

            num_dates = len(dates)
            if num_dates > 0:
                if num_dates > 20:
                    interval = max(5, int(np.ceil(num_dates / 12)))
                else:
                    interval = 1

                locator = mdates.DayLocator(interval=interval)
                formatter = mdates.DateFormatter('%Y-%m-%d')
                ax2.xaxis.set_major_locator(locator)
                ax2.xaxis.set_major_formatter(formatter)

                # ç¢ºä¿æœ€å¾Œä¸€å¤©ä¸€å®šå‡ºç¾åœ¨åˆ»åº¦ä¸Š
                if num_dates > 1:
                    last_date = dates[-1]
                    tick_dates = locator.tick_values(dates[0], dates[-1])
                    if last_date not in pd.to_datetime(tick_dates):
                        ax2.xaxis.set_major_locator(mdates.AutoDateLocator(maxticks=13))
                        ax2.xaxis.set_major_formatter(formatter)

                plt.setp(ax2.get_xticklabels(), rotation=45, ha='right', fontsize=9)
                ax2.set_xlim(dates[0] - pd.Timedelta(days=0.5), dates[-1] + pd.Timedelta(days=0.5))
            else:
                ax2.set_xticks([])

            ax2.legend(title='æƒ…æ„Ÿ')
            fig2.subplots_adjust(bottom=0.28)
            st.pyplot(fig2, use_container_width=True)

            st.subheader("3. å„ä¸»é¡Œæƒ…æ„Ÿä½”æ¯”")
            topic_sentiment = df_result.groupby(['topic', 'sentiment_cn']).size().unstack().fillna(0)
            topic_sentiment = topic_sentiment.reindex(columns=order).dropna(axis=1, how='all')
            topic_sentiment_percent = topic_sentiment.div(topic_sentiment.sum(axis=1), axis=0) * 100

            fig3, ax3 = plt.subplots(figsize=(10, 5))
            topic_sentiment_percent.plot(
                kind='bar',
                stacked=True,
                ax=ax3,
                color=[colors_map[col] for col in topic_sentiment_percent.columns]
            )
            ax3.set_title('å„è¨è«–ä¸»é¡Œçš„æƒ…æ„Ÿä½”æ¯”', fontsize=16)
            ax3.set_xlabel('ä¸»é¡Œ')
            ax3.set_ylabel('ç™¾åˆ†æ¯” (%)')
            ax3.yaxis.set_major_formatter(plt.FuncFormatter('{:.0f}%'.format))
            plt.xticks(rotation=45, ha='right')
            ax3.legend(title='æƒ…æ„Ÿ')
            plt.tight_layout()
            st.pyplot(fig3, use_container_width=True)

            st.subheader("4. ä¸‹è¼‰åˆ†ææ˜ç´°")
            csv = df_result.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                "ğŸ“¥ ä¸‹è¼‰å…¨éƒ¨åˆ†ææ˜ç´° (CSV)",
                csv,
                file_name=f"{movie_title}_analysis_details.csv",
                mime='text/csv'
