# app.py

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
from datetime import datetime, timedelta
import time
import asyncio
import openai
import re
from opencc import OpenCC
from googleapiclient.discovery import build

# =========================
# 0. å¿«å–è¨­å®šèˆ‡å·¥å…·å‡½æ•¸
# =========================

# è¨­å®šå¿«å–å­˜æ´»æ™‚é–“ (ç§’)
CACHE_TTL_SEARCH = 3600          # 1 å°æ™‚ï¼šæœå°‹çµæœã€å½±ç‰‡ç´°ç¯€
CACHE_TTL_COMMENTS = 900         # 15 åˆ†é˜ï¼šç•™è¨€æ¸…å–®

def _get_cached_value(cache_name: str, key, ttl_seconds: int):
    """å¾ st.session_state ç²å–å¿«å–ï¼Œè‹¥éæœŸå‰‡å›å‚³ None"""
    if cache_name not in st.session_state:
        st.session_state[cache_name] = {}
    
    entry = st.session_state[cache_name].get(key)
    if entry:
        # æª¢æŸ¥æ˜¯å¦éæœŸ
        if (time.time() - entry["ts"]) <= ttl_seconds:
            return entry["value"]
        else:
            # éæœŸå‰‡åˆªé™¤
            del st.session_state[cache_name][key]
    return None

def _set_cached_value(cache_name: str, key, value):
    """å¯«å…¥å¿«å–åˆ° st.session_state"""
    if cache_name not in st.session_state:
        st.session_state[cache_name] = {}
    st.session_state[cache_name][key] = {
        "value": value,
        "ts": time.time()
    }

# =========================
# 1. èªè¨€èˆ‡ç²µèªæª¢æ¸¬å·¥å…·
# =========================

def generate_search_queries(movie_title: str):
    """
    ç”Ÿæˆåå‘é¦™æ¸¯/ç²µèªçš„å¯¬é¬†é—œéµå­—çµ„åˆ + å°‘é‡ç²¾ç¢ºçŸ­èªã€‚
    """
    zh_terms = [
        "å½±è©•", "è©•è«–", "è©•åƒ¹", "é»è©•", "è§£æ", "åˆ†æ", "è§€å¾Œæ„Ÿ",
        "ç„¡é›·", "æœ‰é›·", "è¨è«–", "å¥½å””å¥½ç‡", "é å‘Š", "èŠ±çµ®", "ç‰‡æ®µ", "é¦–æ˜ ", "å¹•å¾Œ",
        "é¦™æ¸¯", "é¦™æ¸¯ä¸Šæ˜ ", "é¦™æ¸¯é¦–æ˜ ", "é¦™æ¸¯åæ‡‰", "æˆ²é™¢ åæ‡‰", "é™¢ç·š", "è¡—è¨ª",
        "ç²µèª", "å»£æ±è©±", "ç²µèªé…éŸ³", "ç²µé…", "æ¸¯ç‰ˆ", "æ¸¯ç”¢"
    ]
    en_terms = [
        "review", "reaction", "ending explained", "analysis", "explained",
        "behind the scenes", "bts", "premiere", "interview", "press conference",
        "hong kong", "hk reaction", "hk audience"
    ]

    loose = [f"{movie_title}"]
    loose += [f"{movie_title} {t}" for t in zh_terms]
    loose += [f"{movie_title} {t}" for t in en_terms]

    tight = [
        f"\"{movie_title}\"",
        f"\"{movie_title}\" å½±è©•",
        f"\"{movie_title}\" è©•è«–",
        f"\"{movie_title}\" è§£æ",
        f"\"{movie_title}\" review",
        f"\"{movie_title}\" reaction",
        f"\"{movie_title}\" é¦™æ¸¯",
        f"\"{movie_title}\" ç²µèª",
        f"\"{movie_title}\" å»£æ±è©±",
    ]

    seen = set()
    queries = []
    for q in loose + tight:
        if q not in seen:
            queries.append(q)
            seen.add(q)
    return queries


# ç²µèªæ¨™è¨˜è©ï¼ˆæœ¬å­—/å£èª/åŠ©è©ï¼‰èˆ‡æ¬Šé‡
CANTONESE_CHAR_TOKENS = {
    "å””": 1.0, "å†‡": 1.6, "å’—": 1.6, "å˜…": 1.6, "å•²": 1.2, "å—°": 1.2, "ä½¢": 1.0,
    "å–º": 1.6, "åšŸ": 1.6, "å’ª": 1.2, "å•±": 1.2, "æ‚": 1.2, "éš": 1.2, "æ›³": 1.2,
    "æ”°": 1.2, "å’": 1.0, "å™‰": 1.0, "å¾—": 0.6, "å–": 0.8, "å†§": 1.0, "æ’š": 1.2,
    "ä»†": 1.2, "å±Œ": 1.2, "å—®": 1.0, "ç•€": 0.8, "æ¸": 1.0, "è…": 0.0
}
CANTONESE_PARTICLES = ["å•¦", "å›‰", "å–", "å’©", "å‘¢", "å‘€", "å˜›", "å–‡"]
CANTONESE_PHRASES = {
    "å¥½å””å¥½ç‡": 2.0, "åšå’©": 1.6, "é»è§£": 1.2, "å’©æ–™": 1.6, "ç®—å•¦": 1.2,
    "å¾—å•¦": 1.2, "æ­£å–": 1.2, "å¹¾å¥½ç‡": 1.6, "å¹¾æ­£": 1.2, "å¥½æ­£": 1.0,
    "æœ‰å•²": 0.8, "å—°å•²": 1.2, "å‘¢å•²": 1.2, "è¬›çœŸ": 0.8, "å¥½ä¼¼": 0.5
}
ROMANIZATION_RE = re.compile(r"(?i)(?<![A-Za-z])(la|lor|wor|leh|meh|mah|ga|wo|ar)(?=[\s\W]|$)")

def count_chars(text: str):
    counts = {
        "cjk": 0, "hiragana": 0, "katakana": 0, "half_katakana": 0,
        "hangul": 0, "latin": 0, "digits": 0, "other": 0
    }
    for ch in text:
        code = ord(ch)
        if 0x4E00 <= code <= 0x9FFF or 0x3400 <= code <= 0x4DBF or 0xF900 <= code <= 0xFAFF:
            counts["cjk"] += 1
        elif 0x3040 <= code <= 0x309F:
            counts["hiragana"] += 1
        elif 0x30A0 <= code <= 0x30FF or 0x31F0 <= code <= 0x31FF:
            counts["katakana"] += 1
        elif 0xFF65 <= code <= 0xFF9F:
            counts["half_katakana"] += 1
        elif 0xAC00 <= code <= 0xD7AF:
            counts["hangul"] += 1
        elif (0x0041 <= code <= 0x005A) or (0x0061 <= code <= 0x007A):
            counts["latin"] += 1
        elif 0x0030 <= code <= 0x0039:
            counts["digits"] += 1
        else:
            counts["other"] += 1
    return counts

def diff_chars(a: str, b: str) -> int:
    m = min(len(a), len(b))
    return sum(1 for i in range(m) if a[i] != b[i]) + abs(len(a) - len(b))

def classify_zh_trad_simp(text: str, cc_t2s: OpenCC, cc_s2t: OpenCC):
    if not isinstance(text, str) or len(text.strip()) < 2:
        return "other"
    counts = count_chars(text)
    kana = counts["hiragana"] + counts["katakana"] + counts["half_katakana"]
    cjk = counts["cjk"]
    if kana >= 2 and kana / max(1, (cjk + kana)) >= 0.10:
        return "ja"
    if cjk < 1:
        return "other"
    t2s = cc_t2s.convert(text)
    s2t = cc_s2t.convert(text)
    ct2s = diff_chars(text, t2s)
    cs2t = diff_chars(text, s2t)
    threshold = max(1, int(0.05 * cjk))
    if ct2s > cs2t + threshold:
        return "zh-Hant"
    elif cs2t > ct2s + threshold:
        return "zh-Hans"
    else:
        return "zh-unkn"

def score_cantonese(text: str) -> float:
    if not isinstance(text, str) or not text:
        return 0.0
    score = 0.0
    for phrase, w in CANTONESE_PHRASES.items():
        cnt = text.count(phrase)
        if cnt:
            score += cnt * w
    for ch, w in CANTONESE_CHAR_TOKENS.items():
        cnt = text.count(ch)
        if cnt:
            score += cnt * w
    end_slice = text[-8:] if len(text) > 8 else text
    for p in CANTONESE_PARTICLES:
        cnt = text.count(p)
        if cnt:
            score += cnt * 0.6
        if p in end_slice:
            score += 0.4
    roman_hits = ROMANIZATION_RE.findall(text)
    if roman_hits:
        score += len(roman_hits) * 0.8
    return score


# =========================
# 2. YouTube æœå°‹ï¼ˆæ•´åˆå¿«å– + å…¨åŸŸä¸Šé™ï¼‰
# =========================

def search_youtube_videos(
    keywords,
    youtube_client,
    max_per_keyword,
    start_date,
    end_date,
    add_language_bias=True,
    region_bias=True,
    max_total_videos=300  # å…¨åŸŸå®‰å…¨ä¸Šé™
):
    all_video_ids = set()
    video_meta = {}
    search_cache_name = "yt_search_cache"

    for query in keywords:
        # è‹¥å·²é”å…¨åŸŸä¸Šé™ï¼Œæå‰çµæŸ
        if len(all_video_ids) >= max_total_videos:
            break

        for order in ["relevance", "viewCount"]:
            # å»ºç«‹å¿«å– Key
            cache_key = f"{query}|{order}|{start_date}|{end_date}|{max_per_keyword}"
            cached_records = _get_cached_value(search_cache_name, cache_key, CACHE_TTL_SEARCH)

            query_records = []
            
            if cached_records is not None:
                query_records = cached_records
            else:
                # ç„¡å¿«å–ï¼ŒåŸ·è¡Œ API
                collected_for_query = []
                collected_ids_local = set()
                
                try:
                    request = youtube_client.search().list(
                        q=query,
                        part="id,snippet",
                        type="video",
                        maxResults=50,
                        publishedAfter=f"{start_date}T00:00:00Z",
                        publishedBefore=f"{end_date}T23:59:59Z",
                        order=order,
                        safeSearch="none",
                        **({"relevanceLanguage": "zh-Hant"} if add_language_bias else {}),
                        **({"regionCode": "HK"} if region_bias else {})
                    )
                    while request and len(collected_for_query) < max_per_keyword:
                        response = request.execute()
                        for item in response.get("items", []):
                            vid = item["id"]["videoId"]
                            if vid in collected_ids_local:
                                continue
                            collected_ids_local.add(vid)
                            
                            snip = item.get("snippet", {})
                            record = {
                                "video_id": vid,
                                "title": snip.get("title", ""),
                                "channelTitle": snip.get("channelTitle", ""),
                                "publishedAt": snip.get("publishedAt", "")
                            }
                            collected_for_query.append(record)
                        
                        request = youtube_client.search().list_next(request, response)
                        if len(collected_for_query) >= max_per_keyword:
                            break
                        time.sleep(0.15) # é¿å…é€Ÿç‡é™åˆ¶
                except Exception as e:
                    st.warning(f"æœå°‹é—œéµå­— '{query}'ï¼ˆorder={order}ï¼‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                
                # å¯«å…¥å¿«å–
                _set_cached_value(search_cache_name, cache_key, collected_for_query)
                query_records = collected_for_query

            # æ•´åˆçµæœ
            for record in query_records:
                vid = record["video_id"]
                if vid not in all_video_ids:
                    all_video_ids.add(vid)
                    if vid not in video_meta:
                        video_meta[vid] = {
                            "title": record["title"],
                            "channelTitle": record["channelTitle"],
                            "publishedAt": record["publishedAt"]
                        }
                if len(all_video_ids) >= max_total_videos:
                    break
            
            if len(all_video_ids) >= max_total_videos:
                break

    return list(all_video_ids), video_meta


def fetch_video_and_channel_details(video_ids, youtube_client):
    """
    å–å›æ¯å€‹è¦–é »çš„ snippet ä»¥ç²å¾— channelId/defaultAudioLanguage/tagsï¼Œ
    å†å–å›é »é“ brandingSettings.channel.country ç”¨æ–¼é¦™æ¸¯å‚¾å‘åˆ¤æ–·ã€‚
    (åŠ å…¥é »é“å±¤ç´šå¿«å–)
    """
    video_extra = {}
    channel_ids = set()
    channel_cache_name = "yt_channel_cache"

    # 1. æŠ“å–å½±ç‰‡è©³æƒ… (å½±ç‰‡è©³æƒ…è®Šå‹•å¤§ï¼Œè¼ƒå°‘å¿«å–ï¼Œæˆ–å¯è¦–éœ€æ±‚å¿«å–)
    for i in range(0, len(video_ids), 50):
        chunk = video_ids[i:i+50]
        try:
            resp = youtube_client.videos().list(
                part="snippet,contentDetails",
                id=",".join(chunk)
            ).execute()
            for item in resp.get("items", []):
                vid = item.get("id")
                snip = item.get("snippet", {}) or {}
                ch = snip.get("channelId")
                video_extra[vid] = {
                    "channelId": ch,
                    "defaultLanguage": (snip.get("defaultLanguage") or ""),
                    "defaultAudioLanguage": (snip.get("defaultAudioLanguage") or ""),
                    "tags": snip.get("tags", [])
                }
                if ch:
                    channel_ids.add(ch)
        except Exception as e:
            st.warning(f"videos.list å–è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    # 2. æŠ“å–é »é“è©³æƒ… (ä½¿ç”¨å¿«å–)
    channel_country = {}
    channels_to_fetch = []

    for cid in channel_ids:
        cached_country = _get_cached_value(channel_cache_name, cid, CACHE_TTL_SEARCH)
        if cached_country is not None:
            channel_country[cid] = cached_country
        else:
            channels_to_fetch.append(cid)

    # åªæŠ“å–å¿«å–æ²’æœ‰çš„é »é“
    if channels_to_fetch:
        for i in range(0, len(channels_to_fetch), 50):
            chunk = channels_to_fetch[i:i+50]
            try:
                resp = youtube_client.channels().list(
                    part="brandingSettings",
                    id=",".join(chunk)
                ).execute()
                for item in resp.get("items", []):
                    cid = item.get("id")
                    brand = (item.get("brandingSettings", {}) or {}).get("channel", {}) or {}
                    country = brand.get("country")  # ä¾‹ï¼š'HK'ã€'TW'ã€None
                    channel_country[cid] = country
                    # å¯«å…¥å¿«å–
                    _set_cached_value(channel_cache_name, cid, country)
            except Exception as e:
                st.warning(f"channels.list å–è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    return video_extra, channel_country


def compute_hk_video_score(
    video_id, video_meta, video_extra, channel_country_map
):
    meta = video_meta.get(video_id, {}) or {}
    ext = video_extra.get(video_id, {}) or {}
    title = meta.get("title", "") or ""
    tags = " ".join(ext.get("tags", []) or [])
    ch = ext.get("channelId")
    default_audio = (ext.get("defaultAudioLanguage") or "").lower()
    country = channel_country_map.get(ch)

    score = 0
    if country == "HK":
        score += 3
    if default_audio in ("yue", "zh-hk", "zh-yue", "zh-hant-hk"):
        score += 3
    elif default_audio.startswith("zh"):
        score += 1
    if any(tok in title for tok in ["ç²µèª", "å»£æ±è©±", "ç²µé…", "ç²µèªé…éŸ³"]):
        score += 3
    if any(tok in title for tok in ["é¦™æ¸¯", "æ¸¯ç‰ˆ", "é¦™æ¸¯è§€çœ¾", "é¦™æ¸¯åæ‡‰", "é¦™æ¸¯é¦–æ˜ ", "é¦™æ¸¯ä¸Šæ˜ "]):
        score += 2
    if ("HK" in title) or ("Hong Kong" in title):
        score += 1
    if any(tok in tags for tok in ["ç²µèª", "å»£æ±è©±", "é¦™æ¸¯", "HK"]):
        score += 2
    return score


# =========================
# 3. æ‰¹é‡æŠ“å–ç•™è¨€ï¼ˆæ•´åˆå¿«å– + å…¨åŸŸä¸Šé™ï¼‰
# =========================

def get_all_comments(video_ids, youtube_client, max_per_video, video_meta=None, hk_score_map=None, video_extra=None, channel_country_map=None, max_total_comments=2000):
    video_meta = video_meta or {}
    hk_score_map = hk_score_map or {}
    video_extra = video_extra or {}
    channel_country_map = channel_country_map or {}

    all_comments = []
    total_videos = len(video_ids)
    comments_cache_name = "yt_comments_cache"
    
    progress_bar = st.progress(0, text="æŠ“å– YouTube ç•™è¨€ä¸­...")

    for i, video_id in enumerate(video_ids):
        # å…¨åŸŸä¸Šé™æª¢æŸ¥
        if len(all_comments) >= max_total_comments:
            break

        # å»ºç«‹å¿«å– Key (åŒ…å« max_per_video ä»¥ç¢ºä¿æ•¸é‡ä¸€è‡´)
        cache_key = f"{video_id}|{max_per_video}"
        cached_comments = _get_cached_value(comments_cache_name, cache_key, CACHE_TTL_COMMENTS)

        video_comments = []

        if cached_comments is not None:
            video_comments = cached_comments
        else:
            # ç„¡å¿«å–ï¼Œå‘¼å« API
            try:
                request = youtube_client.commentThreads().list(
                    part="snippet",
                    videoId=video_id,
                    textFormat="plainText",
                    order="time",
                    maxResults=100
                )
                comments_fetched = 0
                raw_comments = []
                
                while request and comments_fetched < max_per_video:
                    response = request.execute()
                    for item in response.get("items", []):
                        if comments_fetched >= max_per_video:
                            break
                        comment = item["snippet"]["topLevelComment"]["snippet"]
                        record = {
                            "comment_text": comment.get("textDisplay", ""),
                            "published_at": comment.get("publishedAt", ""),
                            "like_count": comment.get("likeCount", 0)
                        }
                        raw_comments.append(record)
                        comments_fetched += 1
                    
                    if comments_fetched >= max_per_video:
                        break
                    request = youtube_client.commentThreads().list_next(request, response)
                    time.sleep(0.15)
                
                # å¯«å…¥å¿«å– (åªå­˜åŸå§‹è³‡æ–™ä»¥ç¯€çœç©ºé–“)
                _set_cached_value(comments_cache_name, cache_key, raw_comments)
                video_comments = raw_comments

            except Exception:
                # é‡åˆ°éŒ¯èª¤ (ä¾‹å¦‚ç•™è¨€é—œé–‰) å‰‡å¿½ç•¥
                pass

        # å°‡åŸå§‹è³‡æ–™è½‰æ›ç‚ºåˆ†ææ ¼å¼
        ch_id = (video_extra.get(video_id, {}) or {}).get("channelId")
        for raw in video_comments:
            all_comments.append({
                "video_id": video_id,
                "video_title": video_meta.get(video_id, {}).get("title", ""),
                "video_url": f"https://www.youtube.com/watch?v={video_id}",
                "video_hk_score": hk_score_map.get(video_id, 0),
                "video_channel_id": ch_id,
                "video_channel_country": (channel_country_map.get(ch_id) if ch_id else None),
                "video_default_audio_lang": (video_extra.get(video_id, {}) or {}).get("defaultAudioLanguage", ""),
                "comment_text": raw["comment_text"],
                "published_at": raw["published_at"],
                "like_count": raw["like_count"]
            })
            if len(all_comments) >= max_total_comments:
                break

        progress_bar.progress(
            (i + 1) / max(1, total_videos),
            text=f"æŠ“å– YouTube ç•™è¨€ä¸­... ({min(i+1, total_videos)}/{total_videos} éƒ¨å½±ç‰‡) - å·²æ”¶é›† {len(all_comments)} å‰‡ç•™è¨€"
        )

    progress_bar.empty()
    return pd.DataFrame(all_comments)


# =========================
# 4. DeepSeek AI ç•°æ­¥æƒ…æ„Ÿåˆ†æ
# =========================

async def analyze_comment_deepseek_async(comment_text, deepseek_client, semaphore, max_retries=3):
    import json
    if not isinstance(comment_text, str) or len(comment_text.strip()) < 5:
        return {"sentiment": "Invalid", "topic": "N/A", "summary": "Comment too short or invalid."}

    system_prompt = (
        "You are a professional Hong Kong market sentiment analyst. "
        "Analyze the following movie comment and strictly return the result in JSON format. "
        "The JSON object must contain three keys: "
        "1. 'sentiment': Must be either 'Positive', 'Negative', or 'Neutral'. "
        "2. 'topic': The core topic of the comment, e.g., 'Plot', 'Acting', 'Action Design', "
        "'Visuals', 'Pace', or 'Overall'. If unable to determine, use 'N/A'. "
        "3. 'summary': A concise one-sentence summary of the comment's main point. "
        "Ensure the output is only the JSON object and nothing else."
    )

    async with semaphore:
        for attempt in range(max_retries):
            try:
                response = await deepseek_client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": comment_text}
                    ],
                    response_format={"type": "json_object"},
                    temperature=0.1,
                )
                data = response.choices[0].message.content
                analysis_result = json.loads(data)
                return analysis_result
            except Exception as e:
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                else:
                    return {"sentiment": "Error", "topic": "Error", "summary": f"API Error: {e}"}

async def run_all_analyses(df, deepseek_client):
    semaphore = asyncio.Semaphore(50)
    tasks = []

    async def with_index(idx, text):
        res = await analyze_comment_deepseek_async(text, deepseek_client, semaphore)
        return idx, res

    for i, text in enumerate(df["comment_text"]):
        tasks.append(asyncio.create_task(with_index(i, text)))

    progress_bar = st.progress(0, text="AI æƒ…æ„Ÿåˆ†æä¸­...")
    results = [None] * len(tasks)
    for done_idx, coro in enumerate(asyncio.as_completed(tasks), start=1):
        idx, res = await coro
        results[idx] = res
        progress_bar.progress(done_idx / len(tasks), text=f"AI æƒ…æ„Ÿåˆ†æä¸­... ({done_idx}/{len(tasks)})")
    progress_bar.empty()
    return results


# =========================
# 5. ä¸»æµç¨‹ï¼ˆæ•´åˆåƒæ•¸ï¼‰
# =========================

def movie_comment_analysis(
    movie_title, start_date, end_date,
    yt_api_key, deepseek_api_key,
    max_videos_per_keyword=30, max_comments_per_video=50, sample_size=None,
    relax_trad_filter=True,
    cantonese_threshold=2.0,
    auto_relax_threshold=True,
    target_min_cantonese=300,
    prefer_hk_videos=True,
    # æ–°å¢å…¨åŸŸé™åˆ¶åƒæ•¸ (é è¨­å€¼ä½œç‚ºå®‰å…¨ç¶²)
    max_total_videos_global=200,
    max_total_comments_global=2000
):
    # ç”Ÿæˆé—œéµå­—ï¼ˆåå‘é¦™æ¸¯/ç²µèªï¼‰
    SEARCH_KEYWORDS = generate_search_queries(movie_title)

    youtube_client = build("youtube", "v3", developerKey=yt_api_key)
    deepseek_client = openai.AsyncOpenAI(
        api_key=deepseek_api_key,
        base_url="https://api.deepseek.com/v1"
    )

    # 1) æœå°‹ + å½±ç‰‡/é »é“è£œå……è³‡æ–™
    # å‚³å…¥ max_total_videos_global é™åˆ¶ API å‘¼å«
    video_ids, video_meta = search_youtube_videos(
        SEARCH_KEYWORDS, youtube_client, max_videos_per_keyword, start_date, end_date,
        add_language_bias=True, region_bias=True,
        max_total_videos=max_total_videos_global
    )
    if not video_ids:
        return None, "æ‰¾ä¸åˆ°ç›¸é—œå½±ç‰‡ã€‚"

    video_extra, channel_country_map = fetch_video_and_channel_details(video_ids, youtube_client)

    # 2) å½±ç‰‡é¦™æ¸¯å‚¾å‘æ’åº
    hk_score_map = {vid: compute_hk_video_score(vid, video_meta, video_extra, channel_country_map) for vid in video_ids}
    video_ids_sorted = sorted(video_ids, key=lambda v: hk_score_map.get(v, 0), reverse=True) if prefer_hk_videos else video_ids

    # 3) æŠ“å–ç•™è¨€ï¼ˆå¸¶ä¾†æºå­—æ®µ + hk åˆ†æ•¸ï¼‰
    # å‚³å…¥ max_total_comments_global é™åˆ¶ API å‘¼å«
    df_comments = get_all_comments(
        video_ids_sorted, youtube_client, max_comments_per_video,
        video_meta=video_meta, hk_score_map=hk_score_map,
        video_extra=video_extra, channel_country_map=channel_country_map,
        max_total_comments=max_total_comments_global
    )
    if df_comments.empty:
        return None, "æ‰¾ä¸åˆ°ä»»ä½•ç•™è¨€ã€‚"

    # 4) èªè¨€éæ¿¾ï¼ˆå‰”é™¤æ—¥æ–‡ï¼Œä¿ç•™ç¹é«”/é›£åˆ†ï¼‰+ ç²µèªæ‰“åˆ†
    st.info(f"å·²æŠ“å– {len(df_comments)} å‰‡åŸå§‹ç•™è¨€ï¼Œç¾é–‹å§‹èªè¨€èˆ‡ç²µèªç¯©é¸...")

    cc_t2s = OpenCC("t2s")
    cc_s2t = OpenCC("s2t")

    def lang_pred(text):
        return classify_zh_trad_simp(text, cc_t2s, cc_s2t)

    df_comments["lang_pred"] = df_comments["comment_text"].apply(lang_pred)
    df_comments = df_comments[~df_comments["lang_pred"].isin(["ja", "other", "zh-Hans"])].reset_index(drop=True)

    if relax_trad_filter:
        df_comments = df_comments[df_comments["lang_pred"].isin(["zh-Hant", "zh-unkn"])].reset_index(drop=True)
    else:
        df_comments = df_comments[df_comments["lang_pred"] == "zh-Hant"].reset_index(drop=True)

    if df_comments.empty:
        return None, "åœ¨æŠ“å–çš„ç•™è¨€ä¸­æ²’æœ‰ç¬¦åˆåŸºæœ¬èªè¨€æ¢ä»¶çš„å…§å®¹ã€‚"

    # ç²µèªåˆ†æ•¸
    df_comments["cantonese_score"] = df_comments["comment_text"].apply(score_cantonese)

    # 5) ç²µèªé–€æª» + è‡ªå‹•æ”¾å¯¬
    thr = float(cantonese_threshold)
    def filt(t): return t >= thr
    df_filtered = df_comments[df_comments["cantonese_score"].apply(filt)].reset_index(drop=True)

    if auto_relax_threshold and len(df_filtered) < target_min_cantonese:
        new_thr = thr
        while len(df_filtered) < target_min_cantonese and new_thr > 0.5:
            new_thr = round(new_thr - 0.5, 2)
            df_filtered = df_comments[df_comments["cantonese_score"] >= new_thr].reset_index(drop=True)
        if new_thr != thr:
            st.info(f"è‡ªå‹•æ”¾å¯¬ç²µèªåˆ†æ•¸é–€æª»ï¼š{thr} âœ {new_thr}ï¼ˆç›®å‰ç¬¦åˆæ¢ä»¶ç•™è¨€ï¼š{len(df_filtered)}ï¼‰")
            thr = new_thr

    st.info(f"èªè¨€èˆ‡ç²µèªç¯©é¸å¾Œå‰©ä¸‹ {len(df_filtered)} å‰‡ç•™è¨€ï¼ˆé–€æª»={thr}ï¼‰ã€‚")
    if df_filtered.empty:
        return None, "ç²µèªç¯©é¸å¾Œæ¨£æœ¬ç‚º 0ï¼Œè«‹èª¿ä½é–€æª»æˆ–å»¶é•·æ™‚é–“ç¯„åœã€‚"

    # 6) æ™‚å€èˆ‡æ—¥æœŸç¯©é¸
    df_filtered["published_at"] = pd.to_datetime(df_filtered["published_at"], utc=True, errors="coerce")
    df_filtered["published_at_hk"] = df_filtered["published_at"].dt.tz_convert("Asia/Hong_Kong")

    start_dt = pd.to_datetime(start_date).tz_localize("Asia/Hong_Kong")
    end_dt = pd.to_datetime(end_date).tz_localize("Asia/Hong_Kong") + timedelta(days=1)
    mask_date = (df_filtered["published_at_hk"] >= start_dt) & (df_filtered["published_at_hk"] < end_dt)
    df_filtered = df_filtered.loc[mask_date].reset_index(drop=True)
    if df_filtered.empty:
        return None, "åœ¨æŒ‡å®šæ—¥æœŸç¯„åœå…§æ²’æœ‰ç¬¦åˆç²µèªæ¢ä»¶çš„ç•™è¨€ã€‚"

    # 7) å–æ¨£æ§åˆ¶
    if sample_size and 0 < sample_size < len(df_filtered):
        df_analyze = df_filtered.sample(n=sample_size, random_state=42)
    else:
        df_analyze = df_filtered

    st.info(f"æº–å‚™å° {len(df_analyze)} å‰‡ç•™è¨€é€²è¡Œé«˜é€Ÿä¸¦ç™¼åˆ†æ...")

    # 8) ç•°æ­¥åˆ†æ + å°é½Š
    analysis_results = asyncio.run(run_all_analyses(df_analyze, deepseek_client))
    analysis_df = pd.DataFrame(analysis_results)
    final_df = pd.concat([df_analyze.reset_index(drop=True), analysis_df], axis=1)

    final_df["published_at"] = pd.to_datetime(final_df["published_at"])
    return final_df, None


# =========================
# 6. Streamlit UI
# =========================

st.set_page_config(page_title="YouTube é›»å½±è©•è«– AI åˆ†æï¼ˆé¦™æ¸¯ç²µèªå„ªå…ˆï¼‰", layout="wide")
st.title("ğŸ¬ YouTube é›»å½±è©•è«– AI æƒ…æ„Ÿåˆ†æï¼ˆé¦™æ¸¯ç²µèªå„ªå…ˆï¼‰")

with st.expander("ä½¿ç”¨èªªæ˜"):
    st.markdown("""
    1.  è¼¸å…¥é›»å½±çš„ä¸­æ–‡å…¨åã€åˆ†ææ™‚é–“ç¯„åœåŠæ‰€éœ€çš„ API é‡‘é‘°ã€‚
    2.  æœ¬å·¥å…·æœƒåå‘æŠ“å–é¦™æ¸¯åœ°å€çš„å½±ç‰‡èˆ‡ç•™è¨€ï¼Œä¸¦ç”¨ç²µèªç‰¹å¾µæ‰“åˆ†éæ¿¾ã€‚
    3.  å¯èª¿æ•´ã€Œç²µèªåˆ†æ•¸é–€æª»ã€èˆ‡ã€Œè‡ªå‹•æ”¾å¯¬ã€ç¢ºä¿æ¨£æœ¬é‡ï¼›åŒæ™‚å‰”é™¤æ—¥æ–‡èˆ‡ç°¡é«”ç‚ºä¸»çš„ç•™è¨€ã€‚
    4.  åˆ†æå®Œæˆå¾Œï¼Œæä¾›å¯è¦–åŒ–èˆ‡ CSV ä¸‹è¼‰ï¼ˆå«ä¾†æºå½±ç‰‡æ¨™é¡Œèˆ‡é€£çµï¼‰ã€‚
    """)

movie_title = st.text_input("é›»å½±åç¨± (å»ºè­°ä½¿ç”¨é¦™æ¸¯é€šç”¨çš„ä¸­æ–‡å…¨å)", value="ä¹é¾åŸå¯¨ä¹‹åœåŸ")
col1, col2 = st.columns(2)
with col1:
    start_date = st.date_input("é–‹å§‹æ—¥æœŸ", value=datetime.today() - timedelta(days=30))
with col2:
    end_date = st.date_input("çµæŸæ—¥æœŸ", value=datetime.today())
yt_api_key = st.text_input("YouTube API Key", type='password')
deepseek_api_key = st.text_input("DeepSeek API Key", type='password')

st.subheader("é€²éšè¨­å®š")
max_videos = st.slider("æ¯å€‹é—œéµå­—çš„æœ€å¤§å½±ç‰‡æœå°‹æ•¸", 5, 80, 30, help="æé«˜å¯å¢åŠ è¦†è“‹ï¼Œä½†æœƒå¢åŠ  YouTube API é…é¡æ¶ˆè€—ã€‚")
max_comments = st.slider("æ¯éƒ¨å½±ç‰‡çš„æœ€å¤§ç•™è¨€æŠ“å–æ•¸", 10, 200, 80, help="æ•¸é‡è¶Šå¤šï¼Œåˆ†æçµæœè¶Šå…¨é¢ï¼Œä½† DeepSeek API æˆæœ¬è¶Šé«˜ã€‚")
sample_size = st.number_input("åˆ†æç•™è¨€æ•¸é‡ä¸Šé™ (0=å…¨é‡)", 0, 5000, 500)

relax_trad_filter = st.checkbox("æ”¾å¯¬ç¹é«”åˆ¤å®šï¼ˆå…è¨±é›£åˆ†çš„ä¸­æ–‡ç•™è¨€ï¼‰", value=True)
prefer_hk_videos = st.checkbox("å„ªå…ˆæŠ“å–æ›´å¯èƒ½ä¾†è‡ªé¦™æ¸¯/ç²µèªçš„å½±ç‰‡ï¼ˆæ’åºåŠ æ¬Šï¼‰", value=True)

cantonese_threshold = st.slider("ç²µèªåˆ†æ•¸é–€æª»", 0.5, 6.0, 2.0, 0.5, help="åˆ†æ•¸è¶Šé«˜è¶Šåš´æ ¼ï¼Œ2.0 æ˜¯è¼ƒç©©å¥çš„é–€æª»ã€‚")
auto_relax_threshold = st.checkbox("è‡ªå‹•æ”¾å¯¬é–€æª»ä»¥é”åˆ°ç›®æ¨™æ¨£æœ¬é‡", value=True)
target_min_cantonese = st.number_input("ç›®æ¨™æœ€å°‘ç²µèªè©•è«–æ•¸ï¼ˆå•Ÿç”¨è‡ªå‹•æ”¾å¯¬æ™‚ç”Ÿæ•ˆï¼‰", 50, 5000, 300)

if st.button("ğŸš€ é–‹å§‹åˆ†æ"):
    if not all([movie_title, yt_api_key, deepseek_api_key]):
        st.warning("è«‹å¡«å¯«é›»å½±åç¨±å’Œå…©å€‹ API é‡‘é‘°ã€‚")
    else:
        # ç‚ºäº†ç¯€çœ Tokenï¼Œé€™è£¡è¨­å®šå…¨åŸŸä¸Šé™
        # é‚è¼¯ï¼šå¦‚æœä½¿ç”¨è€…è¨­å®šæ¯å€‹é—œéµå­—æ‰¾ 30 éƒ¨ï¼Œæˆ‘å€‘æœ€å¤šå…è¨±æ‰¾ 200 éƒ¨ç¸½æ•¸ (ç´„ 6-7 å€‹é—œéµå­—æ»¿è¼‰)
        # å¦‚æœä½¿ç”¨è€…è¨­å®šæ¯éƒ¨å½±ç‰‡æ‰¾ 80 å‰‡ï¼Œæˆ‘å€‘æœ€å¤šå…è¨±æ‰¾ 2500 å‰‡ç¸½æ•¸
        GLOBAL_MAX_VIDEOS = 250
        GLOBAL_MAX_COMMENTS = 2500

        with st.spinner("AI é«˜é€Ÿåˆ†æä¸­... è«‹ç¨å€™..."):
            df_result, err = movie_comment_analysis(
                movie_title, str(start_date), str(end_date),
                yt_api_key, deepseek_api_key,
                max_videos, max_comments, sample_size,
                relax_trad_filter=relax_trad_filter,
                cantonese_threshold=cantonese_threshold,
                auto_relax_threshold=auto_relax_threshold,
                target_min_cantonese=target_min_cantonese,
                prefer_hk_videos=prefer_hk_videos,
                max_total_videos_global=GLOBAL_MAX_VIDEOS,
                max_total_comments_global=GLOBAL_MAX_COMMENTS
            )

        if err:
            st.error(err)
        else:
            st.success("åˆ†æå®Œæˆï¼")
            st.dataframe(df_result.head(20), use_container_width=True)

            st.header("ğŸ“Š å¯è¦–åŒ–åˆ†æçµæœ")

            sentiments_order = ['Positive', 'Negative', 'Neutral', 'Invalid', 'Error']
            colors_map = {
                'Positive': '#5cb85c', 'Negative': '#d9534f', 'Neutral': '#f0ad4e',
                'Invalid': '#cccccc', 'Error': '#888888'
            }

            # 1. æƒ…æ„Ÿåˆ†ä½ˆ
            st.subheader("1. Sentiment Distribution (Pie)")
            sentiment_series = df_result['sentiment'].dropna().astype(str)
            sentiment_counts = sentiment_series.value_counts()
            ordered_labels = [label for label in sentiments_order if label in sentiment_counts.index]
            if not sentiment_counts.empty:
                fig1 = px.pie(
                    values=sentiment_counts[ordered_labels].values,
                    names=ordered_labels,
                    title='Overall Sentiment Distribution',
                    color=ordered_labels,
                    color_discrete_map=colors_map
                )
                st.plotly_chart(fig1, use_container_width=True)
            else:
                st.info("No sentiment data available for pie chart.")

            # 2. æ¯æ—¥æƒ…æ„Ÿè¶¨å‹¢
            st.subheader("2. Daily Sentiment Trend")
            if 'published_at_hk' in df_result.columns:
                df_result['date'] = df_result['published_at_hk'].dt.date
            else:
                df_result['date'] = pd.to_datetime(df_result['published_at'], utc=True).dt.tz_convert('Asia/Hong_Kong').dt.date

            daily = df_result.groupby(['date', 'sentiment']).size().unstack().fillna(0)
            daily = daily.reindex(columns=sentiments_order).dropna(axis=1, how='all')
            if not daily.empty:
                daily_long = daily.reset_index().melt(id_vars='date', var_name='sentiment', value_name='count')
                fig_line = px.line(
                    daily_long, x='date', y='count', color='sentiment',
                    title='Daily Comment Volume Trend by Sentiment',
                    labels={'date': 'Date', 'count': 'Number of Comments', 'sentiment': 'Sentiment'},
                    color_discrete_map=colors_map
                )
                st.plotly_chart(fig_line, use_container_width=True)

                fig_bar = px.bar(
                    daily_long, x='date', y='count', color='sentiment',
                    title='Daily Comment Volume by Sentiment (Stacked)',
                    labels={'date': 'Date', 'count': 'Number of Comments', 'sentiment': 'Sentiment'},
                    color_discrete_map=colors_map,
                    barmode='stack'
                )
                st.plotly_chart(fig_bar, use_container_width=True)
            else:
                st.info("Not enough daily sentiment data to display the trend charts.")

            # 3. å„ä¸»é¡Œæƒ…æ„Ÿä½”æ¯”
            st.subheader("3. Sentiment Share by Topic")
            topic_sentiment = df_result.groupby(['topic', 'sentiment']).size().unstack().fillna(0)
            topic_sentiment = topic_sentiment.reindex(columns=sentiments_order).dropna(axis=1, how='all')
            if not topic_sentiment.empty:
                topic_sentiment = topic_sentiment[topic_sentiment.sum(axis=1) > 0]
                if not topic_sentiment.empty:
                    topic_sentiment_percent = topic_sentiment.div(topic_sentiment.sum(axis=1), axis=0).fillna(0) * 100
                    fig3 = px.bar(
                        topic_sentiment_percent.reset_index().melt(id_vars='topic', var_name='sentiment', value_name='pct'),
                        x='topic', y='pct', color='sentiment',
                        title='Sentiment Share by Topic',
                        labels={'topic': 'Topic', 'pct': 'Percentage (%)', 'sentiment': 'Sentiment'},
                        color_discrete_map=colors_map
                    )
                    st.plotly_chart(fig3, use_container_width=True)
                else:
                    st.info("No topic data with comments to display the chart.")
            else:
                st.info("Not enough topic sentiment data to display the stacked bar chart.")

            # 4. ä¸‹è¼‰åˆ†ææ˜ç´°
            st.subheader("4. ä¸‹è¼‰åˆ†ææ˜ç´°")
            csv = df_result.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                "ğŸ“¥ ä¸‹è¼‰å…¨éƒ¨åˆ†ææ˜ç´° (CSV)",
                csv,
                file_name=f"{movie_title}_hk_cantonese_analysis.csv",
                mime='text/csv'
            )
