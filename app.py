import streamlit as st
import pandas as pd
import numpy as np
import googleapiclient.discovery
from openai import AsyncOpenAI, OpenAIError
import asyncio
from tqdm.asyncio import tqdm as tqdm_asyncio
from datetime import datetime, timedelta
import plotly.express as px
from langdetect import detect, LangDetectException
import json

# --- Constants and Configuration ---

# <<< æ–°å¢ï¼šç¹ç°¡é«”ä¸­æ–‡ç‰¹å¾µå­—åˆ¤æ–· >>>
# å»ºç«‹å¸¸ç”¨ä¸”æœ‰ä»£è¡¨æ€§çš„ç¹ç°¡å°ˆç”¨å­—é›†åˆï¼Œç”¨æ–¼åˆ¤æ–·æ–‡æœ¬é¡å‹
# é€™äº›åˆ—è¡¨ä¸éœ€è¦éå¸¸è©³ç›¡ï¼Œæœ‰å¹¾åå€‹å¸¸ç”¨å­—å°±èƒ½é”åˆ°å¾ˆé«˜çš„æº–ç¢ºç‡
TC_UNIQUE_CHARS = set("æ­é«”åœ‹ç™¼è¦‹ç„¡éº¼è£¡è˜‹èªåŠƒè®šè£¡éºµ")
SC_UNIQUE_CHARS = set("æ¬§ä½“å›½å‘è§æ— ä¹ˆé‡Œè‹¹è¯­åˆ’èµé‡Œä¸‹é¢")

# --- Helper Functions ---

def detect_chinese_variant(text: str) -> str:
    """
    ä½¿ç”¨å–®ä¸€å­—å…ƒç‰¹å¾µåˆ¤æ–·æ–‡æœ¬æ˜¯ç¹é«”ã€ç°¡é«”æˆ–æ··åˆã€‚
    :param text: è¼¸å…¥çš„å­—ä¸²ã€‚
    :return: "ç¹é«”ä¸­æ–‡", "ç°¡é«”ä¸­æ–‡", æˆ– "æ··åˆ/æœªçŸ¥"ã€‚
    """
    if not isinstance(text, str) or not text.strip():
        return "æ··åˆ/æœªçŸ¥"

    tc_count = 0
    sc_count = 0

    for char in text:
        if char in TC_UNIQUE_CHARS:
            tc_count += 1
        elif char in SC_UNIQUE_CHARS:
            sc_count += 1
    
    # æ ¹æ“šå‘½ä¸­å°ˆç”¨å­—çš„æ•¸é‡ä¾†åˆ¤æ–·
    # å¢åŠ ä¸€å€‹å°å°çš„æ¬Šé‡ï¼Œé¿å…å› å–®ä¸€å­—å…ƒèª¤åˆ¤
    if tc_count > sc_count:
        return "ç¹é«”ä¸­æ–‡"
    elif sc_count > tc_count:
        return "ç°¡é«”ä¸­æ–‡"
    else:
        # å¦‚æœæ•¸é‡ç›¸ç­‰ï¼ˆåŒ…æ‹¬éƒ½ç‚º0çš„æƒ…æ³ï¼‰ï¼Œå‰‡è¦–ç‚ºæ··åˆæˆ–ç„¡æ³•åˆ¤æ–·
        return "æ··åˆ/æœªçŸ¥"

def is_chinese(text: str) -> bool:
    """
    ä½¿ç”¨ langdetect æª¢æ¸¬æ–‡æœ¬æ˜¯å¦ç‚ºä¸­æ–‡ã€‚
    """
    try:
        # åªæª¢æ¸¬ 'zh-cn' å’Œ 'zh-tw'
        lang = detect(text)
        return lang in ['zh-cn', 'zh-tw']
    except LangDetectException:
        # å¦‚æœ langdetect ç„¡æ³•è­˜åˆ¥ï¼ˆä¾‹å¦‚ï¼Œç´”è¡¨æƒ…ç¬¦è™Ÿæˆ–å¤ªçŸ­ï¼‰ï¼Œå‰‡è¦–ç‚ºéä¸­æ–‡
        return False
    except Exception:
        # è™•ç†å…¶ä»–æ½›åœ¨éŒ¯èª¤
        return False

def get_video_comments(youtube, video_id, max_comments):
    """ç²å–å–®ä¸€å½±ç‰‡çš„ç•™è¨€ã€‚"""
    all_comments = []
    try:
        request = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            maxResults=min(100, max_comments),
            textFormat="plainText"
        )
        
        while request and len(all_comments) < max_comments:
            response = request.execute()
            for item in response["items"]:
                comment = item["snippet"]["topLevelComment"]["snippet"]
                all_comments.append({
                    "author": comment["authorDisplayName"],
                    "publishedAt": comment["publishedAt"],
                    "textDisplay": comment["textDisplay"]
                })
                if len(all_comments) >= max_comments:
                    break
            
            if 'nextPageToken' in response and len(all_comments) < max_comments:
                request = youtube.commentThreads().list_next(previous_request=request, previous_response=response)
            else:
                break
    except Exception as e:
        st.warning(f"ç„¡æ³•ç²å–å½±ç‰‡ ID {video_id} çš„ç•™è¨€: {e}", icon="âš ï¸")
    return all_comments

def search_videos(youtube, query, start_date, end_date, max_videos):
    """æ ¹æ“šé—œéµå­—å’Œæ—¥æœŸç¯„åœæœç´¢å½±ç‰‡ã€‚"""
    all_videos = []
    request = youtube.search().list(
        q=query,
        part="snippet",
        type="video",
        maxResults=min(50, max_videos),
        publishedAfter=start_date + "T00:00:00Z",
        publishedBefore=end_date + "T23:59:59Z"
    )

    while request and len(all_videos) < max_videos:
        response = request.execute()
        for item in response['items']:
            all_videos.append({
                'videoId': item['id']['videoId'],
                'title': item['snippet']['title'],
                'publishedAt': item['snippet']['publishedAt']
            })
            if len(all_videos) >= max_videos:
                break
        
        if 'nextPageToken' in response and len(all_videos) < max_videos:
            request = youtube.search().list_next(previous_request=request, previous_response=response)
        else:
            break
            
    return all_videos

# --- AI Analysis Functions ---

# å®šç¾©ä¸€å€‹ Semaphoreï¼Œä¾‹å¦‚ï¼Œä¸€æ¬¡æœ€å¤šåªå…è¨± 10 å€‹ä¸¦è¡Œè«‹æ±‚
SEMAPHORE = asyncio.Semaphore(10)

async def analyze_comment_async(comment: str, client: AsyncOpenAI):
    """
    ä½¿ç”¨ Semaphore åŒ…è£¹çš„éåŒæ­¥å‡½å¼ï¼Œç”¨æ–¼åˆ†æå–®ä¸€ç•™è¨€ã€‚
    """
    async with SEMAPHORE:
        if not comment or not comment.strip():
            return {"sentiment": "neutral", "positive": 0, "negative": 0, "neutral": 1, "reason": "ç•™è¨€ç‚ºç©º"}

        try:
            response = await client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹æƒ…æ„Ÿåˆ†æå°ˆå®¶ã€‚è«‹å°‡ä»¥ä¸‹è©•è«–åˆ†é¡ç‚º'positive', 'negative', æˆ– 'neutral'ã€‚è«‹åªç”¨JSONæ ¼å¼å›ç­”ï¼ŒåŒ…å«'sentiment'å’Œ'reason'å…©å€‹éµã€‚"},
                    {"role": "user", "content": comment}
                ],
                temperature=0,
                max_tokens=100,
                response_format={"type": "json_object"}
            )
            analysis_text = response.choices[0].message.content
            analysis_json = json.loads(analysis_text)
            
            sentiment = analysis_json.get("sentiment", "neutral").lower()
            
            # ç¢ºä¿æƒ…æ„Ÿæ˜¯ä¸‰ç¨®é¡å‹ä¹‹ä¸€
            if sentiment not in ["positive", "negative", "neutral"]:
                sentiment = "neutral"

            return {
                "sentiment": sentiment,
                "positive": 1 if sentiment == "positive" else 0,
                "negative": 1 if sentiment == "negative" else 0,
                "neutral": 1 if sentiment == "neutral" else 0,
                "reason": analysis_json.get("reason", "")
            }
        except OpenAIError as e:
            return {"sentiment": "error", "positive": 0, "negative": 0, "neutral": 0, "reason": f"API éŒ¯èª¤: {e}"}
        except (json.JSONDecodeError, TypeError):
            return {"sentiment": "error", "positive": 0, "negative": 0, "neutral": 0, "reason": "ç„¡æ³•è§£æAIå›æ‡‰"}
        except Exception as e:
            return {"sentiment": "error", "positive": 0, "negative": 0, "neutral": 0, "reason": f"æœªçŸ¥éŒ¯èª¤: {e}"}

async def run_all_analyses(df: pd.DataFrame, client: AsyncOpenAI):
    """ä½¿ç”¨ asyncio.gather å’Œ Semaphore åŸ·è¡Œæ‰€æœ‰ç•™è¨€çš„æƒ…æ„Ÿåˆ†æã€‚"""
    tasks = [analyze_comment_async(comment, client) for comment in df['textDisplay']]
    
    analysis_results = await tqdm_asyncio.gather(
        *tasks, 
        desc="AI Sentiment Analysis (Concurrent)"
    )
    
    return analysis_results

# --- Visualization Functions ---

def create_sunburst_chart(df: pd.DataFrame):
    """
    å‰µå»ºä¸€å€‹ Plotly æ—­æ—¥åœ–ï¼Œé¡¯ç¤ºæƒ…æ„Ÿå’Œèªè¨€è®Šé«”çš„å±¤ç´šåˆ†ä½ˆã€‚
    """
    # ç¢ºä¿æƒ…æ„Ÿå’Œèªè¨€è®Šé«”æ¬„ä½å­˜åœ¨
    if 'sentiment' not in df.columns or 'script_variant' not in df.columns:
        st.warning("ç¼ºå°‘ 'sentiment' æˆ– 'script_variant' æ¬„ä½ï¼Œç„¡æ³•ç”Ÿæˆæ—­æ—¥åœ–ã€‚")
        return None

    # è™•ç† 'error' æƒ…æ„Ÿï¼Œå°‡å…¶æ­¸é¡ç‚º 'neutral' ä»¥ä¾¿æ–¼è¦–è¦ºåŒ–
    df_plot = df.copy()
    df_plot['sentiment'] = df_plot['sentiment'].replace('error', 'neutral')
    
    # å‰µå»ºæ—­æ—¥åœ–
    fig = px.sunburst(
        df_plot,
        path=['sentiment', 'script_variant'], # <<< ä¿®æ”¹ï¼šå¢åŠ å±¤ç´š
        title="æƒ…æ„Ÿèˆ‡èªè¨€è®Šé«”åˆ†ä½ˆæ—­æ—¥åœ–",
        color='sentiment',
        color_discrete_map={
            'positive': '#2ca02c', # ç¶ è‰²
            'negative': '#d62728', # ç´…è‰²
            'neutral': '#7f7f7f'   # ç°è‰²
        }
    )
    fig.update_traces(textinfo="label+percent parent")
    fig.update_layout(margin=dict(t=50, l=10, r=10, b=10))
    return fig

# --- Main Application Logic ---

@st.cache_data(ttl=3600)
def movie_comment_analysis(movie_title, start_date, end_date, yt_api_key, deepseek_api_key, max_videos, max_comments, sample_size):
    try:
        youtube = googleapiclient.discovery.build("youtube", "v3", developerKey=yt_api_key)
        
        # 1. æœç´¢å½±ç‰‡
        videos = search_videos(youtube, f"{movie_title} é å‘Š", start_date, end_date, max_videos)
        if not videos:
            return None, "åœ¨æŒ‡å®šæ—¥æœŸç¯„åœå…§æ‰¾ä¸åˆ°ç›¸é—œå½±ç‰‡ã€‚"

        # 2. ç²å–æ‰€æœ‰å½±ç‰‡çš„ç•™è¨€
        all_comments = []
        for video in videos:
            comments = get_video_comments(youtube, video['videoId'], max_comments)
            all_comments.extend(comments)
        
        if not all_comments:
            return None, "æ‰¾åˆ°äº†å½±ç‰‡ï¼Œä½†ç„¡æ³•ç²å–ä»»ä½•ç•™è¨€ã€‚"

        df_comments = pd.DataFrame(all_comments)
        df_comments.drop_duplicates(subset=['textDisplay'], inplace=True)

        # 3. èªè¨€æª¢æ¸¬èˆ‡ç¯©é¸
        st.write(f"åˆæ­¥ç²å– {len(df_comments)} å‰‡ä¸é‡è¤‡ç•™è¨€ï¼Œé–‹å§‹é€²è¡Œèªè¨€ç¯©é¸...")
        df_comments['is_chinese'] = df_comments['textDisplay'].apply(is_chinese)
        df_chinese = df_comments[df_comments['is_chinese']].copy()
        
        if df_chinese.empty:
            return None, "éæ¿¾å¾Œæ²’æœ‰æ‰¾åˆ°ä»»ä½•ä¸­æ–‡ç•™è¨€ã€‚"
        
        # <<< æ–°å¢ï¼šæ‡‰ç”¨ç¹ç°¡é«”åˆ¤æ–·å‡½å¼ >>>
        df_chinese['script_variant'] = df_chinese['textDisplay'].apply(detect_chinese_variant)

        # 4. ç•™è¨€æŠ½æ¨£
        num_to_analyze = min(len(df_chinese), sample_size)
        df_analyze = df_chinese.sample(n=num_to_analyze, random_state=42)

        # 5. AI æƒ…æ„Ÿåˆ†æ
        if not deepseek_api_key:
            return None, "è«‹åœ¨å·¦å´è¼¸å…¥ DeepSeek API é‡‘é‘°ä»¥é€²è¡Œæƒ…æ„Ÿåˆ†æã€‚"
            
        deepseek_client = AsyncOpenAI(api_key=deepseek_api_key, base_url="https://api.deepseek.com/v1")
        
        analysis_results = asyncio.run(run_all_analyses(df_analyze, deepseek_client))
        
        df_analysis_results = pd.DataFrame(analysis_results)
        
        # 6. åˆä½µçµæœ
        df_result = df_analyze.reset_index(drop=True).join(df_analysis_results)
        
        return df_result, None

    except Exception as e:
        return None, f"ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}"

# --- Streamlit UI ---

st.set_page_config(page_title="é›»å½±ç¤¾ç¾¤å£ç¢‘åˆ†æå™¨", layout="wide")
st.title("ğŸ¬ é›»å½±ç¤¾ç¾¤å£ç¢‘åˆ†æå™¨")
st.markdown("è¼¸å…¥é›»å½±åç¨±ï¼Œæœ¬å·¥å…·å°‡è‡ªå‹•å¾ YouTube æŠ“å–ç›¸é—œé å‘Šç‰‡çš„ç•™è¨€ï¼Œä¸¦ä½¿ç”¨ AI é€²è¡Œæƒ…æ„Ÿåˆ†æï¼Œå¹«åŠ©æ‚¨å¿«é€Ÿäº†è§£å¤§çœ¾å£ç¢‘ã€‚")

# Sidebar for inputs
with st.sidebar:
    st.header("âš™ï¸ åˆ†æè¨­å®š")
    
    yt_api_key = st.text_input("Google (YouTube) API Key", type="password", help="è«‹è¼¸å…¥æ‚¨çš„ YouTube Data API v3 é‡‘é‘°ã€‚")
    deepseek_api_key = st.text_input("DeepSeek API Key", type="password", help="è«‹è¼¸å…¥æ‚¨çš„ DeepSeek API é‡‘é‘°ã€‚")

    movie_title = st.text_input("é›»å½±åç¨±", "æ²™ä¸˜")
    
    today = datetime.now()
    one_month_ago = today - timedelta(days=30)
    
    col1, col2 = st.columns(2)
    with col1:
        start_date = st.date_input("é–‹å§‹æ—¥æœŸ", one_month_ago)
    with col2:
        end_date = st.date_input("çµæŸæ—¥æœŸ", today)

    st.subheader("è³‡æ–™é‡æ§åˆ¶")
    max_videos = st.slider("æœ€å¤§å½±ç‰‡æœç´¢æ•¸é‡", 1, 50, 5, help="å¾ YouTube æœç´¢å¤šå°‘éƒ¨ç›¸é—œå½±ç‰‡ä¾†æŠ“å–ç•™è¨€ã€‚")
    max_comments = st.slider("æ¯éƒ¨å½±ç‰‡æœ€å¤§ç•™è¨€æ•¸", 50, 500, 100, help="å¾æ¯éƒ¨å½±ç‰‡ä¸­æœ€å¤šæŠ“å–å¤šå°‘å‰‡ç•™è¨€ã€‚")
    sample_size = st.slider("AI åˆ†ææ¨£æœ¬æ•¸", 50, 1000, 200, help="å¾æ‰€æœ‰ä¸­æ–‡ç•™è¨€ä¸­éš¨æ©ŸæŠ½å–å¤šå°‘å‰‡é€²è¡Œ AI æƒ…æ„Ÿåˆ†æã€‚")

    analyze_button = st.button("ğŸš€ é–‹å§‹åˆ†æ", use_container_width=True, type="primary")

if analyze_button:
    if not yt_api_key or not deepseek_api_key:
        st.error("è«‹å‹™å¿…åœ¨å·¦å´è¨­å®šä¸­è¼¸å…¥ YouTube å’Œ DeepSeek çš„ API é‡‘é‘°ï¼")
    else:
        with st.spinner("æ­£åœ¨åŸ·è¡Œåˆ†æï¼Œè«‹ç¨å€™... (å¯èƒ½éœ€è¦å¹¾åˆ†é˜)"):
            df_result, err = movie_comment_analysis(
                movie_title, str(start_date), str(end_date),
                yt_api_key, deepseek_api_key,
                max_videos, max_comments, sample_size
            )

        if err:
            st.error(f"åˆ†æå¤±æ•—ï¼š{err}")
        elif df_result is not None:
            st.success("åˆ†æå®Œæˆï¼")
            
            # --- çµæœå±•ç¤º ---
            total_comments = len(df_result)
            positive_count = df_result['positive'].sum()
            negative_count = df_result['negative'].sum()
            neutral_count = df_result['neutral'].sum()
            error_count = (df_result['sentiment'] == 'error').sum()

            st.header("ğŸ“Š æ•´é«”æƒ…æ„Ÿåˆ†ä½ˆ")
            
            # æŒ‡æ¨™å¡
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("æ­£é¢ç•™è¨€ ğŸ‘", f"{positive_count}", f"{positive_count/total_comments:.1%}")
            col2.metric("è² é¢ç•™è¨€ ğŸ‘", f"{negative_count}", f"-{negative_count/total_comments:.1%}")
            col3.metric("ä¸­æ€§ç•™è¨€ ğŸ˜", f"{neutral_count}", f"{neutral_count/total_comments:.1%}")
            col4.metric("åˆ†æå¤±æ•—æ•¸ âŒ", f"{error_count}", "æ‡‰ç‚º 0")

            # è¦–è¦ºåŒ–åœ–è¡¨
            sunburst_fig = create_sunburst_chart(df_result)
            if sunburst_fig:
                st.plotly_chart(sunburst_fig, use_container_width=True)

            # --- ç•™è¨€è©³æƒ…èˆ‡ç¯©é¸ ---
            st.header("ğŸ“œ ç•™è¨€è©³æƒ…")

            # <<< æ–°å¢ï¼šèªè¨€è®Šé«”ç¯©é¸å™¨ >>>
            variant_options = ["å…¨éƒ¨"] + df_result['script_variant'].unique().tolist()
            selected_variant = st.selectbox("ç¯©é¸èªè¨€è®Šé«”:", options=variant_options)

            # æ ¹æ“šé¸æ“‡é€²è¡Œç¯©é¸
            if selected_variant == "å…¨éƒ¨":
                df_display = df_result
            else:
                df_display = df_result[df_result['script_variant'] == selected_variant]

            # é¡¯ç¤ºç¯©é¸å¾Œçš„è³‡æ–™
            st.dataframe(df_display[[
                'sentiment', 
                'script_variant', # <<< æ–°å¢ï¼šé¡¯ç¤ºèªè¨€è®Šé«”æ¬„
                'textDisplay', 
                'reason', 
                'author', 
                'publishedAt'
            ]], use_container_width=True)
            
            st.info(f"å…±é¡¯ç¤º {len(df_display)} / {total_comments} å‰‡åˆ†æå¾Œçš„ç•™è¨€ã€‚")
