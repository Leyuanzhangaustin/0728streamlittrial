import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import time
import asyncio
import openai
import re
import json
from googleapiclient.discovery import build
from collections import Counter

# ... (å‰é¢çš„ç·©å­˜å‡½æ•¸ _get_cached_value, _set_cached_value ä¿æŒä¸è®Š) ...

# =========================
# 0. å·¥å…·è¨­å®š (æ›´æ–°)
# =========================

CACHE_TTL_SEARCH = 3600
CACHE_TTL_COMMENTS = 900

def _get_cached_value(cache_name: str, key, ttl_seconds: int):
    cache = st.session_state.setdefault(cache_name, {})
    entry = cache.get(key)
    if entry and (time.time() - entry["ts"] <= ttl_seconds):
        return entry["value"]
    return None

def _set_cached_value(cache_name: str, key, value):
    cache = st.session_state.setdefault(cache_name, {})
    cache[key] = {"value": value, "ts": time.time()}

def generate_search_queries(movie_title: str):
    return [
        f"{movie_title}",
        f"{movie_title} å½±è©•",
        f"{movie_title} è§€å¾Œæ„Ÿ",
        f"{movie_title} review",
        f"{movie_title} é›»å½±"
    ]

# =========================
# 1. YouTube API æ ¸å¿ƒ (å«æ”¿æ²»/ç„¡é—œå…§å®¹éæ¿¾)
# =========================

def search_youtube_videos_smart(
    keywords, youtube_client, movie_title,
    max_per_keyword, max_total_videos,
    start_date, end_date,
    negative_keywords_list  # æ–°å¢ï¼šè² é¢é—œéµè©åˆ—è¡¨
):
    all_video_ids = set()
    video_meta = {}
    search_cache_name = "yt_search_smart_filtered_cache"
    
    progress_text = "æ­£åœ¨æœå°‹ä¸¦éæ¿¾ç„¡é—œ/æ”¿æ²»å½±ç‰‡..."
    my_bar = st.progress(0, text=progress_text)
    
    # 1. æº–å‚™æ­£å‘é—œéµè© (é›»å½±åæ‹†åˆ†)
    # ä¾‹å¦‚ "ä¹é¾åŸå¯¨ä¹‹åœåŸ" -> ["ä¹é¾åŸå¯¨", "åœåŸ"]
    title_keywords = [k for k in re.split(r'\s+|ï¼š|:|,|ï¼Œ', movie_title) if len(k) > 1]
    
    # 2. æº–å‚™è² é¢é—œéµè© (ç¡¬ç·¨ç¢¼åŸºç¤ + ç”¨æˆ¶è¼¸å…¥)
    # é€™äº›è©å‡ºç¾åœ¨æ¨™é¡Œä¸­é€šå¸¸ä»£è¡¨æ˜¯æ™‚æ”¿æ–°èè€Œéå½±è©•
    base_negative_keywords = [
        "æ–°è", "ç›´æ’­", "æ–½æ”¿å ±å‘Š", "ç¿’è¿‘å¹³", "ä¸­å…±", "å…±ç”¢é»¨", 
        "ç‰¹é¦–", "æå®¶è¶…", "ç«‹æ³•æœƒ", "ç¤ºå¨", "æ”¿æ²»", "æ”¿ç¶“", 
        "å¤§ç´€å…ƒ", "æ–‡æ˜­", "æ±Ÿå³°", "å¤©äº®æ™‚åˆ†", "æ™‚äº‹", "è²¡ç¶“"
    ]
    # åˆä½µç”¨æˆ¶å®šç¾©çš„æ’é™¤è©
    final_negative_keywords = list(set(base_negative_keywords + negative_keywords_list))

    for idx, query in enumerate(keywords):
        if len(all_video_ids) >= max_total_videos: break
            
        cache_key = f"{query}_{start_date}_{end_date}_{max_per_keyword}_filtered"
        cached_records = _get_cached_value(search_cache_name, cache_key, CACHE_TTL_SEARCH)
        
        query_records = []
        
        if cached_records is None:
            try:
                request = youtube_client.search().list(
                    q=query, part="id,snippet", type="video", maxResults=max_per_keyword,
                    publishedAfter=f"{start_date}T00:00:00Z", publishedBefore=f"{end_date}T23:59:59Z",
                    order="relevance", safeSearch="none", relevanceLanguage="zh-Hant"
                )
                response = request.execute()
                for item in response.get("items", []):
                    vid = item["id"]["videoId"]
                    snip = item.get("snippet", {})
                    title = snip.get("title", "")
                    desc = snip.get("description", "")
                    channel_title = snip.get("channelTitle", "")
                    
                    # === æ ¸å¿ƒä¿®æ”¹ï¼šé›™é‡éæ¿¾é‚è¼¯ ===
                    
                    # A. è² é¢éæ¿¾ (Negative Filter) - å„ªå…ˆç´šæœ€é«˜
                    # å¦‚æœæ¨™é¡Œæˆ–é »é“ååŒ…å«æ”¿æ²»æ•æ„Ÿè©ï¼Œç›´æ¥è·³é
                    if any(nk in title for nk in final_negative_keywords) or \
                       any(nk in channel_title for nk in final_negative_keywords):
                        continue 

                    # B. æ­£å‘ç›¸é—œæ€§æª¢æŸ¥ (Positive Relevance)
                    is_relevant = False
                    
                    # B1. æ¨™é¡Œå¿…é ˆåŒ…å«è‡³å°‘ä¸€å€‹é›»å½±æ ¸å¿ƒè©
                    # é€™æ˜¯ç‚ºäº†é˜²æ­¢ YouTube æ¨é€å®Œå…¨ç„¡é—œçš„ "çŒœä½ å–œæ­¡"
                    if any(tk.lower() in title.lower() for tk in title_keywords):
                        is_relevant = True
                    
                    # B2. å¦‚æœæ¨™é¡Œæ²’æœ‰æ ¸å¿ƒè©ï¼Œä½†æè¿°è£¡æœ‰å®Œæ•´é›»å½±åï¼Œä¹Ÿå¯ä»¥æ¥å— (é˜²æ­¢æ¨™é¡Œé»¨)
                    elif movie_title.lower() in desc.lower():
                        is_relevant = True
                        
                    if is_relevant:
                        query_records.append({
                            "id": vid,
                            "title": title,
                            "channelTitle": channel_title,
                            "publishedAt": snip.get("publishedAt", "")
                        })
                
                _set_cached_value(search_cache_name, cache_key, query_records)
            except Exception as e:
                st.warning(f"æœå°‹ '{query}' å¤±æ•—: {e}")
        else:
            query_records = cached_records

        for rec in query_records:
            if rec["id"] not in all_video_ids:
                all_video_ids.add(rec["id"])
                video_meta[rec["id"]] = rec
                if len(all_video_ids) >= max_total_videos: break
        
        my_bar.progress((idx + 1) / len(keywords), text=f"æœå°‹ä¸­... å·²éæ¿¾æ”¿æ²»/ç„¡é—œå…§å®¹ï¼Œä¿ç•™: {len(all_video_ids)} éƒ¨")

    my_bar.empty()
    return list(all_video_ids), video_meta

# ... (get_all_comments_cached å‡½æ•¸ä¿æŒä¸è®Šï¼Œä½¿ç”¨ä¸Šä¸€ç‰ˆçš„å‹•æ…‹èª¿æ•´é‚è¼¯) ...
def get_all_comments_cached(video_ids, youtube_client, max_per_video, max_total_comments, video_meta):
    all_comments = []
    comments_cache_name = "yt_comments_cache_v4" # Update version
    
    progress_bar = st.progress(0, text="æŠ“å–ç•™è¨€ä¸­...")
    
    # å‹•æ…‹èª¿æ•´ï¼šå½±ç‰‡å°‘å‰‡æŠ“æ›´å¤šè©•è«–
    if len(video_ids) > 0 and len(video_ids) < 5:
        adjusted_max_per_video = max_per_video * 4 # æå‡å€ç‡
        st.caption(f"âš ï¸ ç¶“éæ¿¾å¾Œå½±ç‰‡ä¾†æºè¼ƒå°‘ï¼Œè‡ªå‹•å°‡å–®ä¸€å½±ç‰‡ç•™è¨€æŠ“å–ä¸Šé™å¤§å¹…æå‡è‡³ {adjusted_max_per_video} å‰‡")
    else:
        adjusted_max_per_video = max_per_video

    for i, vid in enumerate(video_ids):
        if len(all_comments) >= max_total_comments: break

        cache_key = f"comments_{vid}_{adjusted_max_per_video}"
        cached_comments = _get_cached_value(comments_cache_name, cache_key, CACHE_TTL_COMMENTS)
        
        video_comments = []
        if cached_comments is not None:
            video_comments = cached_comments
        else:
            try:
                request = youtube_client.commentThreads().list(
                    part="snippet", videoId=vid, textFormat="plainText",
                    order="relevance", maxResults=min(100, adjusted_max_per_video)
                )
                response = request.execute()
                for item in response.get("items", []):
                    if len(video_comments) >= adjusted_max_per_video: break
                    comm = item["snippet"]["topLevelComment"]["snippet"]
                    video_comments.append({
                        "comment_text": comm.get("textDisplay", ""),
                        "published_at": comm.get("publishedAt", ""),
                        "like_count": comm.get("likeCount", 0)
                    })
                _set_cached_value(comments_cache_name, cache_key, video_comments)
            except: pass
        
        title = video_meta.get(vid, {}).get("title", "")
        for c in video_comments:
            c_copy = c.copy()
            c_copy.update({"video_id": vid, "video_title": title})
            all_comments.append(c_copy)
            if len(all_comments) >= max_total_comments: break
            
        progress_bar.progress((i + 1) / len(video_ids), text=f"æŠ“å–ç•™è¨€... ({len(all_comments)}/{max_total_comments})")

    progress_bar.empty()
    return pd.DataFrame(all_comments)

# =========================
# 2. DeepSeek åˆ†æ (æ”¾å®½æ ‡å‡† & è°ƒè¯•ç‰ˆ)
# =========================

async def analyze_comment_deepseek_v2(row, client, semaphore):
    """
    å–®å€‹è©•è«–åˆ†æå‡½æ•¸
    """
    text = row["comment_text"]
    video_title = row.get("video_title", "")
    
    # === ä¿®æ”¹ 1: æ”¾å¯¬ System Promptï¼Œä¸¦è¦æ±‚è¿”å› reason ===
    system_prompt = (
        "You are a lenient movie analyst for the Hong Kong market. "
        f"The comment is from a video: '{video_title}'. "
        "Analyze the comment. "
        "Output JSON with keys: "
        "'sentiment' (Positive/Negative/Neutral), "
        "'keywords' (Extract 1-2 main keywords), "
        "'is_target_audience' (boolean), "
        "'reason' (Short string explaining why is_target_audience is true/false). "
        "\n\n"
        "Rules for 'is_target_audience' (BE LENIENT):\n"
        "1. **TRUE** if it expresses ANY opinion about the movie, actors, or the video content.\n"
        "2. **TRUE** even if it is Simplified Chinese, as long as it is readable.\n"
        "3. **TRUE** even if it mentions politics, AS LONG AS it relates to the movie's themes or context.\n"
        "4. **TRUE** for short comments like 'Good', 'Bad', 'Support'.\n"
        "5. **FALSE** ONLY if it is clearly spam, advertisements, or completely gibberish/unrelated nonsense."
    )

    async with semaphore:
        try:
            response = await client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": text}
                ],
                response_format={"type": "json_object"},
                temperature=0.1,
                timeout=20 
            )
            content = response.choices[0].message.content
            if not content:
                # å¦‚æœå…§å®¹ç‚ºç©ºï¼Œé»˜èªä¿ç•™ (True)ï¼Œä»¥å…èª¤åˆª
                return {"sentiment": "Neutral", "keywords": "Empty Response", "is_target_audience": True, "reason": "API returned empty"}
            
            result = json.loads(content)
            
            # === ä¿®æ”¹ 2: å¼·åˆ¶é¡å‹è½‰æ›ï¼Œé˜²æ­¢ AI è¿”å›å­—ç¬¦ä¸² "true" å°è‡´éæ¿¾å¤±æ•— ===
            is_target = result.get("is_target_audience", True)
            if isinstance(is_target, str):
                is_target = is_target.lower() == 'true'
            result["is_target_audience"] = is_target
            
            # ç¢ºä¿æœ‰ reason å­—æ®µ
            if "reason" not in result:
                result["reason"] = "No reason provided"
                
            return result

        except json.JSONDecodeError:
            # JSON è§£æå¤±æ•—ï¼Œé€šå¸¸æ˜¯å› ç‚º AI è¼¸å‡ºäº†å¤šé¤˜æ–‡å­—ï¼Œæˆ‘å€‘é¸æ“‡ä¿ç•™é€™æ¢è©•è«–äººå·¥æŸ¥çœ‹
            return {"sentiment": "Neutral", "keywords": "JSON Error", "is_target_audience": True, "reason": "JSON Parse Error"}
        except Exception as e:
            # å…¶ä»–éŒ¯èª¤ (ç¶²çµ¡ç­‰)ï¼Œä¹Ÿé»˜èªä¿ç•™ (True)
            return {"sentiment": "Error", "keywords": "System Error", "is_target_audience": True, "reason": f"Error: {str(e)}"}

async def run_deepseek_analysis(df, api_key):
    """
    ä¸»ç•°æ­¥æ§åˆ¶å™¨
    """
    client = openai.AsyncOpenAI(api_key=api_key, base_url="https://api.deepseek.com/v1")
    
    # é™ä½ä½µç™¼æ•¸ä¿è­‰ç©©å®šæ€§
    semaphore = asyncio.Semaphore(10) 
    rows = df.to_dict('records')
    
    progress_bar = st.progress(0, text="AI æ­£åœ¨æ·±åº¦åˆ†æ (å·²æ”¾å¯¬éæ¿¾æ¨™æº–)...")
    
    tasks = [analyze_comment_deepseek_v2(row, client, semaphore) for row in rows]
    
    results = []
    total = len(tasks)
    
    for i, task in enumerate(asyncio.as_completed(tasks)):
        await task 
        progress_bar.progress((i + 1) / total)
    
    results_raw = await asyncio.gather(*tasks, return_exceptions=True)
    
    await client.close()
    progress_bar.empty()
    
    clean_results = []
    for res in results_raw:
        if isinstance(res, Exception):
            # åš´é‡éŒ¯èª¤æ™‚ï¼Œä¿ç•™æ•¸æ“šä¸¦æ¨™è¨˜
            clean_results.append({"sentiment": "Error", "keywords": "Critical Fail", "is_target_audience": True, "reason": str(res)})
        else:
            clean_results.append(res)
            
    return pd.DataFrame(clean_results)
# ... (å‰é¢ä»£ç ä¸å˜) ...
    
    # 3. AI åˆ†æ
    try:
        analysis_df = asyncio.run(run_deepseek_analysis(df_comments, deepseek_api_key))
    except Exception as e:
        return None, f"AI åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}"

    final_df = pd.concat([df_comments, analysis_df], axis=1)
    
    # 4. ç¯©é¸
    original_len = len(final_df)
    
    # ç¢ºä¿æ˜¯å¸ƒæ—å€¼
    final_df["is_target_audience"] = final_df["is_target_audience"].fillna(True).astype(bool)
    
    # === è°ƒè¯•æ¨¡å¼ï¼šå…ˆä¸åˆ é™¤ï¼Œçœ‹çœ‹æ•°æ® ===
    # å¦‚æœä½ å‘ç°è¿˜æ˜¯ 0ï¼Œå¯ä»¥æš‚æ—¶æ³¨é‡Šæ‰ä¸‹é¢è¿™ä¸€è¡Œè¿‡æ»¤ä»£ç ï¼Œçœ‹çœ‹ reason å†™çš„æ˜¯ä»€ä¹ˆ
    final_df_filtered = final_df[final_df["is_target_audience"] == True].copy()
    
    filtered_len = len(final_df_filtered)
    
    if filtered_len == 0 and original_len > 0:
        st.warning("è­¦å‘Šï¼šæ‰€æœ‰è©•è«–éƒ½è¢« AI éæ¿¾æ‰äº†ã€‚ä»¥ä¸‹æ˜¯å‰ 5 æ¢è¢«æ‹’çµ•çš„åŸå› ï¼Œè«‹æª¢æŸ¥ï¼š")
        st.write(final_df[["comment_text", "reason"]].head())
        # ç‚ºäº†è®“ä½ èƒ½çœ‹åˆ°æ•¸æ“šï¼Œé€™ç¨®æƒ…æ³ä¸‹æˆ‘å€‘è¿”å›åŸå§‹æ•¸æ“š
        return final_df, "æ‰€æœ‰æ•¸æ“šè¢«éæ¿¾ï¼Œå·²è¿”å›åŸå§‹æ•¸æ“šä¾›èª¿è©¦ã€‚"

    st.success(f"åˆ†æå®Œæˆï¼åŸå§‹ {original_len} å‰‡ï¼Œæœ‰æ•ˆ {filtered_len} å‰‡ã€‚")
    
    final_df_filtered["published_at"] = pd.to_datetime(final_df_filtered["published_at"])
    
    # è¿”å›åŒ…å« reason çš„æ•¸æ“šï¼Œæ–¹ä¾¿ä½ åœ¨ UI ä¸ŠæŸ¥çœ‹
    return final_df_filtered, None
# =========================
# 4. UI
# =========================

st.set_page_config(page_title="YouTube å½±è©•åˆ†æ (Anti-Spam)", layout="wide")
st.title("ğŸ¬ YouTube å½±è©•åˆ†æ (æ™ºèƒ½éæ¿¾ç‰ˆ)")
st.markdown("### ç‰¹é»ï¼šæ™ºèƒ½æœå°‹ | ğŸš« è‡ªå‹•éæ¿¾æ”¿æ²»/æ–°èå½±ç‰‡ | ç¹é«”/ç²µèªè­˜åˆ¥")

with st.sidebar:
    st.header("è¨­å®š")
    yt_api_key = st.text_input("YouTube API Key", type='password')
    deepseek_api_key = st.text_input("DeepSeek API Key", type='password')
    st.divider()
    max_total_videos = st.number_input("æœ€å¤§å½±ç‰‡æœå°‹æ•¸", 10, 100, 50)
    max_total_comments = st.number_input("æœ€å¤§åˆ†æç•™è¨€æ•¸", 50, 2000, 500)
    
    st.divider()
    st.subheader("ğŸš« æ’é™¤é—œéµè© (é˜²æ­¢æ”¿æ²»å¹²æ“¾)")
    default_neg = "æ–°è, ç›´æ’­, ç¿’è¿‘å¹³, ä¸­å…±, æ”¿æ²»"
    user_neg_input = st.text_area("è¼¸å…¥è¦æ’é™¤çš„è© (é€—è™Ÿåˆ†éš”)", value=default_neg, help="è‹¥æ¨™é¡ŒåŒ…å«é€™äº›è©ï¼Œå°‡ç›´æ¥å¿½ç•¥è©²å½±ç‰‡")
    negative_keywords = [x.strip() for x in user_neg_input.split(",") if x.strip()]

col1, col2, col3 = st.columns([2, 1, 1])
with col1:
    movie_title = st.text_input("é›»å½±åç¨±", value="ä¹é¾åŸå¯¨") 
with col2:
    start_date = st.date_input("é–‹å§‹", value=datetime.today() - timedelta(days=90))
with col3:
    end_date = st.date_input("çµæŸ", value=datetime.today())

if st.button("ğŸš€ é–‹å§‹åˆ†æ", type="primary"):
    if not all([movie_title, yt_api_key, deepseek_api_key]):
        st.error("è«‹å¡«å¯«æ‰€æœ‰æ¬„ä½")
    else:
        with st.spinner("æ­£åœ¨æœå°‹ä¸¦åŸ·è¡Œé›™é‡éæ¿¾ (é—œéµè© + AI)..."):
            df_result, err = main_process(
                movie_title, str(start_date), str(end_date),
                yt_api_key, deepseek_api_key,
                20, max_total_videos, 50, max_total_comments,
                negative_keywords
            )
            
        if err:
            st.error(err)
        else:
            st.divider()
            
            # ç°¡å–®å±•ç¤ºçµæœ (ä¿ç•™åŸæœ‰çš„å¯è¦–åŒ–ä»£ç¢¼çµæ§‹)
            st.subheader("ğŸ”¥ ç†±é–€è©•è«–é—œéµè©")
            # ... (æ­¤è™•å¯è¦–åŒ–ä»£ç¢¼èˆ‡ä¸Šä¸€ç‰ˆç›¸åŒï¼Œçœç•¥ä»¥ç¯€çœç¯‡å¹…) ...
            all_keywords = []
            for item in df_result['keywords']:
                if isinstance(item, str):
                    words = [w.strip() for w in re.split(r'[ï¼Œ,ã€\s]+', item) if len(w.strip()) > 1]
                    all_keywords.extend(words)
                elif isinstance(item, list):
                    all_keywords.extend([str(w).strip() for w in item if len(str(w).strip()) > 1])
            
            if all_keywords:
                kw_counts = Counter(all_keywords).most_common(15)
                kw_df = pd.DataFrame(kw_counts, columns=['Keyword', 'Count']).sort_values(by='Count')
                fig_kw = px.bar(kw_df, x='Count', y='Keyword', orientation='h', title='Top Keywords')
                st.plotly_chart(fig_kw, use_container_width=True)

            with st.expander("æŸ¥çœ‹è©³ç´°æ•¸æ“š"):
                st.dataframe(df_result[['sentiment', 'keywords', 'comment_text', 'video_title']])
