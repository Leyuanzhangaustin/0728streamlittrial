# app.py

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import time
import json
import openai
from googleapiclient.discovery import build
pip install opencc-python-reimplemented
# ==============================================================================
# 1. å„ªåŒ–å¾Œçš„é—œéµè©ç”Ÿæˆç­–ç•¥
# ==============================================================================
def get_optimized_keywords(movie_title, director_name=None, actor_names=None):
    """
    å‹•æ…‹ç”Ÿæˆä¸€çµ„æ›´å…¨é¢çš„æœç´¢é—œéµè©ï¼Œä»¥è¦†è“‹æ›´å¤šé¦™æ¸¯é›»å½±ç›¸é—œè¨è«–ã€‚
    """
    # åŸºç¤é—œéµè© (æ ¸å¿ƒéƒ¨åˆ†)
    base_keywords = [
        f'"{movie_title}" é å‘Š', f'"{movie_title}" å½±è©•', f'"{movie_title}" è§€å¾Œæ„Ÿ',
        f'"{movie_title}" åˆ†æ', f'"{movie_title}" è§£æ', f'"{movie_title}" è§£è®€', f'"{movie_title}" å¿ƒå¾—',
    ]
    # å£èªåŒ–åŠç¶²çµ¡ä¿šèª (æ•æ‰åœ°é“åæ‡‰)
    colloquial_keywords = [
        f'"{movie_title}" å¥½å””å¥½ç‡', f'"{movie_title}" ä¼å””ä¼', f'"{movie_title}" æœ‰å†‡ä¼',
        f'"{movie_title}" å¹æ°´', f'"{movie_title}" è¨è«–', f'"{movie_title}" reaction', f'"{movie_title}" åæ§½',
    ]
    # å…§å®¹å½¢å¼ (è¦†è“‹ä¸åŒè§’åº¦çš„å½±ç‰‡)
    format_keywords = [
        f'"{movie_title}" æ‡¶äººåŒ…', f'"{movie_title}" å½©è›‹', f'"{movie_title}" å¹•å¾ŒèŠ±çµ®',
        f'"{movie_title}" è£½ä½œç‰¹è¼¯', f'"{movie_title}" è¨ªå•',
    ]
    # ä¸Šæ˜ é€±æœŸèˆ‡äº‹ä»¶ (æ•æ‰ç‰¹å®šæ™‚é–“é»çš„ç†±åº¦)
    event_keywords = [
        f'"{movie_title}" ä¸Šæ˜ ', f'"{movie_title}" é¦–æ˜ ', f'"{movie_title}" å„ªå…ˆå ´', f'"{movie_title}" è¬ç¥¨å ´',
    ]
    # è² é¢åŠçˆ­è­°æ€§é—œéµè© (ç¢ºä¿æ•¸æ“šå¹³è¡¡)
    negative_keywords = [
        f'"{movie_title}" è² è©•', f'"{movie_title}" åŠ£è©•', f'"{movie_title}" ä¸­ä¼',
        f'"{movie_title}" å¤±æœ›', f'"{movie_title}" çˆ›ç‰‡', f'"{movie_title}" çˆ­è­°',
    ]
    # ç´”æ¨™é¡Œæœç´¢
    title_only_keywords = [f'"{movie_title}"']
    # å‹•æ…‹é—œéµè© (é¸å¡«ï¼Œä½†å¼·çƒˆå»ºè­°)
    dynamic_keywords = []
    if director_name:
        dynamic_keywords.append(f'"{director_name}" "{movie_title}"')
    if actor_names:
        for actor in actor_names:
            dynamic_keywords.append(f'"{actor}" "{movie_title}"')

    all_keywords = (
        base_keywords + colloquial_keywords + format_keywords + 
        event_keywords + negative_keywords + title_only_keywords + dynamic_keywords
    )
    return list(set(all_keywords))

# ==============================================================================
# 2. YouTube æœç´¢ (å·²ç§»é™¤åœ°ç†ä½ç½®é™åˆ¶)
# ==============================================================================
def search_youtube_videos(keywords, youtube_client, max_per_keyword, start_date, end_date):
    all_video_ids = set()
    for query in keywords:
        nextPageToken = None
        fetched = 0
        while fetched < max_per_keyword:
            try:
                remaining = max_per_keyword - fetched
                max_fetch = min(50, remaining)
                search_response = youtube_client.search().list(
                    q=query,
                    part='id,snippet',
                    type='video',
                    maxResults=max_fetch,
                    publishedAfter=f"{start_date}T00:00:00Z",
                    publishedBefore=f"{end_date}T23:59:59Z",
                    relevanceLanguage='zh-Hant', # å„ªå…ˆè¿”å›ç¹é«”ä¸­æ–‡å…§å®¹
                    # regionCode='HK', # å·²æ ¹æ“šæ‚¨çš„è¦æ±‚ç§»é™¤åœ°ç†ä½ç½®é™åˆ¶
                    pageToken=nextPageToken
                ).execute()
                video_ids = [item['id']['videoId'] for item in search_response.get('items', [])]
                all_video_ids.update(video_ids)
                fetched += len(video_ids)
                nextPageToken = search_response.get('nextPageToken')
                if not nextPageToken or len(video_ids) == 0:
                    break
                time.sleep(0.5)
            except Exception as e:
                st.warning(f"æœç´¢é—œéµè© '{query}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                break
    return list(all_video_ids)

# ==============================================================================
# 3. æ‰¹é‡æŠ“å–è¯„è®º
# ==============================================================================
def get_all_comments(video_ids, youtube_client, max_per_video):
    all_comments = []
    progress_bar = st.progress(0)
    for i, video_id in enumerate(video_ids):
        try:
            request = youtube_client.commentThreads().list(
                part='snippet', videoId=video_id, textFormat='plainText', maxResults=100
            )
            comments_fetched = 0
            while request and comments_fetched < max_per_video:
                response = request.execute()
                for item in response['items']:
                    if comments_fetched >= max_per_video: break
                    comment = item['snippet']['topLevelComment']['snippet']
                    all_comments.append({
                        'video_id': video_id,
                        'comment_text': comment['textDisplay'],
                        'published_at': comment['publishedAt'],
                        'like_count': comment['likeCount']
                    })
                    comments_fetched += 1
                if comments_fetched >= max_per_video: break
                request = youtube_client.commentThreads().list_next(request, response)
        except Exception as e:
            st.warning(f"æŠ“å–å½±ç‰‡ {video_id} çš„è©•è«–æ™‚è·³éï¼ŒåŸå› : {e}")
            continue
        finally:
            progress_bar.progress((i + 1) / len(video_ids), text=f"æ­£åœ¨æŠ“å–å½±ç‰‡è©•è«– ({i+1}/{len(video_ids)})")
    progress_bar.empty()
    return pd.DataFrame(all_comments)

# ==============================================================================
# 4. DeepSeek AIæƒ…æ„Ÿåˆ†æ
# ==============================================================================
def analyze_comment_deepseek(comment_text, deepseek_client, max_retries=3):
    if not isinstance(comment_text, str) or len(comment_text.strip()) < 5:
        return {"sentiment": "Invalid", "topic": "N/A", "summary": "Comment too short or invalid."}
    system_prompt = (
        "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é¦™æ¸¯å¸‚åœºèˆ†æƒ…åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹ç”µå½±è¯„è®ºï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ç»“æœã€‚"
        "JSONå¯¹è±¡å¿…é¡»åŒ…å«ä¸‰ä¸ªé”®ï¼š"
        "1. 'sentiment': å…¶å€¼å¿…é¡»æ˜¯ 'Positive', 'Negative', æˆ– 'Neutral'ã€‚"
        "2. 'topic': è¯„è®ºè®¨è®ºçš„æ ¸å¿ƒä¸»é¢˜ï¼Œä¾‹å¦‚ 'å‰§æƒ…', 'æ¼”å‘˜æ¼”æŠ€', 'åŠ¨ä½œè®¾è®¡', 'ç”»é¢ç¾æœ¯', 'ç”µå½±èŠ‚å¥', 'æ•´ä½“æ„Ÿè§‰'ã€‚å¦‚æœæ— æ³•åˆ¤æ–­ï¼Œåˆ™ä¸º 'N/A'ã€‚"
        "3. 'summary': ç”¨ä¸€å¥è¯ç®€æ½”æ€»ç»“è¯„è®ºçš„æ ¸å¿ƒè§‚ç‚¹ã€‚"
        "ç¡®ä¿è¾“å‡ºåªæœ‰JSONå¯¹è±¡ï¼Œæ— ä»»ä½•é¢å¤–æ–‡å­—ã€‚"
    )
    for attempt in range(max_retries):
        try:
            response = deepseek_client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": comment_text}
                ],
                response_format={"type": "json_object"},
                temperature=0.1,
            )
            analysis_result = json.loads(response.choices[0].message.content)
            return analysis_result
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
            else:
                return {"sentiment": "Error", "topic": "Error", "summary": f"API Error: {e}"}

# ==============================================================================
# 5. ä¸»æµç¨‹ (å·²åŠ å…¥ç¹é«”ä¸­æ–‡éæ¿¾)
# ==============================================================================
def movie_comment_analysis(
    search_keywords, start_date, end_date,
    yt_api_key, deepseek_api_key,
    max_videos_per_keyword, max_comments_per_video, sample_size
):
    # APIåˆå§‹åŒ–
    youtube_client = build('youtube', 'v3', developerKey=yt_api_key)
    deepseek_client = openai.OpenAI(api_key=deepseek_api_key, base_url="https://api.deepseek.com/v1")

    # æœç´¢è§†é¢‘
    st.info(f"æ­£åœ¨ä½¿ç”¨ {len(search_keywords)} çµ„å„ªåŒ–é—œéµè©æœç´¢ç›¸é—œå½±ç‰‡...")
    video_ids = search_youtube_videos(search_keywords, youtube_client, max_videos_per_keyword, start_date, end_date)
    if not video_ids:
        return None, "æœªæ‰¾åˆ°ç›¸é—œè¦–é »ï¼Œè«‹å˜—è©¦æ”¾å¯¬æ—¥æœŸæˆ–èª¿æ•´é›»å½±åç¨±ã€‚"
    st.success(f"æ‰¾åˆ° {len(video_ids)} å€‹ç›¸é—œå½±ç‰‡ï¼Œé–‹å§‹æŠ“å–è©•è«–...")

    # æŠ“å–è¯„è®º
    df_comments = get_all_comments(video_ids, youtube_client, max_comments_per_video)
    if df_comments.empty:
        return None, "æˆåŠŸæ‰¾åˆ°å½±ç‰‡ï¼Œä½†æœªèƒ½æŠ“å–åˆ°ä»»ä½•è©•è«–ã€‚"
    
    # === æ–°å¢ï¼šç¹é«”ä¸­æ–‡è©•è«–éæ¿¾ ===
    try:
        from opencc import OpenCC
        cc = OpenCC('t2s.json')
        def is_traditional_chinese(text):
            if not isinstance(text, str) or not text.strip(): return False
            return cc.convert(text) != text
        
        original_count = len(df_comments)
        df_comments = df_comments[df_comments['comment_text'].apply(is_traditional_chinese)].copy()
        st.info(f"å¾ {original_count} æ¢åŸå§‹è©•è«–ä¸­ï¼Œç¯©é¸å‡º {len(df_comments)} æ¢ç¹é«”ä¸­æ–‡è©•è«–ã€‚")
        if df_comments.empty:
            return None, "æœªæª¢æ¸¬åˆ°ç¹é«”ä¸­æ–‡è©•è«–ã€‚"
    except ImportError:
        st.error("ç¼ºå°‘ 'opencc-python-reimplemented' åº«ï¼Œç„¡æ³•é€²è¡Œç¹é«”ä¸­æ–‡éæ¿¾ã€‚è«‹é‹è¡Œ `pip install opencc-python-reimplemented`ã€‚")
        return None, "ç¼ºå°‘å¿…è¦çµ„ä»¶ã€‚"
    # ==============================

    # æ—¶é—´å¤„ç†
    df_comments['published_at'] = pd.to_datetime(df_comments['published_at'], utc=True)
    df_comments['published_at_hk'] = df_comments['published_at'].dt.tz_convert('Asia/Hong_Kong')
    start_dt = pd.to_datetime(start_date).tz_localize('Asia/Hong_Kong')
    end_dt = pd.to_datetime(end_date).tz_localize('Asia/Hong_Kong') + timedelta(days=1)
    mask = (df_comments['published_at_hk'] >= start_dt) & (df_comments['published_at_hk'] <= end_dt)
    df_comments = df_comments.loc[mask].reset_index(drop=True)
    if df_comments.empty:
        return None, "åœ¨æŒ‡å®šæ—¥æœŸç¯„åœå…§æ²’æœ‰æ‰¾åˆ°ç¹é«”ä¸­æ–‡è©•è«–ã€‚"

    # æŠ½æ ·
    if sample_size and sample_size < len(df_comments):
        df_analyze = df_comments.sample(n=sample_size, random_state=42)
        st.info(f"å¾ {len(df_comments)} æ¢ç¬¦åˆæ¢ä»¶çš„è©•è«–ä¸­ï¼Œéš¨æ©ŸæŠ½æ¨£ {sample_size} æ¢é€²è¡Œåˆ†æã€‚")
    else:
        df_analyze = df_comments
        st.info(f"å°‡å°å…¨éƒ¨ {len(df_comments)} æ¢ç¬¦åˆæ¢ä»¶çš„è©•è«–é€²è¡Œåˆ†æã€‚")

    # AIæƒ…æ„Ÿåˆ†æ
    from tqdm import tqdm
    tqdm.pandas(desc="AIæƒ…æ„Ÿåˆ†æ")
    analysis_results = df_analyze['comment_text'].progress_apply(lambda x: analyze_comment_deepseek(x, deepseek_client))
    analysis_df = pd.json_normalize(analysis_results)
    final_df = pd.concat([df_analyze.reset_index(drop=True), analysis_df], axis=1)
    final_df['published_at'] = pd.to_datetime(final_df['published_at'])
    return final_df, None

# ==============================================================================
# 6. Streamlit UI (å·²æ›´æ–°åƒæ•¸å’Œè¼¸å…¥é …)
# ==============================================================================
st.set_page_config(page_title="ç”µå½±YouTubeè¯„è®ºAIåˆ†æ", layout="wide")
st.title("ğŸ¬ YouTube ç”µå½±è¯„è®ºAIæƒ…æ„Ÿåˆ†æï¼ˆé¦™æ¸¯å¸‚åœºç‰ˆï¼‰")

with st.expander("â„¹ï¸ æ“ä½œèªªæ˜èˆ‡å»ºè­°"):
    st.markdown("""
    1.  **è¼¸å…¥é›»å½±çš„é¦™æ¸¯å®˜æ–¹è­¯å**ï¼Œä»¥ç²å¾—æœ€ç²¾æº–çš„æœç´¢çµæœã€‚
    2.  **ï¼ˆå¯é¸ï¼‰è¼¸å…¥å°æ¼”å’Œä¸»è¦æ¼”å“¡å§“å**ï¼Œèƒ½è¦†è“‹æ›´å¤šåœç¹äººç‰©å±•é–‹çš„è¨è«–å½±ç‰‡ã€‚
    3.  **èª¿æ•´åƒæ•¸**ï¼šæƒ³ç²å¾—æ›´å¤šè©•è«–ï¼Œè«‹**èª¿é«˜â€œæ¯çµ„é—œéµè©æœ€å¤šè¦–é »æ•¸â€**å’Œ**â€œæ¯å€‹è¦–é »æœ€å¤šè©•è«–æ•¸â€**ã€‚
    4.  **API Keys**ï¼šè«‹å¡«å…¥æ‚¨è‡ªå·±çš„ YouTube Data API v3 å’Œ DeepSeek API çš„å¯†é‘°ã€‚
    5.  **é–‹å§‹åˆ†æ**ï¼šé»æ“ŠæŒ‰éˆ•å¾Œï¼Œç¨‹å¼æœƒè‡ªå‹•å®Œæˆï¼š`æœç´¢å½±ç‰‡` -> `æŠ“å–è©•è«–` -> `éæ¿¾ç¹é«”å­—` -> `AIé€æ¢åˆ†æ` -> `ç”Ÿæˆåœ–è¡¨`ã€‚éç¨‹å¯èƒ½éœ€è¦æ•¸åˆ†é˜ï¼Œè«‹è€å¿ƒç­‰å€™ã€‚
    """)

# --- è¼¸å…¥æ¬„ä½ ---
st.header("1. è¼¸å…¥åˆ†æç›®æ¨™")
movie_title = st.text_input("é›»å½±åç¨±ï¼ˆé¦™æ¸¯è­¯åï¼‰", value="ä¹é¾åŸå¯¨ä¹‹åœåŸ")
director_name = st.text_input("å°æ¼”åç¨±ï¼ˆå¯é¸ï¼‰", value="é„­ä¿ç‘")
actors_str = st.text_input("ä¸»è¦æ¼”å“¡ï¼ˆç”¨é€—è™Ÿ/ç©ºæ ¼åˆ†éš”ï¼Œå¯é¸ï¼‰", value="å¤å¤©æ¨‚, æ—å³¯, åŠ‰ä¿Šè¬™")

st.header("2. è¨­å®šåˆ†æç¯„åœèˆ‡å¯†é‘°")
col1, col2 = st.columns(2)
with col1:
    start_date = st.date_input("èµ·å§‹æ—¥æœŸ", value=datetime.today() - timedelta(days=30))
with col2:
    end_date = st.date_input("çµæŸæ—¥æœŸ", value=datetime.today())
yt_api_key = st.text_input("YouTube API Key", type='password')
deepseek_api_key = st.text_input("DeepSeek API Key", type='password')

st.header("3. èª¿æ•´æ•¸æ“šæŠ“å–é‡")
max_videos = st.slider("æ¯çµ„é—œéµè©æœ€å¤šè¦–é »æ•¸ï¼ˆè¶Šé«˜ï¼Œä¾†æºè¶Šå»£ï¼‰", 5, 100, 30)
max_comments = st.slider("æ¯å€‹è¦–é »æœ€å¤šè©•è«–æ•¸ï¼ˆè¶Šé«˜ï¼Œè©•è«–è¶Šå¤šï¼‰", 10, 500, 50)
sample_size = st.number_input("æœ€å¤šåˆ†æè©•è«–æ•¸ï¼ˆè¨­ç‚º 0 å‰‡åˆ†æå…¨éƒ¨ï¼‰", 0, 5000, 0)

# --- åˆ†ææŒ‰éˆ•èˆ‡ä¸»é‚è¼¯ ---
if st.button("ğŸš€ é–‹å§‹åˆ†æ", use_container_width=True):
    if not all([movie_title, yt_api_key, deepseek_api_key]):
        st.warning("è«‹å‹™å¿…å¡«å¯«é›»å½±åç¨±å’Œå…©å€‹ API Keyã€‚")
    else:
        # å‹•æ…‹ç”Ÿæˆé—œéµè©
        actor_names = [name.strip() for name in actors_str.replace('ï¼Œ', ',').replace(' ', ',').split(',') if name.strip()]
        SEARCH_KEYWORDS = get_optimized_keywords(movie_title, director_name, actor_names)
        
        with st.spinner("AIåˆ†æä¸­ï¼Œè«‹è€å¿ƒç­‰å¾…...ï¼ˆå¦‚è©•è«–æ•¸å¤šï¼Œéœ€æ•¸åˆ†é˜ï¼‰"):
            df_result, err = movie_comment_analysis(
                SEARCH_KEYWORDS, str(start_date), str(end_date),
                yt_api_key, deepseek_api_key,
                max_videos, max_comments, sample_size or None
            )
        
        if err:
            st.error(f"åˆ†æä¸­æ–·ï¼š{err}")
        else:
            st.success("åˆ†æå®Œæˆï¼")
            st.header("ğŸ“Š åˆ†æçµæœæ¦‚è¦½")

            # --- æ•¸æ“šå±•ç¤º ---
            st.subheader("éƒ¨åˆ†åŸå§‹æ•¸æ“šé è¦½")
            st.dataframe(df_result.head(20))

            # --- å¯è¦–åŒ– ---
            st.subheader("1. æ•´é«”æƒ…æ„Ÿåˆ†ä½ˆ")
            fig1, ax1 = plt.subplots(figsize=(5, 4))
            sentiment_counts = df_result['sentiment'].value_counts()
            colors_map = {"Positive": "#5cb85c", "Negative": "#d9534f", "Neutral": "#f0ad4e", "Invalid": "#cccccc", "Error": "#888888"}
            pie_colors = [colors_map.get(s, "#333333") for s in sentiment_counts.index]
            sentiment_counts.plot.pie(autopct='%.1f%%', ax=ax1, colors=pie_colors, startangle=90)
            ax1.set_title('Sentiment Distribution')
            ax1.set_ylabel('')
            st.pyplot(fig1)

            st.subheader("2. æ¯æ—¥æƒ…æ„Ÿè¶¨å‹¢")
            df_result['date'] = df_result['published_at_hk'].dt.date
            daily = df_result.groupby(['date', 'sentiment']).size().unstack().fillna(0)
            # ç¢ºä¿æ¬„ä½é †åºä¸€è‡´ï¼Œæ–¹ä¾¿ä¸Šè‰²
            daily = daily.reindex(columns=['Positive', 'Negative', 'Neutral', 'Invalid', 'Error'], fill_value=0)
            
            col_chart, line_chart = st.columns(2)
            with col_chart:
                fig2, ax2 = plt.subplots(figsize=(8, 4))
                daily[['Positive', 'Negative', 'Neutral']].plot(
                    kind='bar', stacked=True, ax=ax2, width=0.8, 
                    color=[colors_map['Positive'], colors_map['Negative'], colors_map['Neutral']]
                )
                ax2.set_title('æ¯æ—¥æƒ…æ„Ÿè¶¨å‹¢ (å †ç–Šé•·æ¢åœ–)')
                ax2.set_xlabel('æ—¥æœŸ')
                ax2.set_ylabel('è©•è«–æ•¸é‡')
                plt.xticks(rotation=45, ha='right')
                plt.tight_layout()
                st.pyplot(fig2)

            with line_chart:
                fig3, ax3 = plt.subplots(figsize=(8, 4))
                for sentiment in ['Positive', 'Negative', 'Neutral']:
                    if sentiment in daily.columns:
                        ax3.plot(daily.index, daily[sentiment], marker='o', linestyle='-', label=sentiment, color=colors_map[sentiment])
                ax3.set_title("æ¯æ—¥æƒ…æ„Ÿè¶¨å‹¢ (æŠ˜ç·šåœ–)")
                ax3.set_xlabel("æ—¥æœŸ")
                ax3.set_ylabel("è©•è«–æ•¸é‡")
                ax3.legend(title="Sentiment")
                plt.xticks(rotation=45, ha='right')
                plt.tight_layout()
                st.pyplot(fig3)

            st.subheader("3. æ ¸å¿ƒä¸»é¡Œè¨è«–ä½”æ¯”")
            fig4, ax4 = plt.subplots(figsize=(8, 5))
            # éæ¿¾æ‰ç„¡æ•ˆä¸»é¡Œ
            topic_counts = df_result[~df_result['topic'].isin(['N/A', 'Error'])]['topic'].value_counts()
            topic_counts.plot(kind='barh', ax=ax4, color='#0288d1')
            ax4.set_title('æ ¸å¿ƒè¨è«–ä¸»é¡Œ Top 10')
            ax4.set_xlabel('è©•è«–æ•¸é‡')
            # åœ¨é•·æ¢åœ–ä¸Šé¡¯ç¤ºæ•¸å­—
            for index, value in enumerate(topic_counts):
                ax4.text(value, index, f' {value}')
            plt.tight_layout()
            st.pyplot(fig4)

            # --- ä¸‹è¼‰æŒ‰éˆ• ---
            st.header("4. ä¸‹è¼‰åˆ†ææ˜ç´°")
            csv = df_result.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰å…¨éƒ¨åˆ†ææ˜ç´° (CSV)",
                data=csv,
                file_name=f"{movie_title}_youtube_analysis_{start_date}_to_{end_date}.csv",
                mime='text/csv',
                use_container_width=True
            )
else:
    st.info("è«‹å¡«å¯«é ‚éƒ¨ä¿¡æ¯ä¸¦é»æ“Šâ€œé–‹å§‹åˆ†æâ€ä»¥ç”Ÿæˆå ±å‘Šã€‚")
