# app.py

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import time
import asyncio
import openai
import re
import json
from collections import Counter
from opencc import OpenCC
from googleapiclient.discovery import build

# =========================
# 0. Âø´ÂèñËàáÂ∑•ÂÖ∑ÂáΩÂºè
# =========================

CACHE_TTL_SEARCH = 3600
CACHE_TTL_RELEVANCE = 86400
CACHE_TTL_CHANNEL = 86400
CACHE_TTL_COMMENTS = 900

def _get_cached_value(cache_name: str, key, ttl_seconds: int):
    if cache_name not in st.session_state:
        st.session_state[cache_name] = {}
    entry = st.session_state[cache_name].get(key)
    if entry:
        if (time.time() - entry["ts"]) <= ttl_seconds:
            return entry["value"]
        else:
            del st.session_state[cache_name][key]
    return None

def _set_cached_value(cache_name: str, key, value):
    if cache_name not in st.session_state:
        st.session_state[cache_name] = {}
    st.session_state[cache_name][key] = {
        "value": value,
        "ts": time.time()
    }

# =========================
# 1. Ë™ûË®ÄËàáÈóúÈçµÂ≠óÂ∑•ÂÖ∑
# =========================

def generate_search_queries(movie_title: str):
    zh_terms = [
        "ÂΩ±Ë©ï", "Ë©ïË´ñ", "Ë©ïÂÉπ", "ÈªûË©ï", "Ëß£Êûê", "ÂàÜÊûê", "ËßÄÂæåÊÑü",
        "ÁÑ°Èõ∑", "ÊúâÈõ∑", "Ë®éË´ñ", "Â•ΩÂîîÂ•ΩÁùá", "È†êÂëä", "Ëä±ÁµÆ", "ÁâáÊÆµ", "È¶ñÊò†", "ÂπïÂæå",
        "È¶ôÊ∏Ø", "È¶ôÊ∏Ø‰∏äÊò†", "È¶ôÊ∏ØÈ¶ñÊò†", "È¶ôÊ∏ØÂèçÊáâ", "Êà≤Èô¢ ÂèçÊáâ", "Èô¢Á∑ö", "Ë°óË®™",
        "Á≤µË™û", "Âª£Êù±Ë©±", "Á≤µË™ûÈÖçÈü≥", "Á≤µÈÖç", "Ê∏ØÁâà", "Ê∏ØÁî¢"
    ]
    en_terms = [
        "review", "reaction", "ending explained", "analysis", "explained",
        "behind the scenes", "bts", "premiere", "interview", "press conference",
        "hong kong", "hk reaction", "hk audience"
    ]
    loose = [f"{movie_title}"]
    loose += [f"{movie_title} {t}" for t in zh_terms]
    loose += [f"{movie_title} {t}" for t in en_terms]
    tight = [
        f"\"{movie_title}\"",
        f"\"{movie_title}\" ÂΩ±Ë©ï",
        f"\"{movie_title}\" Ë©ïË´ñ",
        f"\"{movie_title}\" Ëß£Êûê",
        f"\"{movie_title}\" review",
        f"\"{movie_title}\" reaction",
        f"\"{movie_title}\" È¶ôÊ∏Ø",
        f"\"{movie_title}\" Á≤µË™û",
        f"\"{movie_title}\" Âª£Êù±Ë©±",
    ]
    seen = set()
    queries = []
    for q in loose + tight:
        if q not in seen:
            queries.append(q)
            seen.add(q)
    return queries

# Á≤µË™ûÁâπÂæµÂ≠óÂÖ∏
CANTONESE_CHAR_TOKENS = {
    "Âîî": 1.0, "ÂÜá": 1.6, "Âíó": 1.6, "ÂòÖ": 1.6, "Âï≤": 1.2, "Âó∞": 1.2, "‰Ω¢": 1.0,
    "Âñ∫": 1.6, "Âöü": 1.6, "Âí™": 1.2, "Âï±": 1.2, "ÊéÇ": 1.2, "Èùö": 1.2, "Êõ≥": 1.2,
    "Êî∞": 1.2, "ÂíÅ": 1.0, "Âôâ": 1.0, "Âæó": 0.6, "Âêñ": 0.8, "ÂÜß": 1.0, "Êíö": 1.2,
    "‰ªÜ": 1.2, "Â±å": 1.2, "ÂóÆ": 1.0, "ÁïÄ": 0.8, "Êè∏": 1.0, "ËÖé": 0.0,
    "Á≥ª": 0.5, "‰øÇ": 1.5, "9": 0.5, "7": 0.5, "6": 0.3, "‰∫û": 0.5, "Èáé": 0.5,
    "Êó¢": 0.5, "Â∑¶": 0.5, "d": 0.8, "D": 0.8,
}

CANTONESE_PARTICLES = ["Âï¶", "Âõâ", "Âñé", "Âí©", "Âë¢", "ÂëÄ", "Âòõ", "Âñá", "Êù∞", "Âßê", "ÂôÉ"]
CANTONESE_PHRASES = {
    "Â•ΩÂîîÂ•ΩÁùá": 2.0, "ÂÅöÂí©": 1.6, "ÈªûËß£": 1.6, "Âí©Êñô": 1.6, "ÁÆóÂï¶": 1.2,
    "ÂæóÂï¶": 1.2, "Ê≠£Âñé": 1.2, "ÂπæÂ•ΩÁùá": 1.6, "ÂπæÊ≠£": 1.2, "Â•ΩÊ≠£": 1.0,
    "ÊúâÂï≤": 0.8, "Âó∞Âï≤": 1.2, "Âë¢Âï≤": 1.2, "Ë¨õÁúü": 0.8, "Â•Ω‰ºº": 0.5,
    "Â§ö9‰Ωô": 2.0, "Â§öÈ§ò": 0.5, "ÁúüÁ≥ª": 1.0, "Áúü‰øÇ": 1.5, "ÊâìÈ¢®": 1.0
}
ROMANIZATION_RE = re.compile(r"(?i)(?<![A-Za-z])(la|lor|wor|leh|meh|mah|ga|wo|ar)(?=[\s\W]|$)")

def count_chars(text: str):
    counts = {
        "cjk": 0, "hiragana": 0, "katakana": 0, "half_katakana": 0,
        "hangul": 0, "latin": 0, "digits": 0, "other": 0
    }
    for ch in text:
        code = ord(ch)
        if 0x4E00 <= code <= 0x9FFF or 0x3400 <= code <= 0x4DBF or 0xF900 <= code <= 0xFAFF:
            counts["cjk"] += 1
        elif 0x3040 <= code <= 0x309F:
            counts["hiragana"] += 1
        elif 0x30A0 <= code <= 0x30FF or 0x31F0 <= code <= 0x31FF:
            counts["katakana"] += 1
        elif 0xFF65 <= code <= 0xFF9F:
            counts["half_katakana"] += 1
        elif 0xAC00 <= code <= 0xD7AF:
            counts["hangul"] += 1
        elif (0x0041 <= code <= 0x005A) or (0x0061 <= code <= 0x007A):
            counts["latin"] += 1
        elif 0x0030 <= code <= 0x0039:
            counts["digits"] += 1
        else:
            counts["other"] += 1
    return counts

def diff_chars(a: str, b: str) -> int:
    m = min(len(a), len(b))
    return sum(1 for i in range(m) if a[i] != b[i]) + abs(len(a) - len(b))

def classify_zh_trad_simp(text: str, cc_t2s: OpenCC, cc_s2t: OpenCC):
    if not isinstance(text, str) or len(text.strip()) < 1:
        return "other"
    counts = count_chars(text)
    
    total_chars = len(text.strip())
    if counts["latin"] / max(1, total_chars) > 0.7:
        return "en"

    kana = counts["hiragana"] + counts["katakana"] + counts["half_katakana"]
    cjk = counts["cjk"]
    
    if kana >= 2 and kana / max(1, (cjk + kana)) >= 0.10:
        return "ja"
    if cjk < 1:
        return "other" if counts["latin"] == 0 else "en"

    t2s = cc_t2s.convert(text)
    s2t = cc_s2t.convert(text)
    ct2s = diff_chars(text, t2s)
    cs2t = diff_chars(text, s2t)
    threshold = max(1, int(0.05 * cjk))
    if ct2s > cs2t + threshold:
        return "zh-Hant"
    elif cs2t > ct2s + threshold:
        return "zh-Hans"
    else:
        return "zh-unkn"

def score_cantonese(text: str) -> float:
    if not isinstance(text, str) or not text:
        return 0.0
    score = 0.0
    text_lower = text.lower()
    
    for phrase, w in CANTONESE_PHRASES.items():
        if phrase in text:
            score += text.count(phrase) * w
            
    for ch, w in CANTONESE_CHAR_TOKENS.items():
        if ch in ['d', 'D']:
            cnt = text_lower.count('d')
            score += cnt * w * 0.5 
        else:
            cnt = text.count(ch)
            if cnt:
                score += cnt * w

    end_slice = text[-8:] if len(text) > 8 else text
    for p in CANTONESE_PARTICLES:
        if p in end_slice:
            score += 0.4
        elif p in text:
            score += 0.2

    roman_hits = ROMANIZATION_RE.findall(text)
    if roman_hits:
        score += len(roman_hits) * 0.8
    return score

# =========================
# 2. YouTube ÊêúÂ∞ã
# =========================

def search_youtube_videos(keywords, youtube_client, max_per_keyword, start_date, end_date, add_language_bias=True, region_bias=True, max_total_videos=150):
    all_video_ids = set()
    video_meta = {}
    status_text = st.empty()

    for idx, query in enumerate(keywords):
        if len(all_video_ids) >= max_total_videos:
            status_text.info(f"Â∑≤ÈÅîÂà∞ÊêúÂ∞ã‰∏äÈôê ({max_total_videos} ÈÉ®)ÔºåÂÅúÊ≠¢ÂæåÁ∫åÊêúÂ∞ã„ÄÇ")
            break

        cache_key = f"{query}_{start_date}_{end_date}_{max_per_keyword}_{add_language_bias}_{region_bias}"
        cached_data = _get_cached_value("search_cache", cache_key, CACHE_TTL_SEARCH)
        query_records = []

        if cached_data is not None:
            query_records = cached_data
        else:
            collected_records = []
            collected_ids_for_query = set()
            for order in ["relevance", "viewCount"]:
                if len(collected_records) >= max_per_keyword: break
                try:
                    request = youtube_client.search().list(
                        q=query, part="id,snippet", type="video", maxResults=50,
                        publishedAfter=f"{start_date}T00:00:00Z", publishedBefore=f"{end_date}T23:59:59Z",
                        order=order, safeSearch="none",
                        **({"relevanceLanguage": "zh-Hant"} if add_language_bias else {}),
                        **({"regionCode": "HK"} if region_bias else {})
                    )
                    while request and len(collected_records) < max_per_keyword:
                        response = request.execute()
                        for item in response.get("items", []):
                            vid = item["id"]["videoId"]
                            if vid in collected_ids_for_query: continue
                            collected_ids_for_query.add(vid)
                            snip = item.get("snippet", {})
                            collected_records.append({
                                "video_id": vid, "title": snip.get("title", ""),
                                "channelTitle": snip.get("channelTitle", ""),
                                "publishedAt": snip.get("publishedAt", "")
                            })
                        if len(collected_records) >= max_per_keyword: break
                        request = youtube_client.search().list_next(request, response)
                        time.sleep(0.1)
                except Exception as e:
                    st.warning(f"ÊêúÂ∞ã '{query}' ÈåØË™§: {e}")
                    continue
            _set_cached_value("search_cache", cache_key, collected_records)
            query_records = collected_records

        for record in query_records:
            vid = record["video_id"]
            if vid not in all_video_ids:
                all_video_ids.add(vid)
                if vid not in video_meta:
                    video_meta[vid] = {
                        "title": record["title"],
                        "channelTitle": record["channelTitle"],
                        "publishedAt": record["publishedAt"]
                    }
            if len(all_video_ids) >= max_total_videos: break
    status_text.empty()
    return list(all_video_ids), video_meta

# =========================
# 3. AI Áõ∏ÈóúÊÄßÈÅéÊøæ
# =========================

async def check_relevance_batch_async(movie_title, batch_videos, deepseek_client):
    if not batch_videos: return []
    prompt_items = [f"ID: {v['id']}\nTitle: {v['title']}\nChannel: {v['channel']}" for v in batch_videos]
    prompt_text = "\n---\n".join(prompt_items)
    system_prompt = (
        f"You are a data cleaner. The user is analyzing the movie '{movie_title}'. "
        "Identify which videos are actually discussing this specific movie. "
        "Exclude unrelated videos. Return JSON: {\"vid123\": true, \"vid456\": false}"
    )
    try:
        response = await deepseek_client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": prompt_text}],
            response_format={"type": "json_object"}, temperature=0.1,
        )
        data = json.loads(response.choices[0].message.content)
        return [vid for vid, is_rel in data.items() if is_rel is True]
    except Exception:
        return [v['id'] for v in batch_videos]

async def filter_videos_by_relevance(movie_title, video_ids, video_meta, deepseek_client):
    to_check = []
    valid_ids = set()
    for vid in video_ids:
        cached = _get_cached_value("relevance_cache", f"{movie_title}_{vid}", CACHE_TTL_RELEVANCE)
        if cached is not None:
            if cached: valid_ids.add(vid)
        else:
            meta = video_meta.get(vid, {})
            to_check.append({"id": vid, "title": meta.get("title", ""), "channel": meta.get("channelTitle", "")})
    
    if to_check:
        batch_size = 20
        tasks = [check_relevance_batch_async(movie_title, to_check[i:i+batch_size], deepseek_client) for i in range(0, len(to_check), batch_size)]
        progress = st.empty()
        progress.info(f"AI ÈÅéÊøæ {len(to_check)} ÈÉ®ÂΩ±ÁâáÁõ∏ÈóúÊÄß...")
        results = await asyncio.gather(*tasks)
        for batch_idx, res_list in enumerate(results):
            batch_input = to_check[batch_idx*batch_size : (batch_idx+1)*batch_size]
            rel_set = set(res_list)
            for item in batch_input:
                is_rel = item["id"] in rel_set
                if is_rel: valid_ids.add(item["id"])
                _set_cached_value("relevance_cache", f"{movie_title}_{item['id']}", is_rel)
        progress.empty()
    return list(valid_ids)

# =========================
# 4. Ë©≥ÊÉÖËàáÁïôË®Ä
# =========================

def fetch_video_and_channel_details(video_ids, youtube_client):
    video_extra = {}
    channel_ids = set()
    for i in range(0, len(video_ids), 50):
        try:
            resp = youtube_client.videos().list(part="snippet,contentDetails", id=",".join(video_ids[i:i+50])).execute()
            for item in resp.get("items", []):
                vid = item.get("id")
                snip = item.get("snippet", {})
                ch = snip.get("channelId")
                video_extra[vid] = {
                    "channelId": ch,
                    "defaultLanguage": snip.get("defaultLanguage", ""),
                    "defaultAudioLanguage": snip.get("defaultAudioLanguage", ""),
                    "tags": snip.get("tags", [])
                }
                if ch: channel_ids.add(ch)
        except Exception: pass

    channel_country = {}
    to_fetch = []
    for cid in channel_ids:
        cached = _get_cached_value("channel_cache", cid, CACHE_TTL_CHANNEL)
        if cached: channel_country[cid] = cached
        else: to_fetch.append(cid)
    
    if to_fetch:
        for i in range(0, len(to_fetch), 50):
            try:
                resp = youtube_client.channels().list(part="brandingSettings", id=",".join(to_fetch[i:i+50])).execute()
                for item in resp.get("items", []):
                    cid = item.get("id")
                    country = item.get("brandingSettings", {}).get("channel", {}).get("country")
                    channel_country[cid] = country
                    _set_cached_value("channel_cache", cid, country)
            except Exception: pass
    return video_extra, channel_country

def compute_hk_video_score(video_id, video_meta, video_extra, channel_country_map):
    meta = video_meta.get(video_id, {})
    ext = video_extra.get(video_id, {})
    title = meta.get("title", "")
    tags = " ".join(ext.get("tags", []) or [])
    ch = ext.get("channelId")
    audio = (ext.get("defaultAudioLanguage") or "").lower()
    country = channel_country_map.get(ch)

    score = 0
    if country == "HK": score += 3
    if audio in ("yue", "zh-hk", "zh-yue", "zh-hant-hk"): score += 3
    elif audio.startswith("zh"): score += 1
    
    if any(t in title for t in ["Á≤µË™û", "Âª£Êù±Ë©±", "Á≤µÈÖç", "Á≤µË™ûÈÖçÈü≥"]): score += 3
    if any(t in title for t in ["È¶ôÊ∏Ø", "Ê∏ØÁâà", "È¶ôÊ∏ØËßÄÁúæ", "È¶ôÊ∏ØÂèçÊáâ", "È¶ôÊ∏ØÈ¶ñÊò†", "È¶ôÊ∏Ø‰∏äÊò†"]): score += 2
    if ("HK" in title) or ("Hong Kong" in title): score += 1
    if any(t in tags for t in ["Á≤µË™û", "Âª£Êù±Ë©±", "È¶ôÊ∏Ø", "HK"]): score += 2
    return score

def get_all_comments(video_ids, youtube_client, max_per_video, video_meta, hk_score_map, video_extra, channel_country_map, max_total_comments):
    all_comments = []
    total_fetched = 0
    progress = st.progress(0, text="ÊäìÂèñÁïôË®Ä...")
    
    for i, vid in enumerate(video_ids):
        if total_fetched >= max_total_comments: break
        
        cache_key = f"{vid}_{max_per_video}"
        cached = _get_cached_value("comments_cache", cache_key, CACHE_TTL_COMMENTS)
        raw_recs = []

        if cached is not None:
            raw_recs = cached
        else:
            try:
                req = youtube_client.commentThreads().list(part="snippet", videoId=vid, textFormat="plainText", order="time", maxResults=100)
                fetched_vid = 0
                while req and fetched_vid < max_per_video:
                    if total_fetched + fetched_vid >= max_total_comments: break
                    resp = req.execute()
                    for item in resp.get("items", []):
                        if fetched_vid >= max_per_video: break
                        cmt = item["snippet"]["topLevelComment"]["snippet"]
                        raw_recs.append({
                            "textDisplay": cmt.get("textDisplay", ""),
                            "publishedAt": cmt.get("publishedAt", ""),
                            "likeCount": cmt.get("likeCount", 0)
                        })
                        fetched_vid += 1
                    req = youtube_client.commentThreads().list_next(req, resp)
                    if req and fetched_vid < max_per_video: time.sleep(0.1)
                if raw_recs: _set_cached_value("comments_cache", cache_key, raw_recs)
            except Exception: pass
        
        ch_id = video_extra.get(vid, {}).get("channelId")
        for r in raw_recs:
            all_comments.append({
                "video_id": vid,
                "video_title": video_meta.get(vid, {}).get("title", ""),
                "video_hk_score": hk_score_map.get(vid, 0),
                "video_channel_country": channel_country_map.get(ch_id),
                "comment_text": r["textDisplay"],
                "published_at": r["publishedAt"],
                "like_count": r["likeCount"]
            })
        total_fetched += len(raw_recs)
        progress.progress((i+1)/len(video_ids), text=f"ÊäìÂèñÁïôË®Ä... ({min(i+1, len(video_ids))}/{len(video_ids)})")
    progress.empty()
    return pd.DataFrame(all_comments)

# =========================
# 5. DeepSeek ÂàÜÊûê
# =========================

async def analyze_comment_deepseek_async(comment_text, deepseek_client, semaphore, max_retries=3):
    if not isinstance(comment_text, str) or len(comment_text.strip()) < 2:
        return {"sentiment": "Invalid", "topic": "N/A", "summary": "Too short."}
    
    system_prompt = (
        "You are a professional Hong Kong market sentiment analyst. "
        "Analyze the movie comment. Return JSON with keys: "
        "'sentiment' (Positive/Negative/Neutral), 'topic' (Plot/Acting/Action Design/Visuals/Pace/Overall/N/A), 'summary'. "
        "Treat 'Thank you' or 'Good' as Positive/Overall."
    )
    async with semaphore:
        for attempt in range(max_retries):
            try:
                response = await deepseek_client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": comment_text}],
                    response_format={"type": "json_object"}, temperature=0.1,
                )
                return json.loads(response.choices[0].message.content)
            except Exception:
                if attempt < max_retries - 1: await asyncio.sleep(2**attempt)
                else: return {"sentiment": "Error", "topic": "Error", "summary": "API Error"}

async def run_all_analyses(df, deepseek_client):
    semaphore = asyncio.Semaphore(50)
    tasks = []
    async def with_index(idx, text):
        res = await analyze_comment_deepseek_async(text, deepseek_client, semaphore)
        return idx, res
    for i, text in enumerate(df["comment_text"]):
        tasks.append(asyncio.create_task(with_index(i, text)))
    
    progress = st.progress(0, text="AI ÂàÜÊûê‰∏≠...")
    results = [None]*len(tasks)
    for done, coro in enumerate(asyncio.as_completed(tasks), 1):
        idx, res = await coro
        results[idx] = res
        progress.progress(done/len(tasks))
    progress.empty()
    return results

async def generate_summary(df, deepseek_client):
    comments_preview = "\n".join(df["comment_text"].sample(min(50, len(df))).tolist())
    prompt = f"Based on these comments about a movie, summarize the general sentiment and key discussion points in Traditional Chinese (Hong Kong style). Comments:\n{comments_preview}"
    try:
        response = await deepseek_client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
        )
        return response.choices[0].message.content
    except:
        return "ÁÑ°Ê≥ïÁî¢ÁîüÁ∏ΩÁµê„ÄÇ"

# =========================
# 6. ‰∏ªÊµÅÁ®ã
# =========================

def movie_comment_analysis(
    movie_title, start_date, end_date, yt_api_key, deepseek_api_key,
    max_videos_per_keyword=30, max_comments_per_video=50, sample_size=None,
    relax_trad_filter=True, cantonese_threshold=2.0, auto_relax_threshold=True,
    target_min_cantonese=300, prefer_hk_videos=True
):
    target_sample = sample_size if sample_size and sample_size > 0 else 1000
    GLOBAL_MAX_COMMENTS = max(2000, target_sample * 4)
    GLOBAL_MAX_VIDEOS = 150
    SEARCH_KEYWORDS = generate_search_queries(movie_title)

    youtube_client = build("youtube", "v3", developerKey=yt_api_key)
    deepseek_client = openai.AsyncOpenAI(api_key=deepseek_api_key, base_url="https://api.deepseek.com/v1")

    # 1. ÊêúÂ∞ã
    video_ids, video_meta = search_youtube_videos(
        SEARCH_KEYWORDS, youtube_client, max_videos_per_keyword, start_date, end_date,
        add_language_bias=True, region_bias=True, max_total_videos=GLOBAL_MAX_VIDEOS
    )
    if not video_ids: return None, "Êâæ‰∏çÂà∞Áõ∏ÈóúÂΩ±Áâá„ÄÇ", None
    
    # 2. Áõ∏ÈóúÊÄßÈÅéÊøæ
    relevant_video_ids = asyncio.run(filter_videos_by_relevance(movie_title, video_ids, video_meta, deepseek_client))
    if not relevant_video_ids: return None, "AI ÈÅéÊøæÂæåÁÑ°Áõ∏ÈóúÂΩ±Áâá„ÄÇ", None
    
    # 3. Ë©≥ÊÉÖËàáÂàÜÊï∏
    video_extra, channel_country_map = fetch_video_and_channel_details(relevant_video_ids, youtube_client)
    hk_score_map = {vid: compute_hk_video_score(vid, video_meta, video_extra, channel_country_map) for vid in relevant_video_ids}
    
    sorted_ids = sorted(relevant_video_ids, key=lambda v: hk_score_map.get(v, 0), reverse=True) if prefer_hk_videos else relevant_video_ids

    # 4. ÊäìÂèñÁïôË®Ä
    df_comments = get_all_comments(
        sorted_ids, youtube_client, max_comments_per_video,
        video_meta, hk_score_map, video_extra, channel_country_map, GLOBAL_MAX_COMMENTS
    )
    if df_comments.empty: return None, "Êâæ‰∏çÂà∞‰ªª‰ΩïÁïôË®Ä„ÄÇ", None

    st.info(f"Â∑≤ÊäìÂèñ {len(df_comments)} ÂâáÂéüÂßãÁïôË®ÄÔºåÈñãÂßãÈÄ≤Ë°åÂö¥Ê†ºÁØ©ÈÅ∏ÔºàÊéíÈô§Èùû HK È†ªÈÅìÁöÑÈùûÁ≤µË™ûÁïôË®ÄÔºâ...")

    # 5. Ë™ûË®ÄËàáÊÉÖÂ¢ÉÁØ©ÈÅ∏ (Êõ¥Êñ∞ÈÇèËºØ)
    cc_t2s = OpenCC("t2s")
    cc_s2t = OpenCC("s2t")
    
    df_comments["lang_pred"] = df_comments["comment_text"].apply(lambda x: classify_zh_trad_simp(x, cc_t2s, cc_s2t))
    df_comments["cantonese_score"] = df_comments["comment_text"].apply(score_cantonese)
    
    def is_target_audience(row):
        text_score = row["cantonese_score"]
        country = row["video_channel_country"] # e.g., 'HK', 'TW', 'US'
        lang = row["lang_pred"]
        
        # Ê¢ù‰ª∂ A: ÁµïÂ∞çÂÑ™ÂÖà - Âº∑Á≤µË™ûÁâπÂæµ (Score >= 2.0)
        # ÁÑ°Ë´ñÈ†ªÈÅìÊòØÂì™ÂúãÁöÑÔºåÂè™Ë¶ÅÁïôË®ÄÊòØÂº∑Á≤µË™ûÔºåÊàëÂÄëÂ∞±Êî∂ (ÂÅáË®≠ÊòØÊµ∑Â§ñÊ∏Ø‰∫∫)
        if text_score >= 2.0:
            return True
            
        # Ê¢ù‰ª∂ B: È†ªÈÅìÁî¢Âú∞ÁØ©ÈÅ∏
        # Â¶ÇÊûúÈ†ªÈÅìÊòØ 'HK'ÔºåÊàëÂÄëÂÖÅË®±Ê®ôÊ∫ñÁπÅÈ´î‰∏≠Êñá„ÄÅËã±Êñá„ÄÅÊàñÂº±Á≤µË™û
        if country == 'HK':
            if lang in ["zh-Hant", "zh-unkn", "en"]:
                return True
            if text_score >= 0.5: # Âº±Á≤µË™û
                return True
                
        # Ê¢ù‰ª∂ C: Èùû HK È†ªÈÅì (Â¶Ç TW, US, CN)
        # ÈÄôË£°ÊàëÂÄëÂü∑Ë°åÂö¥Ê†ºÈÅéÊøæÔºöÂ¶ÇÊûú‰∏çÊòØÂº∑Á≤µË™û (Â∑≤Á∂ìÂú® A Ë¢´ÊäìËµ∞)ÔºåÂâáÂÖ®ÈÉ®‰∏üÊ£Ñ
        # ÈÄôÊÑèÂë≥Ëëó TW È†ªÈÅìÁöÑ "ÁúüÁöÑÂæàÂ•ΩÁúã" ÊúÉË¢´‰∏üÊ£ÑÔºåUS È†ªÈÅìÁöÑ "Good movie" ÊúÉË¢´‰∏üÊ£Ñ
        # Âè™Êúâ HK È†ªÈÅìÁöÑ "Good movie" ÊúÉË¢´‰øùÁïô
        
        return False

    df_comments["is_target"] = df_comments.apply(is_target_audience, axis=1)
    df_filtered = df_comments[df_comments["is_target"]].reset_index(drop=True)

    if df_filtered.empty: return None, "ÁØ©ÈÅ∏ÂæåÁÑ°Á¨¶ÂêàÊ¢ù‰ª∂ÁöÑÁïôË®Ä„ÄÇ", None

    # 6. Êó•ÊúüËàáÂèñÊ®£
    df_filtered["published_at"] = pd.to_datetime(df_filtered["published_at"], utc=True, errors="coerce")
    df_filtered["published_at_hk"] = df_filtered["published_at"].dt.tz_convert("Asia/Hong_Kong")
    start_dt = pd.to_datetime(start_date).tz_localize("Asia/Hong_Kong")
    end_dt = pd.to_datetime(end_date).tz_localize("Asia/Hong_Kong") + timedelta(days=1)
    df_filtered = df_filtered[(df_filtered["published_at_hk"] >= start_dt) & (df_filtered["published_at_hk"] < end_dt)].reset_index(drop=True)
    
    if df_filtered.empty: return None, "Êó•ÊúüÁØÑÂúçÂÖßÁÑ°ÁïôË®Ä„ÄÇ", None
    
    df_analyze = df_filtered.sample(n=sample_size, random_state=42) if sample_size and 0 < sample_size < len(df_filtered) else df_filtered
    
    # 7. ÂàÜÊûê
    analysis_results = asyncio.run(run_all_analyses(df_analyze, deepseek_client))
    final_df = pd.concat([df_analyze.reset_index(drop=True), pd.DataFrame(analysis_results)], axis=1)
    final_df["published_at"] = pd.to_datetime(final_df["published_at"])
    
    # 8. ÁîüÊàêÁ∏ΩÁµê
    ai_summary = asyncio.run(generate_summary(final_df, deepseek_client))
    
    return final_df, None, ai_summary

# =========================
# 7. UI
# =========================

st.set_page_config(page_title="YouTube ÈõªÂΩ±Ë©ïË´ñ AI ÂàÜÊûêÔºàÈ¶ôÊ∏ØÁ≤µË™ûÂÑ™ÂÖàÔºâ", layout="wide")
st.title("üé¨ YouTube ÈõªÂΩ±Ë©ïË´ñ AI ÊÉÖÊÑüÂàÜÊûêÔºàÈ¶ôÊ∏ØÁ≤µË™ûÂÑ™ÂÖàÔºâ")

with st.expander("‰ΩøÁî®Ë™™Êòé"):
    st.markdown("""
    **ÁØ©ÈÅ∏ÈÇèËºØÊõ¥Êñ∞Ôºö**
    1.  **Âº∑Á≤µË™û‰øùÁïô**ÔºöÁÑ°Ë´ñÈ†ªÈÅìÂúãÂÆ∂ÔºåÂè™Ë¶ÅÁïôË®ÄÂåÖÂê´Âº∑ÁÉàÁ≤µË™ûÂè£Ë™ûÁâπÂæµÔºå‰∏ÄÂæã‰øùÁïô„ÄÇ
    2.  **Áî¢Âú∞Âö¥Ê†ºÈÅéÊøæ**Ôºö
        *   **È¶ôÊ∏ØÈ†ªÈÅì (HK)**Ôºö‰øùÁïôÊ®ôÊ∫ñÁπÅÈ´î‰∏≠Êñá„ÄÅËã±ÊñáÂèäÁ≤µË™ûÁïôË®Ä„ÄÇ
        *   **ÈùûÈ¶ôÊ∏ØÈ†ªÈÅì (TW, US, etc.)**Ôºö**ÂâîÈô§**Ê®ôÊ∫ñ‰∏≠ÊñáËàáËã±ÊñáÁïôË®ÄÔºåÂÉÖ‰øùÁïôÂº∑Á≤µË™ûÁïôË®Ä„ÄÇ
    """)

movie_title = st.text_input("ÈõªÂΩ±ÂêçÁ®±", value="‰πùÈæçÂüéÂØ®‰πãÂúçÂüé")
col1, col2 = st.columns(2)
with col1: start_date = st.date_input("ÈñãÂßãÊó•Êúü", value=datetime.today() - timedelta(days=30))
with col2: end_date = st.date_input("ÁµêÊùüÊó•Êúü", value=datetime.today())
yt_api_key = st.text_input("YouTube API Key", type='password')
deepseek_api_key = st.text_input("DeepSeek API Key", type='password')

st.subheader("ÈÄ≤ÈöéË®≠ÂÆö")
max_videos = st.slider("ÊØèÂÄãÈóúÈçµÂ≠óÊêúÂ∞ãÊï∏", 5, 80, 30)
max_comments = st.slider("ÊØèÈÉ®ÂΩ±ÁâáÁïôË®ÄÊï∏", 10, 200, 80)
sample_size = st.number_input("ÂàÜÊûê‰∏äÈôê", 0, 5000, 500)

if st.button("üöÄ ÈñãÂßãÂàÜÊûê"):
    if not all([movie_title, yt_api_key, deepseek_api_key]):
        st.warning("Ë´ãÂ°´ÂØ´ÊâÄÊúâÊ¨Ñ‰Ωç„ÄÇ")
    else:
        with st.spinner("AI ÂàÜÊûê‰∏≠..."):
            df_result, err, summary = movie_comment_analysis(
                movie_title, str(start_date), str(end_date), yt_api_key, deepseek_api_key,
                max_videos, max_comments, sample_size
            )
        if err: st.error(err)
        else:
            st.success("ÂÆåÊàêÔºÅ")
            
            # AI Á∏ΩÁµê
            st.subheader("ü§ñ AI Ë©ïË´ñÁ∏ΩÁµê")
            st.info(summary)

            # Êï∏ÊìöÈ†êË¶Ω
            st.dataframe(df_result.head(10), use_container_width=True)
            
            # Ë¶ñË¶∫ÂåñÂçÄÂüü
            st.markdown("---")
            st.subheader("üìä Êï∏ÊìöË¶ñË¶∫Âåñ")
            
            # Row 1: ÊÉÖÊÑüÂàÜ‰Ωà & ‰∏ªÈ°åÂàÜ‰Ωà
            c1, c2 = st.columns(2)
            with c1:
                st.markdown("### ÊÉÖÊÑüÂàÜ‰Ωà")
                vc = df_result['sentiment'].value_counts()
                fig_pie = px.pie(values=vc.values, names=vc.index, color=vc.index, 
                               color_discrete_map={'Positive':'#5cb85c','Negative':'#d9534f','Neutral':'#f0ad4e'})
                st.plotly_chart(fig_pie, use_container_width=True)
                
            with c2:
                st.markdown("### Ë®éË´ñ‰∏ªÈ°åÂàÜ‰Ωà")
                df_topic = df_result[df_result['topic'] != 'N/A']
                if not df_topic.empty:
                    topic_counts = df_topic['topic'].value_counts().reset_index()
                    topic_counts.columns = ['Topic', 'Count']
                    fig_bar = px.bar(topic_counts, x='Count', y='Topic', orientation='h', color='Count', color_continuous_scale='Blues')
                    fig_bar.update_layout(yaxis={'categoryorder':'total ascending'})
                    st.plotly_chart(fig_bar, use_container_width=True)

            # Row 2: ÊôÇÈñìË∂®Âã¢
            st.markdown("### üìÖ ÊÉÖÊÑüË∂®Âã¢ËÆäÂåñ")
            df_result['date_only'] = df_result['published_at'].dt.date
            trend_data = df_result.groupby(['date_only', 'sentiment']).size().reset_index(name='count')
            fig_line = px.line(trend_data, x='date_only', y='count', color='sentiment', 
                             color_discrete_map={'Positive':'#5cb85c','Negative':'#d9534f','Neutral':'#f0ad4e'},
                             markers=True)
            st.plotly_chart(fig_line, use_container_width=True)

            # Row 3: ÁÜ±ÈñÄÈóúÈçµÂ≠ó (Á∞°ÂñÆÊñ∑Ë©û)
            st.markdown("### üîë ÁÜ±ÈñÄÈóúÈçµË©û")
            # Á∞°ÂñÆÁöÑÂÅúÁî®Ë©ûÈÅéÊøæ
            stopwords = set(['ÁöÑ', '‰∫Ü', 'ÊòØ', 'Êàë', '‰Ω†', '‰ªñ', 'ÈÉΩ', 'Â∞±', 'Âú®', '‰πü', 'Êúâ', 'Âéª', 'Â•Ω', 'Áùá', '‰∫∫', 'Áâá', 'ÈõªÂΩ±', 'Áúü', '‰øÇ', 'Âîî', 'ÂíÅ', 'Èªû', 'Êó¢', 'ÂòÖ', 'Âíó'])
            all_text = "".join(df_result['comment_text'].tolist())
            # Á∞°ÂñÆÁöÑ n-gram ÊàñÁµêÂ∑¥ÂàÜË©ûÈÄôË£°Áî®Ê≠£ÂâáÁ∞°ÂñÆÂàáÂàÜ‰∏≠ÊñáË©û
            words = re.findall(r'[\u4e00-\u9fa5]{2,}', all_text)
            filtered_words = [w for w in words if w not in stopwords]
            word_counts = Counter(filtered_words).most_common(20)
            
            if word_counts:
                wc_df = pd.DataFrame(word_counts, columns=['Word', 'Frequency'])
                fig_wc = px.bar(wc_df, x='Word', y='Frequency', color='Frequency', color_continuous_scale='Greens')
                st.plotly_chart(fig_wc, use_container_width=True)

            st.download_button("üì• ‰∏ãËºâ CSV", df_result.to_csv(index=False, encoding='utf-8-sig'), f"{movie_title}_analysis.csv", "text/csv")
