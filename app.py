import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
from datetime import datetime, timedelta
import time
import asyncio
import openai
import re
import json
from opencc import OpenCC
from googleapiclient.discovery import build

# =========================
# 0. ç·©å­˜èˆ‡å·¥å…·è¨­å®š (Caching & Utils)
# =========================

# ç·©å­˜æ™‚é–“è¨­å®š
CACHE_TTL_SEARCH = 3600          # 1 å°æ™‚ï¼šæœå°‹çµæœã€å½±ç‰‡ç´°ç¯€
CACHE_TTL_COMMENTS = 900         # 15 åˆ†é˜ï¼šç•™è¨€æ¸…å–®

def _get_cached_value(cache_name: str, key, ttl_seconds: int):
    """ç²å–ç·©å­˜æ•¸æ“š"""
    cache = st.session_state.setdefault(cache_name, {})
    entry = cache.get(key)
    if entry and (time.time() - entry["ts"] <= ttl_seconds):
        return entry["value"]
    return None

def _set_cached_value(cache_name: str, key, value):
    """å¯«å…¥ç·©å­˜æ•¸æ“š"""
    cache = st.session_state.setdefault(cache_name, {})
    cache[key] = {"value": value, "ts": time.time()}

# ç²µèªç‰¹å¾µè©åº« (ä¿æŒåŸæœ‰é‚è¼¯ä½œç‚ºç¬¬ä¸€é“å¿«é€Ÿç¯©é¸)
CANTONESE_CHAR_TOKENS = {
    "å””": 1.0, "å†‡": 1.6, "å’—": 1.6, "å˜…": 1.6, "å•²": 1.2, "å—°": 1.2, "ä½¢": 1.0,
    "å–º": 1.6, "åšŸ": 1.6, "å’ª": 1.2, "å•±": 1.2, "æ‚": 1.2, "éš": 1.2, "æ›³": 1.2,
    "æ”°": 1.2, "å’": 1.0, "å™‰": 1.0, "å¾—": 0.6, "å–": 0.8, "å†§": 1.0, "æ’š": 1.2,
    "ä»†": 1.2, "å±Œ": 1.2, "å—®": 1.0, "ç•€": 0.8, "æ¸": 1.0
}
CANTONESE_PARTICLES = ["å•¦", "å›‰", "å–", "å’©", "å‘¢", "å‘€", "å˜›", "å–‡"]
CANTONESE_PHRASES = {
    "å¥½å””å¥½ç‡": 2.0, "åšå’©": 1.6, "é»è§£": 1.2, "å’©æ–™": 1.6, "ç®—å•¦": 1.2,
    "å¾—å•¦": 1.2, "æ­£å–": 1.2, "å¹¾å¥½ç‡": 1.6, "å¹¾æ­£": 1.2, "å¥½æ­£": 1.0,
    "æœ‰å•²": 0.8, "å—°å•²": 1.2, "å‘¢å•²": 1.2, "è¬›çœŸ": 0.8, "å¥½ä¼¼": 0.5
}
ROMANIZATION_RE = re.compile(r"(?i)(?<![A-Za-z])(la|lor|wor|leh|meh|mah|ga|wo|ar)(?=[\s\W]|$)")

def score_cantonese(text: str) -> float:
    """è¨ˆç®—ç²µèªç‰¹å¾µåˆ†æ•¸"""
    if not isinstance(text, str) or not text: return 0.0
    score = 0.0
    for phrase, w in CANTONESE_PHRASES.items():
        if phrase in text: score += text.count(phrase) * w
    for ch, w in CANTONESE_CHAR_TOKENS.items():
        if ch in text: score += text.count(ch) * w
    for p in CANTONESE_PARTICLES:
        if p in text: score += 0.4
    if ROMANIZATION_RE.search(text):
        score += 0.5
    return score

def generate_search_queries(movie_title: str):
    """ç”Ÿæˆæœå°‹é—œéµå­—"""
    # ç‚ºäº†ç¯€çœ APIï¼Œæˆ‘å€‘ç²¾ç°¡é—œéµå­—ï¼Œä¾é  DeepSeek å¾ŒæœŸéæ¿¾
    base = [
        f"{movie_title} å½±è©•",
        f"{movie_title} è©•åƒ¹",
        f"{movie_title} è§€å¾Œæ„Ÿ",
        f"{movie_title} é¦™æ¸¯",
        f"{movie_title} review",
        f"{movie_title} reaction"
    ]
    return base

# =========================
# 1. DeepSeek è¼”åŠ©åŠŸèƒ½ (AI Helpers)
# =========================

async def check_video_relevance_async(video_list, movie_title, deepseek_client):
    """
    ä½¿ç”¨ DeepSeek æ‰¹é‡åˆ¤æ–·è¦–é »æ¨™é¡Œæ˜¯å¦çœŸçš„èˆ‡é›»å½±ç›¸é—œã€‚
    video_list: list of dict {'id': vid, 'title': title}
    """
    if not video_list:
        return []
    
    # æ§‹é€  Prompt
    titles_text = "\n".join([f"{i}. {v['title']}" for i, v in enumerate(video_list)])
    system_prompt = (
        f"You are a strict movie content filter. Identify which of the following video titles are strictly discussing the movie '{movie_title}'. "
        "Exclude generic vlogs, travel guides, or unrelated news unless they explicitly mention the movie context. "
        "Return a JSON object with a single key 'relevant_indices' containing the list of integer indices (0-based) of the relevant titles."
    )

    try:
        response = await deepseek_client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": titles_text}
            ],
            response_format={"type": "json_object"},
            temperature=0.1,
        )
        data = json.loads(response.choices[0].message.content)
        indices = data.get("relevant_indices", [])
        
        valid_ids = []
        for idx in indices:
            if 0 <= idx < len(video_list):
                valid_ids.append(video_list[idx]['id'])
        return valid_ids
    except Exception as e:
        st.warning(f"DeepSeek è¦–é »éæ¿¾å¤±æ•—ï¼Œå°‡ä¿ç•™æ‰€æœ‰è¦–é »: {e}")
        return [v['id'] for v in video_list]

# =========================
# 2. YouTube API æ ¸å¿ƒ (Cached & Optimized)
# =========================

def search_youtube_videos_optimized(
    keywords, youtube_client, deepseek_client, movie_title,
    max_per_keyword, max_total_videos,
    start_date, end_date
):
    """
    å„ªåŒ–ç‰ˆæœå°‹ï¼š
    1. ç·©å­˜æœå°‹çµæœ
    2. å…¨å±€ç¸½é‡æ§åˆ¶
    3. DeepSeek æ¨™é¡Œéæ¿¾ (å¤§å¹…æ¸›å°‘ç„¡é—œè¦–é »)
    """
    all_video_ids = set()
    video_meta = {}
    
    search_cache_name = "yt_search_cache"
    
    # é€²åº¦æ¢
    progress_text = "æ­£åœ¨æœå°‹ YouTube å½±ç‰‡..."
    my_bar = st.progress(0, text=progress_text)
    
    total_keywords = len(keywords)
    
    for idx, query in enumerate(keywords):
        # æª¢æŸ¥å…¨å±€ä¸Šé™
        if len(all_video_ids) >= max_total_videos:
            break
            
        cache_key = f"{query}_{start_date}_{end_date}_{max_per_keyword}"
        cached_records = _get_cached_value(search_cache_name, cache_key, CACHE_TTL_SEARCH)
        
        query_records = []
        
        if cached_records is None:
            # æ²’ç·©å­˜ï¼ŒCall API
            try:
                request = youtube_client.search().list(
                    q=query, part="id,snippet", type="video", maxResults=max_per_keyword,
                    publishedAfter=f"{start_date}T00:00:00Z", publishedBefore=f"{end_date}T23:59:59Z",
                    order="relevance", safeSearch="none", relevanceLanguage="zh-Hant", regionCode="HK"
                )
                response = request.execute()
                for item in response.get("items", []):
                    vid = item["id"]["videoId"]
                    snip = item.get("snippet", {})
                    query_records.append({
                        "id": vid,
                        "title": snip.get("title", ""),
                        "channelTitle": snip.get("channelTitle", ""),
                        "publishedAt": snip.get("publishedAt", "")
                    })
                # å¯«å…¥ç·©å­˜
                _set_cached_value(search_cache_name, cache_key, query_records)
            except Exception as e:
                st.warning(f"æœå°‹ '{query}' å¤±æ•—: {e}")
        else:
            query_records = cached_records

        # æ”¶é›†å…ƒæ•¸æ“š
        temp_video_list = []
        for rec in query_records:
            if rec["id"] not in all_video_ids:
                video_meta[rec["id"]] = rec
                temp_video_list.append(rec)
        
        # === DeepSeek ä»‹å…¥ï¼šéæ¿¾ç„¡é—œæ¨™é¡Œ ===
        if temp_video_list:
            # ç•°æ­¥é‹è¡Œ DeepSeek éæ¿¾
            valid_ids = asyncio.run(check_video_relevance_async(temp_video_list, movie_title, deepseek_client))
            
            # åªæ·»åŠ é€šé AI é©—è­‰çš„ ID
            for vid in valid_ids:
                all_video_ids.add(vid)
                if len(all_video_ids) >= max_total_videos:
                    break
        
        my_bar.progress((idx + 1) / total_keywords, text=f"æœå°‹ä¸­... å·²æ‰¾åˆ° {len(all_video_ids)} éƒ¨ç›¸é—œå½±ç‰‡")

    my_bar.empty()
    return list(all_video_ids), video_meta

def fetch_channel_details_cached(channel_ids, youtube_client):
    """ç·©å­˜ç‰ˆï¼šç²å–é »é“åœ°å€è³‡è¨Š"""
    channel_country = {}
    channels_to_fetch = []
    cache_name = "yt_channel_cache"
    
    # å…ˆæŸ¥ç·©å­˜
    for cid in channel_ids:
        cached = _get_cached_value(cache_name, cid, CACHE_TTL_SEARCH) # é »é“è³‡è¨Šå¯ç·©å­˜ä¹…ä¸€é»
        if cached is not None:
            channel_country[cid] = cached
        else:
            channels_to_fetch.append(cid)
            
    # æ‰¹é‡æŠ“å–æœªç·©å­˜çš„
    if channels_to_fetch:
        for i in range(0, len(channels_to_fetch), 50):
            chunk = channels_to_fetch[i:i+50]
            try:
                resp = youtube_client.channels().list(
                    part="brandingSettings", id=",".join(chunk)
                ).execute()
                for item in resp.get("items", []):
                    cid = item.get("id")
                    country = item.get("brandingSettings", {}).get("channel", {}).get("country", "Unknown")
                    channel_country[cid] = country
                    _set_cached_value(cache_name, cid, country)
            except Exception:
                pass
                
    return channel_country

def get_all_comments_optimized(
    video_ids, youtube_client, 
    max_per_video, max_total_comments,
    video_meta, channel_country_map
):
    """
    å„ªåŒ–ç‰ˆç•™è¨€æŠ“å–ï¼š
    1. ç·©å­˜ç•™è¨€
    2. å…¨å±€ç¸½é‡æ§åˆ¶
    3. æ™ºèƒ½æ¨™è¨˜è¦–é »ä¾†æº (HK Score)
    """
    all_comments = []
    comments_cache_name = "yt_comments_cache"
    
    progress_bar = st.progress(0, text="æŠ“å–ç•™è¨€ä¸­...")
    total_videos = len(video_ids)
    
    # ç²å–è¦–é »è©³æƒ…ä»¥å¾—åˆ° channelId (ç‚ºäº†æŸ¥åœ°å€)
    video_channel_map = {}
    # é€™è£¡ç°¡åŒ–è™•ç†ï¼Œå‡è¨­ video_ids å·²ç¶“ç¶“éç¯©é¸ã€‚
    # ç‚ºäº†çœé…é¡ï¼Œæˆ‘å€‘åªå°çœŸæ­£è¦æŠ“ç•™è¨€çš„è¦–é »å»æŸ¥ channelId
    # å¯¦éš›æ“ä½œä¸­ï¼Œvideos.list æ¶ˆè€—è¼ƒå° (1 unit)ï¼Œå¯ä»¥æ‰¹é‡åš
    
    # æ‰¹é‡ç²å– video details (channelId)
    for i in range(0, len(video_ids), 50):
        chunk = video_ids[i:i+50]
        try:
            resp = youtube_client.videos().list(
                part="snippet", id=",".join(chunk)
            ).execute()
            for item in resp.get("items", []):
                video_channel_map[item["id"]] = item["snippet"]["channelId"]
        except: pass

    for i, vid in enumerate(video_ids):
        # å…¨å±€ä¸Šé™æª¢æŸ¥
        if len(all_comments) >= max_total_comments:
            st.info(f"å·²é”åˆ°å…¨å±€ç•™è¨€ä¸Šé™ ({max_total_comments})ï¼Œåœæ­¢æŠ“å–ã€‚")
            break

        cache_key = f"comments_{vid}_{max_per_video}"
        cached_comments = _get_cached_value(comments_cache_name, cache_key, CACHE_TTL_COMMENTS)
        
        video_comments = []
        
        if cached_comments is not None:
            video_comments = cached_comments
        else:
            # Call API
            try:
                request = youtube_client.commentThreads().list(
                    part="snippet", videoId=vid, textFormat="plainText",
                    order="relevance", maxResults=min(100, max_per_video)
                )
                response = request.execute()
                for item in response.get("items", []):
                    if len(video_comments) >= max_per_video: break
                    comm = item["snippet"]["topLevelComment"]["snippet"]
                    video_comments.append({
                        "comment_text": comm.get("textDisplay", ""),
                        "published_at": comm.get("publishedAt", ""),
                        "like_count": comm.get("likeCount", 0)
                    })
                # å¯«å…¥ç·©å­˜
                _set_cached_value(comments_cache_name, cache_key, video_comments)
            except Exception:
                pass # è©•è«–å¯èƒ½è¢«é—œé–‰
        
        # æ•´åˆæ•¸æ“š
        cid = video_channel_map.get(vid)
        country = channel_country_map.get(cid, "Unknown")
        title = video_meta.get(vid, {}).get("title", "")
        
        # è¨ˆç®—ç°¡å–®çš„ HK Score (ç”¨æ–¼å¾ŒçºŒæ™ºèƒ½èªè¨€ç¯©é¸)
        is_hk_source = (country == "HK") or any(k in title for k in ["é¦™æ¸¯", "ç²µèª", "å»£æ±è©±", "HK"])
        
        for c in video_comments:
            c_copy = c.copy()
            c_copy.update({
                "video_id": vid,
                "video_title": title,
                "video_url": f"https://www.youtube.com/watch?v={vid}",
                "is_hk_source": is_hk_source
            })
            all_comments.append(c_copy)
            if len(all_comments) >= max_total_comments: break
            
        progress_bar.progress((i + 1) / total_videos, text=f"æŠ“å–ç•™è¨€... ({len(all_comments)}/{max_total_comments})")

    progress_bar.empty()
    return pd.DataFrame(all_comments)

# =========================
# 3. DeepSeek åˆ†æèˆ‡æ™ºèƒ½èªè¨€éæ¿¾
# =========================

async def analyze_comment_deepseek_smart(row, deepseek_client, semaphore):
    """
    DeepSeek æ ¸å¿ƒåˆ†æå‡½æ•¸ï¼š
    åŒæ™‚åšï¼šæƒ…æ„Ÿåˆ†æ + ä¸»é¡Œåˆ†é¡ + æ™ºèƒ½èªè¨€/ç›¸é—œæ€§éæ¿¾
    """
    text = row["comment_text"]
    is_hk_source = row["is_hk_source"]
    
    # æ™ºèƒ½èªè¨€é‚è¼¯ï¼š
    # å¦‚æœè¦–é »ä¾†æºæ˜¯é¦™æ¸¯ (is_hk_source=True)ï¼Œæˆ‘å€‘å°è‹±æ–‡ç•™è¨€å¯¬å®¹ (å¯èƒ½æ˜¯é¦™æ¸¯äººè¬›è‹±æ–‡)ã€‚
    # å¦‚æœè¦–é »ä¾†æºä¸æ˜ï¼Œæˆ‘å€‘å°è‹±æ–‡ç•™è¨€åš´æ ¼ (å¯èƒ½æ˜¯å¤–åœ‹äººäº‚å…¥)ï¼Œéœ€è¦å¼·åˆ¶ç²µèªç‰¹å¾µã€‚
    
    system_prompt = (
        "You are a Hong Kong movie analyst. Analyze the comment for the movie. "
        "Output JSON with keys: 'sentiment' (Positive/Negative/Neutral), "
        "'topic' (Plot/Acting/Action/Visuals/Overall/N/A), "
        "'is_relevant_hk_audience' (boolean). "
        "\n\n"
        "Rules for 'is_relevant_hk_audience':\n"
        "1. If the comment is in Cantonese/Traditional Chinese, set True.\n"
        f"2. If the comment is in English: Set {str(is_hk_source).lower()} (based on video source context). "
        "However, if the English comment explicitly mentions Hong Kong cultural context, override to True.\n"
        "3. If Simplified Chinese: Set False unless it uses Cantonese slang.\n"
        "4. If unrelated/spam: Set False."
    )

    async with semaphore:
        try:
            response = await deepseek_client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": text}
                ],
                response_format={"type": "json_object"},
                temperature=0.1,
            )
            return json.loads(response.choices[0].message.content)
        except:
            return {"sentiment": "Error", "topic": "Error", "is_relevant_hk_audience": False}

async def run_deepseek_analysis(df, deepseek_client):
    semaphore = asyncio.Semaphore(50) # ä¸¦ç™¼æ§åˆ¶
    tasks = []
    
    # å°‡ DataFrame è½‰ç‚º list of dict æ–¹ä¾¿è™•ç†
    rows = df.to_dict('records')
    
    for row in rows:
        tasks.append(analyze_comment_deepseek_smart(row, deepseek_client, semaphore))
    
    progress_bar = st.progress(0, text="AI æ™ºèƒ½åˆ†æèˆ‡éæ¿¾ä¸­...")
    results = []
    for i, f in enumerate(asyncio.as_completed(tasks)):
        res = await f
        results.append(res)
        progress_bar.progress((i + 1) / len(rows))
    
    progress_bar.empty()
    
    # åˆä½µçµæœï¼Œæ³¨æ„ç•°æ­¥è¿”å›é †åºå¯èƒ½äº‚ï¼Œé€™è£¡ç°¡å–®è™•ç†å‡è¨­é †åºä¸€è‡´ (as_completed ä¸ä¿è­‰é †åºï¼Œéœ€ä¿®æ­£)
    # ä¿®æ­£ï¼šä½¿ç”¨ gather ä¿è­‰é †åº
    results = await asyncio.gather(*[analyze_comment_deepseek_smart(row, deepseek_client, semaphore) for row in rows])
    
    return pd.DataFrame(results)

# =========================
# 4. ä¸»æµç¨‹
# =========================

def main_analysis_process(
    movie_title, start_date, end_date, yt_api_key, deepseek_api_key,
    max_per_keyword, max_total_videos, max_per_video, max_total_comments
):
    # åˆå§‹åŒ– Clients
    youtube = build("youtube", "v3", developerKey=yt_api_key)
    deepseek = openai.AsyncOpenAI(api_key=deepseek_api_key, base_url="https://api.deepseek.com/v1")
    
    # 1. æœå°‹èˆ‡éæ¿¾ (å« DeepSeek æ¨™é¡Œéæ¿¾)
    keywords = generate_search_queries(movie_title)
    video_ids, video_meta = search_youtube_videos_optimized(
        keywords, youtube, deepseek, movie_title,
        max_per_keyword, max_total_videos, start_date, end_date
    )
    
    if not video_ids:
        return None, "æ‰¾ä¸åˆ°ç›¸é—œå½±ç‰‡ (ç¶“ AI åš´æ ¼æ¨™é¡Œéæ¿¾)ã€‚"
    
    st.info(f"AI éæ¿¾å¾Œä¿ç•™ {len(video_ids)} éƒ¨é«˜åº¦ç›¸é—œå½±ç‰‡ï¼Œé–‹å§‹åˆ†æä¾†æº...")

    # 2. ç²å–é »é“åœ°å€ (ç”¨æ–¼æ™ºèƒ½èªè¨€åˆ¤æ–·)
    # ç‚ºäº†çœé…é¡ï¼Œæˆ‘å€‘éœ€è¦å…ˆæ‹¿åˆ° channel IDs
    # é€™è£¡ç¨å¾®å–å·§ï¼Œå…ˆä¸ call videos.list æ‹¿ channelIdï¼Œç­‰åˆ°æŠ“ comment æ™‚é †ä¾¿æ‹¿ï¼Œæˆ–è€…åªå°å‰ N å€‹æ‹¿
    # ç‚ºäº†æº–ç¢ºæ€§ï¼Œé‚„æ˜¯å¾—æ‹¿ã€‚ä½¿ç”¨ç·©å­˜å„ªåŒ–ã€‚
    temp_vids_chunk = video_ids[:max_total_videos] # å†æ¬¡ç¢ºä¿ä¸è¶…é‡
    
    # å¿«é€Ÿç²å– Channel IDs (æ¶ˆè€— 1 unit per 50 videos)
    vid_to_cid = {}
    for i in range(0, len(temp_vids_chunk), 50):
        try:
            resp = youtube.videos().list(part="snippet", id=",".join(temp_vids_chunk[i:i+50])).execute()
            for item in resp.get("items", []):
                vid_to_cid[item["id"]] = item["snippet"]["channelId"]
        except: pass
        
    channel_ids = list(set(vid_to_cid.values()))
    channel_country_map = fetch_channel_details_cached(channel_ids, youtube)
    
    # 3. æŠ“å–ç•™è¨€ (å«å…¨å±€ç¸½é‡æ§åˆ¶)
    df_comments = get_all_comments_optimized(
        temp_vids_chunk, youtube, max_per_video, max_total_comments,
        video_meta, channel_country_map
    )
    
    if df_comments.empty:
        return None, "æ‰¾ä¸åˆ°ä»»ä½•ç•™è¨€ã€‚"
        
    st.info(f"å·²æŠ“å– {len(df_comments)} å‰‡åŸå§‹ç•™è¨€ï¼Œæ­£åœ¨é€²è¡Œ DeepSeek æ™ºèƒ½èªç¾©åˆ†æèˆ‡ç¯©é¸...")
    
    # 4. DeepSeek çµ‚æ¥µåˆ†æ (æƒ…æ„Ÿ + æ™ºèƒ½èªè¨€éæ¿¾)
    analysis_df = asyncio.run(run_deepseek_analysis(df_comments, deepseek))
    
    # åˆä½µä¸¦éæ¿¾
    final_df = pd.concat([df_comments, analysis_df], axis=1)
    
    # æ‡‰ç”¨ "is_relevant_hk_audience" éæ¿¾
    original_count = len(final_df)
    final_df = final_df[final_df["is_relevant_hk_audience"] == True].copy()
    filtered_count = len(final_df)
    
    st.success(f"åˆ†æå®Œæˆï¼AI å‰”é™¤äº† {original_count - filtered_count} å‰‡éç›®æ¨™å—çœ¾(ç´”å¤–èª/ç°¡é«”/ç„¡é—œ)ç•™è¨€ï¼Œä¿ç•™ {filtered_count} å‰‡æœ‰æ•ˆç²µèª/é¦™æ¸¯è§€é»ç•™è¨€ã€‚")
    
    # æ ¼å¼åŒ–æ™‚é–“ä¾›åœ–è¡¨ä½¿ç”¨
    final_df["published_at"] = pd.to_datetime(final_df["published_at"])
    
    return final_df, None

# =========================
# 5. Streamlit UI
# =========================

st.set_page_config(page_title="YouTube é›»å½±è©•è«– AI åˆ†æ (Pro)", layout="wide")
st.title("ğŸ¬ YouTube é›»å½±è©•è«– AI åˆ†æ (Pro ç‰ˆ)")
st.markdown("### ğŸš€ æ™ºèƒ½çœæµç‰ˆï¼šDeepSeek æ·±åº¦ä»‹å…¥ + å…¨å±€é…é¡æ§åˆ¶")

with st.sidebar:
    st.header("è¨­å®š")
    yt_api_key = st.text_input("YouTube API Key", type='password')
    deepseek_api_key = st.text_input("DeepSeek API Key", type='password')
    
    st.divider()
    st.subheader("é…é¡èˆ‡éæ¿¾æ§åˆ¶")
    max_total_videos = st.number_input("å…¨å±€æœ€å¤§å½±ç‰‡åˆ†ææ•¸", 10, 200, 50, help="é”åˆ°æ­¤æ•¸é‡å¾Œåœæ­¢æœå°‹ï¼Œç¯€çœé…é¡")
    max_total_comments = st.number_input("å…¨å±€æœ€å¤§ç•™è¨€åˆ†ææ•¸", 50, 2000, 500, help="é”åˆ°æ­¤æ•¸é‡å¾Œåœæ­¢æŠ“å–ï¼Œç¯€çœé…é¡")
    
    st.divider()
    max_per_keyword = st.slider("å–®é—œéµå­—æœå°‹ä¸Šé™", 10, 50, 20)
    max_per_video = st.slider("å–®å½±ç‰‡ç•™è¨€ä¸Šé™", 20, 100, 50)

col1, col2, col3 = st.columns([2, 1, 1])
with col1:
    movie_title = st.text_input("é›»å½±åç¨±", value="ä¹é¾åŸå¯¨ä¹‹åœåŸ")
with col2:
    start_date = st.date_input("é–‹å§‹æ—¥æœŸ", value=datetime.today() - timedelta(days=30))
with col3:
    end_date = st.date_input("çµæŸæ—¥æœŸ", value=datetime.today())

if st.button("ğŸš€ é–‹å§‹æ™ºèƒ½åˆ†æ", type="primary"):
    if not all([movie_title, yt_api_key, deepseek_api_key]):
        st.error("è«‹å¡«å¯«æ‰€æœ‰ API Key å’Œé›»å½±åç¨±")
    else:
        with st.spinner("æ­£åœ¨èª¿ç”¨ AI é€²è¡Œå¤šå±¤æ¬¡åˆ†æ... (æœå°‹çµæœå°‡è¢«ç·©å­˜)"):
            df_result, err = main_analysis_process(
                movie_title, str(start_date), str(end_date),
                yt_api_key, deepseek_api_key,
                max_per_keyword, max_total_videos, max_per_video, max_total_comments
            )
            
        if err:
            st.error(err)
        else:
            # =========================
            # Visualization (ä¿æŒåŸæ¨£)
            # =========================
            st.divider()
            st.subheader("ğŸ“Š åˆ†æçµæœå¯è¦–åŒ–")
            
            # 1. æƒ…æ„Ÿåˆ†ä½ˆ
            sentiments_order = ['Positive', 'Negative', 'Neutral']
            colors_map = {'Positive': '#5cb85c', 'Negative': '#d9534f', 'Neutral': '#f0ad4e'}
            
            c1, c2 = st.columns(2)
            with c1:
                vc = df_result['sentiment'].value_counts()
                fig1 = px.pie(values=vc.values, names=vc.index, title='æ•´é«”æƒ…æ„Ÿåˆ†ä½ˆ', 
                              color=vc.index, color_discrete_map=colors_map)
                st.plotly_chart(fig1, use_container_width=True)
            
            with c2:
                # ä¸»é¡Œåˆ†ä½ˆ
                if 'topic' in df_result.columns:
                    topic_counts = df_result['topic'].value_counts()
                    fig2 = px.bar(x=topic_counts.index, y=topic_counts.values, title='è©•è«–ä¸»é¡Œåˆ†ä½ˆ',
                                  labels={'x': 'ä¸»é¡Œ', 'y': 'æ•¸é‡'})
                    st.plotly_chart(fig2, use_container_width=True)

            # 2. æ™‚é–“è¶¨å‹¢
            if not df_result.empty:
                df_result['date'] = df_result['published_at'].dt.date
                daily = df_result.groupby(['date', 'sentiment']).size().unstack().fillna(0)
                daily = daily.reindex(columns=sentiments_order).dropna(axis=1, how='all')
                
                if not daily.empty:
                    daily_long = daily.reset_index().melt(id_vars='date', var_name='sentiment', value_name='count')
                    fig3 = px.line(daily_long, x='date', y='count', color='sentiment',
                                   title='æ¯æ—¥æƒ…æ„Ÿè¶¨å‹¢', color_discrete_map=colors_map)
                    st.plotly_chart(fig3, use_container_width=True)

            # 3. æ•¸æ“šè¡¨
            st.subheader("ğŸ“ è©³ç´°æ•¸æ“š (å«ä¾†æºæ¨™è¨˜)")
            st.dataframe(
                df_result[['sentiment', 'topic', 'comment_text', 'video_title', 'is_hk_source', 'published_at']], 
                use_container_width=True
            )
            
            csv = df_result.to_csv(index=False).encode('utf-8-sig')
            st.download_button("ğŸ“¥ ä¸‹è¼‰ CSV", csv, "analysis_result.csv", "text/csv")
