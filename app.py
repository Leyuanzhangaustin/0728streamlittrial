# app.py

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import time
import re

# ========== æ£€æŸ¥æ˜¯å¦ä¸ºç¹ä½“ä¸­æ–‡ ==========
def is_traditional_chinese(text):
    """
    Returns True if text is mostly Traditional Chinese.
    Simple heuristic: if there are more Traditional than Simplified chars, treat as Traditional.
    """
    # å¸¸è§ç®€ä½“å­—é›†åˆ
    simplified_chars = set("ä»¬ä½“ä¸ºäº§ä¸¾ä¹ä¹¡ä¹¦ä¹°ä¹±äº‰äºšä¼ªä¼—ä¼˜ä¼šä¼ ä¼¤ä¼¦ä½“ä»·ä¼—ä¼˜ä¼¤ä¼¦ä½“ä»·ä¼—ä¼˜ä¼¤ä¼¦ä½“ä»·ä¼—ä¼˜ä¼¤ä¼¦ä½“ä»·ä¼—ä¼˜ä¼¤ä¼¦ä½“ä»·ä¼—ä¼˜ä¼¤ä¼¦ä½“ä»·ä¼—ä¼˜ä¼¤ä¼¦ä½“ä»·ä¼—ä¼˜ä¼¤ä¼¦ä½“ä»·ä¼—ä¼˜ä¼¤ä¼¦ä½“ä»·ä¼—ä¼˜ä¼¤ä¼¦ä½“ä»·ä¼—ä¼˜ä¼¤ä¼¦ä½“ä»·ä¼—ä¼˜ä¼¤ä¼¦ä½“ä»·ä¼—ä¼˜ä¼¤ä¼¦ä½“ä»·ä¼—ä¼˜ä¼¤ä¼¦ä½“ä»·ä¼—ä¼˜ä¼¤ä¼¦ä½“ä»·")
    traditional_chars = set("å€‘é«”ç‚ºç”¢èˆ‰æ¨‚é„‰æ›¸è²·äº‚çˆ­äºå½çœ¾å„ªæœƒå‚³å‚·å€«é«”åƒ¹")
    # ç»Ÿè®¡å‡ºç°çš„ç¹ç®€ä½“å­—ç¬¦æ•°
    simplified_count = sum(1 for c in text if c in simplified_chars)
    traditional_count = sum(1 for c in text if c in traditional_chars)
    # è‹¥ç¹ä½“å­—ç¬¦å¤šäºç®€ä½“å­—ç¬¦ï¼Œåˆ™è§†ä¸ºç¹ä½“
    return traditional_count >= simplified_count and traditional_count > 0

# ========== 1. YouTube æœç´¢ ==========
def search_youtube_videos(keywords, youtube_client, max_per_keyword, start_date, end_date):
    all_video_ids = set()
    for query in keywords:
        try:
            search_response = youtube_client.search().list(
                q=query,
                part='id,snippet',
                type='video',
                maxResults=max_per_keyword,
                publishedAfter=f"{start_date}T00:00:00Z",
                publishedBefore=f"{end_date}T23:59:59Z",
                relevanceLanguage='zh-Hant',
                regionCode='HK'
            ).execute()
            video_ids = [item['id']['videoId'] for item in search_response.get('items', [])]
            all_video_ids.update(video_ids)
            time.sleep(0.5)
        except Exception as e:
            continue
    return list(all_video_ids)

# ========== 2. æ‰¹é‡æŠ“å–è¯„è®º ==========
def get_all_comments(video_ids, youtube_client, max_per_video):
    all_comments = []
    for video_id in video_ids:
        try:
            request = youtube_client.commentThreads().list(
                part='snippet', videoId=video_id, textFormat='plainText', maxResults=100
            )
            comments_fetched = 0
            while request and comments_fetched < max_per_video:
                response = request.execute()
                for item in response['items']:
                    if comments_fetched >= max_per_video:
                        break
                    comment = item['snippet']['topLevelComment']['snippet']
                    all_comments.append({
                        'video_id': video_id,
                        'comment_text': comment['textDisplay'],
                        'published_at': comment['publishedAt'],
                        'like_count': comment['likeCount']
                    })
                    comments_fetched += 1
                if comments_fetched >= max_per_video:
                    break
                request = youtube_client.commentThreads().list_next(request, response)
        except Exception as e:
            continue
    return pd.DataFrame(all_comments)

# ========== 3. DeepSeek AIæƒ…æ„Ÿåˆ†æ ==========
def analyze_comment_deepseek(comment_text, deepseek_client, max_retries=3):
    import json
    if not isinstance(comment_text, str) or len(comment_text.strip()) < 5:
        return {"sentiment": "Invalid", "topic": "N/A", "summary": "Comment too short or invalid."}
    system_prompt = (
        "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é¦™æ¸¯å¸‚åœºèˆ†æƒ…åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹ç”µå½±è¯„è®ºï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ç»“æœã€‚"
        "JSONå¯¹è±¡å¿…é¡»åŒ…å«ä¸‰ä¸ªé”®ï¼š"
        "1. 'sentiment': å…¶å€¼å¿…é¡»æ˜¯ 'Positive', 'Negative', æˆ– 'Neutral'ã€‚"
        "2. 'topic': è¯„è®ºè®¨è®ºçš„æ ¸å¿ƒä¸»é¢˜ï¼Œä¾‹å¦‚ 'å‰§æƒ…', 'æ¼”å‘˜æ¼”æŠ€', 'åŠ¨ä½œè®¾è®¡', 'ç”»é¢ç¾æœ¯', 'ç”µå½±èŠ‚å¥', 'æ•´ä½“æ„Ÿè§‰'ã€‚å¦‚æœæ— æ³•åˆ¤æ–­ï¼Œåˆ™ä¸º 'N/A'ã€‚"
        "3. 'summary': ç”¨ä¸€å¥è¯ç®€æ½”æ€»ç»“è¯„è®ºçš„æ ¸å¿ƒè§‚ç‚¹ã€‚"
        "ç¡®ä¿è¾“å‡ºåªæœ‰JSONå¯¹è±¡ï¼Œæ— ä»»ä½•é¢å¤–æ–‡å­—ã€‚"
    )
    for attempt in range(max_retries):
        try:
            response = deepseek_client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": comment_text}
                ],
                response_format={"type": "json_object"},
                temperature=0.1,
            )
            analysis_result = json.loads(response.choices[0].message.content)
            return analysis_result
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
            else:
                return {"sentiment": "Error", "topic": "Error", "summary": f"API Error: {e}"}

# ========== 4. ä¸»æµç¨‹ ==========
def movie_comment_analysis(
    movie_title, start_date, end_date,
    yt_api_key, deepseek_api_key,
    max_videos_per_keyword=30, max_comments_per_video=50, sample_size=None
):
    # å…³é”®è¯
    SEARCH_KEYWORDS = [
        f'"{movie_title}" é å‘Š',
        f'"{movie_title}" å½±è©•',
        f'"{movie_title}" åˆ†æ',
        f'"{movie_title}" å¥½å””å¥½ç‡',
        f'"{movie_title}" è¨è«–',
        f'"{movie_title}" reaction'
    ]
    # APIåˆå§‹åŒ–
    from googleapiclient.discovery import build
    youtube_client = build('youtube', 'v3', developerKey=yt_api_key)
    import openai
    deepseek_client = openai.OpenAI(
        api_key=deepseek_api_key,
        base_url="https://api.deepseek.com/v1"
    )

    # æœç´¢è§†é¢‘
    video_ids = search_youtube_videos(
        SEARCH_KEYWORDS, youtube_client, max_videos_per_keyword, start_date, end_date
    )
    if not video_ids:
        return None, "æœªæ‰¾åˆ°ç›¸å…³è§†é¢‘"
    # æŠ“å–è¯„è®º
    df_comments = get_all_comments(video_ids, youtube_client, max_comments_per_video)
    if df_comments.empty:
        return None, "æ²¡æœ‰æŠ“åˆ°è¯„è®º"
    # æ—¶é—´å¤„ç†
    df_comments['published_at'] = pd.to_datetime(df_comments['published_at'], utc=True)
    df_comments['published_at_hk'] = df_comments['published_at'].dt.tz_convert('Asia/Hong_Kong')
    # æŒ‰é¦™æ¸¯æ—¶åŒºæ—¶é—´è¿‡æ»¤
    start = pd.to_datetime(start_date).tz_localize('Asia/Hong_Kong')
    end = pd.to_datetime(end_date).tz_localize('Asia/Hong_Kong') + timedelta(days=1)
    mask = (df_comments['published_at_hk'] >= start) & (df_comments['published_at_hk'] <= end)
    df_comments = df_comments.loc[mask].reset_index(drop=True)
    if df_comments.empty:
        return None, "æ²¡æœ‰ç¬¦åˆæ—¥æœŸèŒƒå›´çš„è¯„è®º"
    # ------ æ–°å¢: è¿‡æ»¤éç¹ä½“è¯„è®º ------
    df_comments = df_comments[df_comments['comment_text'].apply(is_traditional_chinese)].reset_index(drop=True)
    if df_comments.empty:
        return None, "æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„ç¹ä½“ä¸­æ–‡è¯„è®º"
    # æŠ½æ ·
    if sample_size and sample_size < len(df_comments):
        df_analyze = df_comments.sample(n=sample_size, random_state=42)
    else:
        df_analyze = df_comments

    # AIæƒ…æ„Ÿåˆ†æ
    from tqdm import tqdm
    tqdm.pandas(desc="AIæƒ…æ„Ÿåˆ†æ")
    analysis_results = df_analyze['comment_text'].progress_apply(
        lambda x: analyze_comment_deepseek(x, deepseek_client)
    )
    analysis_df = pd.json_normalize(analysis_results)
    final_df = pd.concat([df_analyze.reset_index(drop=True), analysis_df], axis=1)
    final_df['published_at'] = pd.to_datetime(final_df['published_at'])
    return final_df, None

# ========== 5. Streamlit UI ==========
st.set_page_config(page_title="ç”µå½±YouTubeè¯„è®ºAIåˆ†æ", layout="wide")
st.title("ğŸ¬ YouTube ç”µå½±è¯„è®ºAIæƒ…æ„Ÿåˆ†æ")

with st.expander("æ“ä½œè¯´æ˜"):
    st.markdown("""
    1. è¾“å…¥ç”µå½±åç§°ã€åˆ†ææ—¶é—´èŒƒå›´ã€API KEYã€‚
    2. å¯è‡ªå®šä¹‰æ¯ç»„å…³é”®è¯æœ€å¤§è§†é¢‘æ•°åŠæ¯ä¸ªè§†é¢‘æœ€å¤§è¯„è®ºæ•°ã€‚
    3. ç‚¹å‡»â€œå¼€å§‹åˆ†æâ€æŒ‰é’®ï¼Œè‡ªåŠ¨æŠ“å–è¯„è®ºï¼Œé€æ¡è°ƒç”¨AIåˆ†ææƒ…æ„Ÿä¸ä¸»é¢˜ã€‚
    4. åˆ†æå®Œæˆåå¯æµè§ˆå¯è§†åŒ–ç»“æœï¼Œå¹¶ä¸‹è½½æ˜ç»†ã€‚
    """)

movie_title = st.text_input("ç”µå½±åç§°", value="")
col1, col2 = st.columns(2)
with col1:
    start_date = st.date_input("èµ·å§‹æ—¥æœŸ", value=datetime.today() - timedelta(days=30))
with col2:
    end_date = st.date_input("ç»“æŸæ—¥æœŸ", value=datetime.today())
yt_api_key = st.text_input("YouTube API Key", type='password')
deepseek_api_key = st.text_input("DeepSeek API Key", type='password')
max_videos = st.slider("æ¯ç»„å…³é”®è¯æœ€å¤šè§†é¢‘æ•°", 5, 50, 10)
max_comments = st.slider("æ¯ä¸ªè§†é¢‘æœ€å¤šè¯„è®ºæ•°", 5, 100, 20)
sample_size = st.number_input("æœ€å¤šåˆ†æè¯„è®ºæ•°ï¼ˆ0ä¸ºå…¨é‡ï¼‰", 0, 2000, 0)

if st.button("ğŸš€ å¼€å§‹åˆ†æ"):
    if not all([movie_title, yt_api_key, deepseek_api_key]):
        st.warning("è¯·å¡«å†™æ‰€æœ‰å†…å®¹ã€‚")
    else:
        with st.spinner("AIåˆ†æä¸­ï¼Œè¯·è€å¿ƒç­‰å¾…...ï¼ˆå¦‚è¯„è®ºæ•°å¤šï¼Œéœ€æ•°åˆ†é’Ÿï¼‰"):
            df_result, err = movie_comment_analysis(
                movie_title, str(start_date), str(end_date),
                yt_api_key, deepseek_api_key,
                max_videos, max_comments, sample_size or None
            )
        if err:
            st.error(err)
        else:
            st.success("åˆ†æå®Œæˆï¼")
            st.dataframe(df_result.head(20))

            # ========== å¯è§†åŒ– (Visualization in English) ==========
            st.subheader("1. Sentiment Distribution")
            fig1, ax1 = plt.subplots(figsize=(4, 4))
            valmap = {
                "Positive": "Positive",
                "Negative": "Negative",
                "Neutral": "Neutral",
                "Invalid": "Invalid",
                "Error": "Error"
            }
            df_result['sentiment_en'] = df_result['sentiment'].map(lambda x: valmap.get(str(x).capitalize(), x))
            df_result['sentiment_en'].value_counts().plot.pie(
                autopct='%.1f%%',
                ax=ax1,
                colors=['#5cb85c', '#d9534f', '#f0ad4e', '#cccccc', '#888888']
            )
            ax1.set_title('Sentiment Distribution')
            ax1.set_ylabel('')
            st.pyplot(fig1, use_container_width=False)

            st.subheader("2. Daily Sentiment Trend (Bar)")
            df_result['date'] = df_result['published_at_hk'].dt.date
            daily = df_result.groupby(['date', 'sentiment_en']).size().unstack().fillna(0)
            fig2, ax2 = plt.subplots(figsize=(8, 3))
            daily.plot(kind='bar', stacked=True, ax=ax2, width=0.8, color=['#5cb85c', '#d9534f', '#f0ad4e', '#cccccc', '#888888'])
            ax2.set_title('Daily Sentiment Trend (Bar)')
            ax2.set_xlabel('Date')
            ax2.set_ylabel('Number of Comments')
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            st.pyplot(fig2, use_container_width=True)

            st.subheader("3. Daily Sentiment Trend (Line)")
            fig3, ax3 = plt.subplots(figsize=(8, 3))
            for sentiment in ['Positive', 'Negative', 'Neutral']:
                if sentiment in daily.columns:
                    ax3.plot(daily.index, daily[sentiment], marker='o', label=sentiment)
            ax3.set_title("Daily Sentiment Trend (Line)")
            ax3.set_xlabel("Date")
            ax3.set_ylabel("Number of Comments")
            ax3.legend(title="Sentiment")
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            st.pyplot(fig3, use_container_width=True)

            st.subheader("4. ä¸‹è½½åˆ†ææ˜ç»†")
            csv = df_result.to_csv(index=False, encoding='utf-8-sig')
            st.download_button("ä¸‹è½½å…¨éƒ¨åˆ†ææ˜ç»†CSV", csv, file_name=f"{movie_title}_analysis.csv", mime='text/csv')

else:
    st.info("è¯·å¡«å†™ä¿¡æ¯å¹¶ç‚¹å‡»â€œå¼€å§‹åˆ†æâ€")
