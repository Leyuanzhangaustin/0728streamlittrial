import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import time
import asyncio
import openai
import re
import json
from googleapiclient.discovery import build
from collections import Counter

# ... (å‰é¢çš„ç·©å­˜å‡½æ•¸ _get_cached_value, _set_cached_value ä¿æŒä¸è®Š) ...

# =========================
# 0. å·¥å…·è¨­å®š (æ›´æ–°)
# =========================

CACHE_TTL_SEARCH = 3600
CACHE_TTL_COMMENTS = 900

def _get_cached_value(cache_name: str, key, ttl_seconds: int):
    cache = st.session_state.setdefault(cache_name, {})
    entry = cache.get(key)
    if entry and (time.time() - entry["ts"] <= ttl_seconds):
        return entry["value"]
    return None

def _set_cached_value(cache_name: str, key, value):
    cache = st.session_state.setdefault(cache_name, {})
    cache[key] = {"value": value, "ts": time.time()}

def generate_search_queries(movie_title: str):
    return [
        f"{movie_title}",
        f"{movie_title} å½±è©•",
        f"{movie_title} è§€å¾Œæ„Ÿ",
        f"{movie_title} review",
        f"{movie_title} é›»å½±"
    ]

# =========================
# 1. YouTube API æ ¸å¿ƒ (å«æ”¿æ²»/ç„¡é—œå…§å®¹éæ¿¾)
# =========================

def search_youtube_videos_smart(
    keywords, youtube_client, movie_title,
    max_per_keyword, max_total_videos,
    start_date, end_date,
    negative_keywords_list  # æ–°å¢ï¼šè² é¢é—œéµè©åˆ—è¡¨
):
    all_video_ids = set()
    video_meta = {}
    search_cache_name = "yt_search_smart_filtered_cache"
    
    progress_text = "æ­£åœ¨æœå°‹ä¸¦éæ¿¾ç„¡é—œ/æ”¿æ²»å½±ç‰‡..."
    my_bar = st.progress(0, text=progress_text)
    
    # 1. æº–å‚™æ­£å‘é—œéµè© (é›»å½±åæ‹†åˆ†)
    # ä¾‹å¦‚ "ä¹é¾åŸå¯¨ä¹‹åœåŸ" -> ["ä¹é¾åŸå¯¨", "åœåŸ"]
    title_keywords = [k for k in re.split(r'\s+|ï¼š|:|,|ï¼Œ', movie_title) if len(k) > 1]
    
    # 2. æº–å‚™è² é¢é—œéµè© (ç¡¬ç·¨ç¢¼åŸºç¤ + ç”¨æˆ¶è¼¸å…¥)
    # é€™äº›è©å‡ºç¾åœ¨æ¨™é¡Œä¸­é€šå¸¸ä»£è¡¨æ˜¯æ™‚æ”¿æ–°èè€Œéå½±è©•
    base_negative_keywords = [
        "æ–°è", "ç›´æ’­", "æ–½æ”¿å ±å‘Š", "ç¿’è¿‘å¹³", "ä¸­å…±", "å…±ç”¢é»¨", 
        "ç‰¹é¦–", "æå®¶è¶…", "ç«‹æ³•æœƒ", "ç¤ºå¨", "æ”¿æ²»", "æ”¿ç¶“", 
        "å¤§ç´€å…ƒ", "æ–‡æ˜­", "æ±Ÿå³°", "å¤©äº®æ™‚åˆ†", "æ™‚äº‹", "è²¡ç¶“"
    ]
    # åˆä½µç”¨æˆ¶å®šç¾©çš„æ’é™¤è©
    final_negative_keywords = list(set(base_negative_keywords + negative_keywords_list))

    for idx, query in enumerate(keywords):
        if len(all_video_ids) >= max_total_videos: break
            
        cache_key = f"{query}_{start_date}_{end_date}_{max_per_keyword}_filtered"
        cached_records = _get_cached_value(search_cache_name, cache_key, CACHE_TTL_SEARCH)
        
        query_records = []
        
        if cached_records is None:
            try:
                request = youtube_client.search().list(
                    q=query, part="id,snippet", type="video", maxResults=max_per_keyword,
                    publishedAfter=f"{start_date}T00:00:00Z", publishedBefore=f"{end_date}T23:59:59Z",
                    order="relevance", safeSearch="none", relevanceLanguage="zh-Hant"
                )
                response = request.execute()
                for item in response.get("items", []):
                    vid = item["id"]["videoId"]
                    snip = item.get("snippet", {})
                    title = snip.get("title", "")
                    desc = snip.get("description", "")
                    channel_title = snip.get("channelTitle", "")
                    
                    # === æ ¸å¿ƒä¿®æ”¹ï¼šé›™é‡éæ¿¾é‚è¼¯ ===
                    
                    # A. è² é¢éæ¿¾ (Negative Filter) - å„ªå…ˆç´šæœ€é«˜
                    # å¦‚æœæ¨™é¡Œæˆ–é »é“ååŒ…å«æ”¿æ²»æ•æ„Ÿè©ï¼Œç›´æ¥è·³é
                    if any(nk in title for nk in final_negative_keywords) or \
                       any(nk in channel_title for nk in final_negative_keywords):
                        continue 

                    # B. æ­£å‘ç›¸é—œæ€§æª¢æŸ¥ (Positive Relevance)
                    is_relevant = False
                    
                    # B1. æ¨™é¡Œå¿…é ˆåŒ…å«è‡³å°‘ä¸€å€‹é›»å½±æ ¸å¿ƒè©
                    # é€™æ˜¯ç‚ºäº†é˜²æ­¢ YouTube æ¨é€å®Œå…¨ç„¡é—œçš„ "çŒœä½ å–œæ­¡"
                    if any(tk.lower() in title.lower() for tk in title_keywords):
                        is_relevant = True
                    
                    # B2. å¦‚æœæ¨™é¡Œæ²’æœ‰æ ¸å¿ƒè©ï¼Œä½†æè¿°è£¡æœ‰å®Œæ•´é›»å½±åï¼Œä¹Ÿå¯ä»¥æ¥å— (é˜²æ­¢æ¨™é¡Œé»¨)
                    elif movie_title.lower() in desc.lower():
                        is_relevant = True
                        
                    if is_relevant:
                        query_records.append({
                            "id": vid,
                            "title": title,
                            "channelTitle": channel_title,
                            "publishedAt": snip.get("publishedAt", "")
                        })
                
                _set_cached_value(search_cache_name, cache_key, query_records)
            except Exception as e:
                st.warning(f"æœå°‹ '{query}' å¤±æ•—: {e}")
        else:
            query_records = cached_records

        for rec in query_records:
            if rec["id"] not in all_video_ids:
                all_video_ids.add(rec["id"])
                video_meta[rec["id"]] = rec
                if len(all_video_ids) >= max_total_videos: break
        
        my_bar.progress((idx + 1) / len(keywords), text=f"æœå°‹ä¸­... å·²éæ¿¾æ”¿æ²»/ç„¡é—œå…§å®¹ï¼Œä¿ç•™: {len(all_video_ids)} éƒ¨")

    my_bar.empty()
    return list(all_video_ids), video_meta

# ... (get_all_comments_cached å‡½æ•¸ä¿æŒä¸è®Šï¼Œä½¿ç”¨ä¸Šä¸€ç‰ˆçš„å‹•æ…‹èª¿æ•´é‚è¼¯) ...
def get_all_comments_cached(video_ids, youtube_client, max_per_video, max_total_comments, video_meta):
    all_comments = []
    comments_cache_name = "yt_comments_cache_v4" # Update version
    
    progress_bar = st.progress(0, text="æŠ“å–ç•™è¨€ä¸­...")
    
    # å‹•æ…‹èª¿æ•´ï¼šå½±ç‰‡å°‘å‰‡æŠ“æ›´å¤šè©•è«–
    if len(video_ids) > 0 and len(video_ids) < 5:
        adjusted_max_per_video = max_per_video * 4 # æå‡å€ç‡
        st.caption(f"âš ï¸ ç¶“éæ¿¾å¾Œå½±ç‰‡ä¾†æºè¼ƒå°‘ï¼Œè‡ªå‹•å°‡å–®ä¸€å½±ç‰‡ç•™è¨€æŠ“å–ä¸Šé™å¤§å¹…æå‡è‡³ {adjusted_max_per_video} å‰‡")
    else:
        adjusted_max_per_video = max_per_video

    for i, vid in enumerate(video_ids):
        if len(all_comments) >= max_total_comments: break

        cache_key = f"comments_{vid}_{adjusted_max_per_video}"
        cached_comments = _get_cached_value(comments_cache_name, cache_key, CACHE_TTL_COMMENTS)
        
        video_comments = []
        if cached_comments is not None:
            video_comments = cached_comments
        else:
            try:
                request = youtube_client.commentThreads().list(
                    part="snippet", videoId=vid, textFormat="plainText",
                    order="relevance", maxResults=min(100, adjusted_max_per_video)
                )
                response = request.execute()
                for item in response.get("items", []):
                    if len(video_comments) >= adjusted_max_per_video: break
                    comm = item["snippet"]["topLevelComment"]["snippet"]
                    video_comments.append({
                        "comment_text": comm.get("textDisplay", ""),
                        "published_at": comm.get("publishedAt", ""),
                        "like_count": comm.get("likeCount", 0)
                    })
                _set_cached_value(comments_cache_name, cache_key, video_comments)
            except: pass
        
        title = video_meta.get(vid, {}).get("title", "")
        for c in video_comments:
            c_copy = c.copy()
            c_copy.update({"video_id": vid, "video_title": title})
            all_comments.append(c_copy)
            if len(all_comments) >= max_total_comments: break
            
        progress_bar.progress((i + 1) / len(video_ids), text=f"æŠ“å–ç•™è¨€... ({len(all_comments)}/{max_total_comments})")

    progress_bar.empty()
    return pd.DataFrame(all_comments)

# =========================
# 2. DeepSeek åˆ†æ (ä¿®å¤ RuntimeError ç‰ˆ)
# =========================

async def analyze_comment_deepseek_v2(row, client, semaphore):
    """
    å–®å€‹è©•è«–åˆ†æå‡½æ•¸
    """
    text = row["comment_text"]
    video_title = row.get("video_title", "")
    
    system_prompt = (
        "You are a movie analyst focusing on the Hong Kong market. "
        f"The comment is from a video titled: '{video_title}'. "
        "Analyze the comment. "
        "Output JSON with keys: "
        "'sentiment' (Positive/Negative/Neutral), "
        "'keywords' (Extract 1-2 main keywords in Traditional Chinese), "
        "'is_target_audience' (boolean). "
        "\n\n"
        "Rules for 'is_target_audience':\n"
        "1. **TRUE** if it is a relevant movie review/reaction in Cantonese, Traditional Chinese, or mixed English.\n"
        "2. **FALSE** if it is purely about politics (e.g., discussing government policies, CCP, democracy) without relating to the movie plot.\n"
        "3. **FALSE** if it is Simplified Chinese (unless clearly HK slang).\n"
        "4. **FALSE** if it is spam or ads."
    )

    async with semaphore:
        try:
            # è¨­ç½®è¶…æ™‚ï¼Œé˜²æ­¢å¡æ­»
            response = await client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": text}
                ],
                response_format={"type": "json_object"},
                temperature=0.1,
                timeout=15  # å¢åŠ è¶…æ™‚è¨­å®š
            )
            content = response.choices[0].message.content
            if not content:
                return {"sentiment": "Error", "keywords": "", "is_target_audience": False}
            return json.loads(content)
        except Exception as e:
            # æ•æ‰éŒ¯èª¤ä½†ä¸ä¸­æ–·ç¨‹åºï¼Œè¿”å›é»˜èªå¤±æ•—å€¼
            return {"sentiment": "Error", "keywords": f"Error: {str(e)}", "is_target_audience": False}

async def run_deepseek_analysis(df, api_key):
    """
    ä¸»ç•°æ­¥æ§åˆ¶å™¨ï¼šè² è²¬åˆå§‹åŒ– Client å’Œç®¡ç†ä½µç™¼
    """
    # === é—œéµä¿®å¾©ï¼šåœ¨ç•°æ­¥å‡½æ•¸å…§éƒ¨åˆå§‹åŒ– Client ===
    # é€™æ¨£å¯ä»¥ç¢ºä¿ Client ç¶å®šåˆ°æ­£ç¢ºçš„ Event Loop
    client = openai.AsyncOpenAI(api_key=api_key, base_url="https://api.deepseek.com/v1")
    
    semaphore = asyncio.Semaphore(20) # é™ä½ä½µç™¼æ•¸ä»¥æ±‚ç©©å®š (50 -> 20)
    rows = df.to_dict('records')
    
    progress_bar = st.progress(0, text="AI æ­£åœ¨åˆ†æ (å·²å•Ÿç”¨æ”¿æ²»å…§å®¹éæ¿¾)...")
    
    # å‰µå»ºä»»å‹™åˆ—è¡¨
    tasks = [analyze_comment_deepseek_v2(row, client, semaphore) for row in rows]
    
    # åŸ·è¡Œä»»å‹™ä¸¦æ›´æ–°é€²åº¦æ¢
    results = []
    total = len(tasks)
    
    # ä½¿ç”¨ as_completed æ›´æ–°é€²åº¦æ¢
    for i, task in enumerate(asyncio.as_completed(tasks)):
        await task # ç­‰å¾…ä»»æ„ä¸€å€‹å®Œæˆ
        progress_bar.progress((i + 1) / total)
    
    # === é—œéµä¿®å¾©ï¼šæ”¶é›†çµæœä¸¦å®¹éŒ¯ ===
    # return_exceptions=True ç¢ºä¿å³ä½¿æŸå€‹ä»»å‹™å ±éŒ¯ï¼Œä¹Ÿä¸æœƒæ‹‹å‡º RuntimeError
    results_raw = await asyncio.gather(*tasks, return_exceptions=True)
    
    # æ¸…ç†ï¼šé—œé–‰ Client é€£æ¥
    await client.close()
    progress_bar.empty()
    
    # è™•ç†å¯èƒ½çš„ç•°å¸¸çµæœ
    clean_results = []
    for res in results_raw:
        if isinstance(res, Exception):
            clean_results.append({"sentiment": "Error", "keywords": "System Error", "is_target_audience": False})
        else:
            clean_results.append(res)
            
    return pd.DataFrame(clean_results)

# =========================
# 3. ä¸»æµç¨‹ (ä¿®å¾©ç‰ˆ)
# =========================

def main_process(movie_title, start_date, end_date, yt_api_key, deepseek_api_key, 
                 max_per_keyword, max_total_videos, max_per_video, max_total_comments,
                 negative_keywords):
    
    youtube = build("youtube", "v3", developerKey=yt_api_key)
    # æ³¨æ„ï¼šé€™è£¡ä¸å†åˆå§‹åŒ– DeepSeek Clientï¼Œæ”¹åœ¨ç•°æ­¥å‡½æ•¸å…§éƒ¨åˆå§‹åŒ–
    
    # 1. æœå°‹ (å‚³å…¥è² é¢é—œéµè©)
    keywords = generate_search_queries(movie_title)
    video_ids, video_meta = search_youtube_videos_smart(
        keywords, youtube, movie_title,
        max_per_keyword, max_total_videos, start_date, end_date,
        negative_keywords
    )
    
    if not video_ids:
        return None, f"æ‰¾ä¸åˆ°ç›¸é—œå½±ç‰‡ã€‚è«‹æª¢æŸ¥é›»å½±åç¨±ï¼Œæˆ–å˜—è©¦æ¸›å°‘è² é¢é—œéµè©ã€‚"
    
    st.info(f"éæ¿¾æ”¿æ²»/ç„¡é—œå…§å®¹å¾Œï¼Œé–å®š {len(video_ids)} éƒ¨å½±ç‰‡ï¼Œé–‹å§‹æŠ“å–ç•™è¨€...")
    
    # 2. æŠ“å–
    df_comments = get_all_comments_cached(video_ids, youtube, max_per_video, max_total_comments, video_meta)
    
    if df_comments.empty:
        return None, "é€™äº›å½±ç‰‡ä¸‹æ²’æœ‰æ‰¾åˆ°ç•™è¨€ã€‚"
    
    # 3. AI åˆ†æ
    try:
        # === é—œéµä¿®å¾©ï¼šå‚³å…¥ API Key å­—ç¬¦ä¸² ===
        analysis_df = asyncio.run(run_deepseek_analysis(df_comments, deepseek_api_key))
    except Exception as e:
        return None, f"AI åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}"

    final_df = pd.concat([df_comments, analysis_df], axis=1)
    
    # 4. ç¯©é¸
    original_len = len(final_df)
    # ç¢ºä¿ is_target_audience æ˜¯å¸ƒæ—å€¼ï¼Œé˜²æ­¢ AI è¿”å›éŒ¯èª¤æ ¼å¼å°è‡´å ±éŒ¯
    final_df["is_target_audience"] = final_df["is_target_audience"].fillna(False).astype(bool)
    
    final_df = final_df[final_df["is_target_audience"] == True].copy()
    filtered_len = len(final_df)
    
    st.success(f"åˆ†æå®Œæˆï¼åŸå§‹æŠ“å– {original_len} å‰‡ï¼ŒAI å‰”é™¤éæ¸¯å¼/æ”¿æ²»é›¢é¡Œå…§å®¹å¾Œï¼Œå‰©é¤˜ {filtered_len} å‰‡æœ‰æ•ˆè©•è«–ã€‚")
    
    final_df["published_at"] = pd.to_datetime(final_df["published_at"])
    return final_df, None

# =========================
# 4. UI
# =========================

st.set_page_config(page_title="YouTube å½±è©•åˆ†æ (Anti-Spam)", layout="wide")
st.title("ğŸ¬ YouTube å½±è©•åˆ†æ (æ™ºèƒ½éæ¿¾ç‰ˆ)")
st.markdown("### ç‰¹é»ï¼šæ™ºèƒ½æœå°‹ | ğŸš« è‡ªå‹•éæ¿¾æ”¿æ²»/æ–°èå½±ç‰‡ | ç¹é«”/ç²µèªè­˜åˆ¥")

with st.sidebar:
    st.header("è¨­å®š")
    yt_api_key = st.text_input("YouTube API Key", type='password')
    deepseek_api_key = st.text_input("DeepSeek API Key", type='password')
    st.divider()
    max_total_videos = st.number_input("æœ€å¤§å½±ç‰‡æœå°‹æ•¸", 10, 100, 50)
    max_total_comments = st.number_input("æœ€å¤§åˆ†æç•™è¨€æ•¸", 50, 2000, 500)
    
    st.divider()
    st.subheader("ğŸš« æ’é™¤é—œéµè© (é˜²æ­¢æ”¿æ²»å¹²æ“¾)")
    default_neg = "æ–°è, ç›´æ’­, ç¿’è¿‘å¹³, ä¸­å…±, æ”¿æ²»"
    user_neg_input = st.text_area("è¼¸å…¥è¦æ’é™¤çš„è© (é€—è™Ÿåˆ†éš”)", value=default_neg, help="è‹¥æ¨™é¡ŒåŒ…å«é€™äº›è©ï¼Œå°‡ç›´æ¥å¿½ç•¥è©²å½±ç‰‡")
    negative_keywords = [x.strip() for x in user_neg_input.split(",") if x.strip()]

col1, col2, col3 = st.columns([2, 1, 1])
with col1:
    movie_title = st.text_input("é›»å½±åç¨±", value="ä¹é¾åŸå¯¨") 
with col2:
    start_date = st.date_input("é–‹å§‹", value=datetime.today() - timedelta(days=90))
with col3:
    end_date = st.date_input("çµæŸ", value=datetime.today())

if st.button("ğŸš€ é–‹å§‹åˆ†æ", type="primary"):
    if not all([movie_title, yt_api_key, deepseek_api_key]):
        st.error("è«‹å¡«å¯«æ‰€æœ‰æ¬„ä½")
    else:
        with st.spinner("æ­£åœ¨æœå°‹ä¸¦åŸ·è¡Œé›™é‡éæ¿¾ (é—œéµè© + AI)..."):
            df_result, err = main_process(
                movie_title, str(start_date), str(end_date),
                yt_api_key, deepseek_api_key,
                20, max_total_videos, 50, max_total_comments,
                negative_keywords
            )
            
        if err:
            st.error(err)
        else:
            st.divider()
            
            # ç°¡å–®å±•ç¤ºçµæœ (ä¿ç•™åŸæœ‰çš„å¯è¦–åŒ–ä»£ç¢¼çµæ§‹)
            st.subheader("ğŸ”¥ ç†±é–€è©•è«–é—œéµè©")
            # ... (æ­¤è™•å¯è¦–åŒ–ä»£ç¢¼èˆ‡ä¸Šä¸€ç‰ˆç›¸åŒï¼Œçœç•¥ä»¥ç¯€çœç¯‡å¹…) ...
            all_keywords = []
            for item in df_result['keywords']:
                if isinstance(item, str):
                    words = [w.strip() for w in re.split(r'[ï¼Œ,ã€\s]+', item) if len(w.strip()) > 1]
                    all_keywords.extend(words)
                elif isinstance(item, list):
                    all_keywords.extend([str(w).strip() for w in item if len(str(w).strip()) > 1])
            
            if all_keywords:
                kw_counts = Counter(all_keywords).most_common(15)
                kw_df = pd.DataFrame(kw_counts, columns=['Keyword', 'Count']).sort_values(by='Count')
                fig_kw = px.bar(kw_df, x='Count', y='Keyword', orientation='h', title='Top Keywords')
                st.plotly_chart(fig_kw, use_container_width=True)

            with st.expander("æŸ¥çœ‹è©³ç´°æ•¸æ“š"):
                st.dataframe(df_result[['sentiment', 'keywords', 'comment_text', 'video_title']])
