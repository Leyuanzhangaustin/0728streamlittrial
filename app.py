import streamlit as st
import pandas as pd
from googleapiclient.discovery import build
from textblob import TextBlob
import plotly.express as px
import plotly.graph_objects as go
from collections import Counter
import re
import html
from datetime import datetime

# ==========================================
# 1. è¨­å®šèˆ‡å·¥å…·å‡½å¼
# ==========================================

st.set_page_config(page_title="YouTube Movie Sentiment Analysis Pro", layout="wide")

# åœç”¨è©åˆ—è¡¨ (å¯ä»¥æ ¹æ“šéœ€è¦æ“´å……)
STOPWORDS = set([
    'the', 'a', 'an', 'in', 'on', 'at', 'of', 'to', 'is', 'are', 'was', 'were',
    'and', 'but', 'or', 'so', 'it', 'this', 'that', 'my', 'your', 'his', 'her',
    'movie', 'film', 'video', 'really', 'very', 'just', 'like', 'good', 'bad',
    'watch', 'watching', 'time', 'people', 'think', 'know', 'would', 'could',
    'should', 'get', 'got', 'make', 'made', 'see', 'saw', 'seen', 'one', 'much',
    'many', 'well', 'way', 'even', 'also', 'back', 'go', 'going', 'want',
    'did', 'do', 'does', 'done', 'actually', 'literally', 'thing', 'things',
    'something', 'anything', 'nothing', 'say', 'said', 'says', 'story', 'character',
    'characters', 'plot', 'scene', 'scenes', 'end', 'ending', 'best', 'better',
    'great', 'amazing', 'love', 'loved', 'bit', 'little', 'lot', 'movies',
    'films', 'cinema', 'actor', 'actress', 'director', 'acting', 'show', 'series'
])

def clean_text(text):
    """æ¸…ç†è©•è«–æ–‡å­—ï¼šç§»é™¤ HTML æ¨™ç±¤ã€ç‰¹æ®Šç¬¦è™Ÿ"""
    if not isinstance(text, str):
        return ""
    text = html.unescape(text)
    text = re.sub(r'<[^>]+>', '', text)  # ç§»é™¤ HTML tag
    text = re.sub(r'http\S+', '', text)  # ç§»é™¤ URL
    text = re.sub(r'[^\w\s]', '', text)  # ç§»é™¤æ¨™é»ç¬¦è™Ÿ
    text = text.lower().strip()
    return text

def get_sentiment(text):
    """è¨ˆç®—æƒ…æ„Ÿåˆ†æ•¸ (-1 åˆ° 1)"""
    blob = TextBlob(text)
    return blob.sentiment.polarity

def get_sentiment_label(score):
    """å°‡åˆ†æ•¸è½‰æ›ç‚ºæ¨™ç±¤"""
    if score > 0.1:
        return "Positive"
    elif score < -0.1:
        return "Negative"
    else:
        return "Neutral"

# ==========================================
# 2. YouTube API æ ¸å¿ƒé‚è¼¯ (åŒ…å«éæ¿¾æ©Ÿåˆ¶)
# ==========================================

def search_videos_strict(api_key, query, max_results=5):
    """
    æœå°‹å½±ç‰‡ï¼Œä¸¦åŸ·è¡Œåš´æ ¼çš„æ¨™é¡ŒåŒ¹é…èˆ‡é¡åˆ¥éæ¿¾ã€‚
    """
    youtube = build('youtube', 'v3', developerKey=api_key)
    
    # 1. æ“´å¤§æœå°‹ç¯„åœï¼šè«‹æ±‚æ¯” max_results æ›´å¤šçš„å½±ç‰‡ (ä¾‹å¦‚ 30 å€‹)ï¼Œä»¥ä¾¿éæ¿¾
    fetch_count = 30 
    
    search_response = youtube.search().list(
        q=query,
        part='id,snippet',
        maxResults=fetch_count,
        type='video',
        relevanceLanguage='en', # å„ªå…ˆæœå°‹è‹±æ–‡å…§å®¹ (å¯é¸)
        order='relevance'
    ).execute()

    video_ids = []
    videos_meta = []
    
    # æº–å‚™æ­£è¦è¡¨é”å¼é€²è¡Œä¸åˆ†å¤§å°å¯«çš„åŒ¹é…
    # å°‡ query ä¸­çš„ç©ºæ ¼æ›¿æ›ç‚ºæ­£å‰‡çš„ ".*" ä»¥å…è¨±ä¸­é–“æœ‰å…¶ä»–è© (å¯¬é¬†åŒ¹é…) æˆ–ç›´æ¥åŒ¹é… (åš´æ ¼åŒ¹é…)
    # é€™è£¡æˆ‘å€‘ä½¿ç”¨ç°¡å–®çš„åŒ…å«æª¢æŸ¥
    query_lower = query.lower().strip()
    
    # æš«å­˜åˆæ­¥ç¯©é¸çš„ ID
    temp_ids = []
    temp_snippets = {}

    for item in search_response.get('items', []):
        vid = item['id']['videoId']
        title = item['snippet']['title']
        title_lower = title.lower()
        
        # --- éæ¿¾å±¤ 1: æ¨™é¡Œé—œéµå­—æª¢æŸ¥ ---
        # æª¢æŸ¥æœå°‹è©æ˜¯å¦åœ¨æ¨™é¡Œä¸­ã€‚
        # å¦‚æœæœå°‹è©æ˜¯ä¸­æ–‡ï¼Œç›´æ¥æª¢æŸ¥ï¼›å¦‚æœæ˜¯è‹±æ–‡ï¼Œæª¢æŸ¥å–®è©é‚Šç•Œå¯èƒ½æ›´æº–ç¢ºï¼Œä½†é€™è£¡ç”¨ç°¡å–®åŒ…å«å³å¯ã€‚
        if query_lower not in title_lower:
            # å˜—è©¦è™•ç† "éå¸¸ç›—3" vs "Now You See Me 3" çš„æƒ…æ³
            # å¦‚æœç”¨æˆ¶æœä¸­æ–‡ï¼Œä½†çµæœæ˜¯è‹±æ–‡ï¼Œé€™è£¡æœƒè¢«æ¿¾æ‰ã€‚
            # å»ºè­°ç”¨æˆ¶è¼¸å…¥é›»å½±çš„åŸåæˆ–æœ€å¸¸ç”¨çš„è­¯åã€‚
            continue
            
        temp_ids.append(vid)
        temp_snippets[vid] = item['snippet']

    if not temp_ids:
        return [], []

    # --- éæ¿¾å±¤ 2: é¡åˆ¥æª¢æŸ¥ (Category Check) ---
    # æˆ‘å€‘éœ€è¦å‘¼å« videos().list ä¾†ç²å– categoryId
    videos_response = youtube.videos().list(
        id=','.join(temp_ids),
        part='snippet,statistics'
    ).execute()

    # å®šç¾©æˆ‘å€‘ä¸æƒ³è¦çš„é¡åˆ¥ ID (YouTube API Category IDs)
    # 25: News & Politics (æ–°èæ”¿æ²» - é€™æ˜¯æ…ˆæ¿Ÿ/æ¥Šä¸ç³å½±ç‰‡å¸¸å‡ºç¾çš„åœ°æ–¹)
    # 29: Nonprofits & Activism
    # 19: Travel & Events (æœ‰æ™‚ç„¡é—œ)
    BLOCKED_CATEGORIES = ['25', '29'] 

    filtered_videos = []

    for item in videos_response.get('items', []):
        vid = item['id']
        cat_id = item['snippet'].get('categoryId', '')
        stats = item['statistics']
        snippet = temp_snippets.get(vid, item['snippet']) # ä½¿ç”¨ search çš„ snippet æˆ– video çš„ snippet
        
        # æ’é™¤è¢«å°é–çš„é¡åˆ¥
        if cat_id in BLOCKED_CATEGORIES:
            continue
            
        # å»ºç«‹è³‡æ–™ç‰©ä»¶
        video_data = {
            'video_id': vid,
            'title': snippet['title'],
            'channel': snippet['channelTitle'],
            'published_at': snippet['publishedAt'], # å½±ç‰‡ç™¼ä½ˆæ™‚é–“
            'view_count': int(stats.get('viewCount', 0)),
            'like_count': int(stats.get('likeCount', 0)),
            'comment_count': int(stats.get('commentCount', 0)),
            'thumbnail': snippet['thumbnails']['high']['url']
        }
        filtered_videos.append(video_data)

    # --- éæ¿¾å±¤ 3: æ’åºèˆ‡æˆªæ–· ---
    # æ ¹æ“šè§€çœ‹æ¬¡æ•¸æ’åºï¼Œå–å‰ max_results å€‹
    filtered_videos.sort(key=lambda x: x['view_count'], reverse=True)
    final_videos = filtered_videos[:max_results]
    
    return [v['video_id'] for v in final_videos], final_videos

def get_video_comments(youtube, video_id, max_comments=100):
    """ç²å–å–®å€‹å½±ç‰‡çš„è©•è«– (åŒ…å«æ™‚é–“æˆ³)"""
    comments = []
    try:
        request = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            maxResults=100, # æ¯æ¬¡è«‹æ±‚æœ€å¤š 100
            textFormat="plainText",
            order="relevance" 
        )
        
        while request and len(comments) < max_comments:
            response = request.execute()
            
            for item in response.get("items", []):
                comment_snippet = item["snippet"]["topLevelComment"]["snippet"]
                text = comment_snippet.get("textDisplay", "")
                published_at = comment_snippet.get("publishedAt", "") # é€™æ˜¯è©•è«–ç™¼ä½ˆæ™‚é–“
                like_count = comment_snippet.get("likeCount", 0)
                
                comments.append({
                    "video_id": video_id,
                    "text": text,
                    "published_at": published_at,
                    "like_count": like_count
                })
            
            if len(comments) < max_comments and "nextPageToken" in response:
                request = youtube.commentThreads().list_next(request, response)
            else:
                break
                
    except Exception as e:
        # æŸäº›å½±ç‰‡å¯èƒ½ç¦ç”¨äº†è©•è«–
        print(f"Error fetching comments for {video_id}: {e}")
        
    return comments

def analyze_data(api_key, query, num_videos, num_comments_per_video):
    """ä¸»åˆ†ææµç¨‹"""
    status_text = st.empty()
    progress_bar = st.progress(0)
    
    status_text.text("Step 1/4: Searching and Filtering Videos...")
    progress_bar.progress(10)
    
    # 1. æœå°‹ä¸¦éæ¿¾å½±ç‰‡
    video_ids, videos_meta = search_videos_strict(api_key, query, max_results=num_videos)
    
    if not video_ids:
        status_text.text("No relevant videos found after filtering.")
        progress_bar.progress(100)
        return None, None
    
    status_text.text(f"Found {len(video_ids)} relevant videos. Step 2/4: Fetching Comments...")
    progress_bar.progress(30)
    
    # 2. æŠ“å–è©•è«–
    youtube = build('youtube', 'v3', developerKey=api_key)
    all_comments = []
    
    for i, vid in enumerate(video_ids):
        # æ›´æ–°é€²åº¦
        current_progress = 30 + int((i / len(video_ids)) * 40)
        progress_bar.progress(current_progress)
        
        vid_comments = get_video_comments(youtube, vid, max_comments=num_comments_per_video)
        all_comments.extend(vid_comments)
        
    if not all_comments:
        status_text.text("No comments found on these videos.")
        progress_bar.progress(100)
        return videos_meta, pd.DataFrame()

    status_text.text("Step 3/4: Analyzing Sentiment...")
    progress_bar.progress(80)
    
    # 3. æƒ…æ„Ÿåˆ†æ
    df = pd.DataFrame(all_comments)
    df['clean_text'] = df['text'].apply(clean_text)
    df['sentiment_score'] = df['clean_text'].apply(get_sentiment)
    df['sentiment_label'] = df['sentiment_score'].apply(get_sentiment_label)
    
    # 4. æ™‚é–“è™•ç† (è½‰æ›ç‚ºé¦™æ¸¯æ™‚é–“)
    df['published_at'] = pd.to_datetime(df['published_at'])
    # è½‰æ›æ™‚å€ï¼šå…ˆè½‰ç‚º UTCï¼Œå†è½‰ç‚ºé¦™æ¸¯æ™‚é–“
    if df['published_at'].dt.tz is None:
         df['published_at'] = df['published_at'].dt.tz_localize('UTC')
    df['published_at_hk'] = df['published_at'].dt.tz_convert('Asia/Hong_Kong')
    df['date'] = df['published_at_hk'].dt.date
    
    status_text.text("Analysis Complete!")
    progress_bar.progress(100)
    status_text.empty()
    
    return videos_meta, df

# ==========================================
# 3. Streamlit UI ä»‹é¢
# ==========================================

st.title("ğŸ¬ Smart Movie Review Analyzer")
st.markdown("""
This tool searches for movie reviews on YouTube, **filters out irrelevant content (like news, gossip)**, 
analyzes audience sentiment, and visualizes the trends based on **comment dates**.
""")

with st.sidebar:
    st.header("Configuration")
    api_key = st.text_input("Enter YouTube API Key", type="password")
    movie_name = st.text_input("Movie Name (e.g., Venom 3)", "Venom: The Last Dance")
    
    st.markdown("---")
    st.subheader("Advanced Settings")
    num_videos = st.slider("Number of Videos to Analyze", 1, 10, 5)
    num_comments = st.slider("Max Comments per Video", 50, 500, 100)
    
    start_btn = st.button("Start Analysis", type="primary")

if start_btn and api_key and movie_name:
    try:
        videos_meta, df_comments = analyze_data(api_key, movie_name, num_videos, num_comments)
        
        if videos_meta is None or (isinstance(df_comments, pd.DataFrame) and df_comments.empty):
            st.error(f"Could not find relevant videos or comments for '{movie_name}'. Try using the exact English title.")
        else:
            # --- é¡¯ç¤ºå½±ç‰‡è³‡è¨Š ---
            st.subheader(f"ğŸ“º Analyzed Videos for: {movie_name}")
            st.markdown(f"These videos passed the **strict relevance filter** (Title match & Category check).")
            
            cols = st.columns(len(videos_meta))
            for idx, vid in enumerate(videos_meta):
                with cols[idx % 3]: # ç°¡å–®çš„æ’ç‰ˆï¼Œæ¯è¡Œ3å€‹
                    st.image(vid['thumbnail'], use_container_width=True)
                    st.markdown(f"**{vid['title']}**")
                    st.caption(f"Channel: {vid['channel']} | Views: {vid['view_count']:,}")
            
            st.divider()
            
            # --- 1. é—œéµæŒ‡æ¨™ ---
            st.subheader("ğŸ“Š Sentiment Overview")
            avg_sentiment = df_comments['sentiment_score'].mean()
            sentiment_counts = df_comments['sentiment_label'].value_counts()
            
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("Total Comments", len(df_comments))
            col2.metric("Average Sentiment", f"{avg_sentiment:.2f}", 
                        delta="Positive" if avg_sentiment > 0 else "Negative")
            col3.metric("Positive Comments", sentiment_counts.get("Positive", 0))
            col4.metric("Negative Comments", sentiment_counts.get("Negative", 0))
            
            # --- 2. æƒ…æ„Ÿåˆ†ä½ˆåœ“é¤…åœ– ---
            fig_pie = px.pie(
                names=sentiment_counts.index, 
                values=sentiment_counts.values,
                title="Sentiment Distribution",
                color=sentiment_counts.index,
                color_discrete_map={"Positive": "#00CC96", "Neutral": "#636EFA", "Negative": "#EF553B"}
            )
            st.plotly_chart(fig_pie, use_container_width=True)
            
            # --- 3. æ¯æ—¥æƒ…æ„Ÿè¶¨å‹¢ (åŸºæ–¼è©•è«–æ—¥æœŸ) ---
            st.subheader("ğŸ“… Daily Sentiment Trend (Based on Comment Date)")
            st.info("This chart shows when people commented, not when the video was uploaded.")
            
            # èšåˆæ•¸æ“šï¼šæŒ‰æ—¥æœŸè¨ˆç®—å¹³å‡æƒ…æ„Ÿåˆ†æ•¸å’Œè©•è«–æ•¸é‡
            daily_stats = df_comments.groupby('date').agg(
                avg_sentiment=('sentiment_score', 'mean'),
                comment_count=('sentiment_score', 'count')
            ).reset_index()
            
            # å»ºç«‹é›™è»¸åœ–è¡¨
            fig_trend = go.Figure()
            
            # é•·æ¢åœ–ï¼šè©•è«–æ•¸é‡
            fig_trend.add_trace(go.Bar(
                x=daily_stats['date'],
                y=daily_stats['comment_count'],
                name='Comment Volume',
                marker_color='rgba(200, 200, 200, 0.5)',
                yaxis='y2'
            ))
            
            # ç·šåœ–ï¼šæƒ…æ„Ÿåˆ†æ•¸
            fig_trend.add_trace(go.Scatter(
                x=daily_stats['date'],
                y=daily_stats['avg_sentiment'],
                name='Avg Sentiment',
                mode='lines+markers',
                line=dict(color='#636EFA', width=3)
            ))
            
            fig_trend.update_layout(
                title="Sentiment & Volume Over Time",
                xaxis_title="Date (Hong Kong Time)",
                yaxis=dict(title="Sentiment Score (-1 to 1)", range=[-1, 1]),
                yaxis2=dict(title="Number of Comments", overlaying='y', side='right', showgrid=False),
                legend=dict(x=0, y=1.1, orientation='h'),
                hovermode="x unified"
            )
            st.plotly_chart(fig_trend, use_container_width=True)
            
            # --- 4. æ–‡å­—é›² (ä½¿ç”¨é »ç‡çµ±è¨ˆæ¨¡æ“¬) ---
            st.subheader("â˜ï¸ Most Frequent Words")
            
            # ç°¡å–®çš„è©é »çµ±è¨ˆ
            all_words = ' '.join(df_comments['clean_text']).split()
            filtered_words = [w for w in all_words if w not in STOPWORDS and len(w) > 2]
            word_counts = Counter(filtered_words).most_common(20)
            
            df_words = pd.DataFrame(word_counts, columns=['Word', 'Count'])
            
            fig_bar = px.bar(
                df_words, 
                x='Count', 
                y='Word', 
                orientation='h',
                title="Top 20 Words in Comments",
                color='Count',
                color_continuous_scale='Viridis'
            )
            fig_bar.update_layout(yaxis={'categoryorder':'total ascending'})
            st.plotly_chart(fig_bar, use_container_width=True)
            
            # --- 5. æ•¸æ“šè¡¨æ ¼ ---
            with st.expander("View Raw Data"):
                st.dataframe(df_comments[['date', 'text', 'sentiment_label', 'sentiment_score', 'like_count']])

    except Exception as e:
        st.error(f"An error occurred: {str(e)}")
        st.info("Please check your API Key and internet connection.")

elif start_btn and not api_key:
    st.warning("Please enter your YouTube API Key.")
