import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import time
import asyncio
import openai
import re
import json
from googleapiclient.discovery import build
from collections import Counter

# ... (å‰é¢çš„ç·©å­˜å‡½æ•¸ _get_cached_value, _set_cached_value ä¿æŒä¸è®Š) ...

# =========================
# 0. å·¥å…·è¨­å®š (æ›´æ–°)
# =========================

CACHE_TTL_SEARCH = 3600
CACHE_TTL_COMMENTS = 900

def _get_cached_value(cache_name: str, key, ttl_seconds: int):
    cache = st.session_state.setdefault(cache_name, {})
    entry = cache.get(key)
    if entry and (time.time() - entry["ts"] <= ttl_seconds):
        return entry["value"]
    return None

def _set_cached_value(cache_name: str, key, value):
    cache = st.session_state.setdefault(cache_name, {})
    cache[key] = {"value": value, "ts": time.time()}

def generate_search_queries(movie_title: str):
    return [
        f"{movie_title}",
        f"{movie_title} å½±è©•",
        f"{movie_title} è§€å¾Œæ„Ÿ",
        f"{movie_title} review",
        f"{movie_title} é›»å½±"
    ]

# =========================
# 1. YouTube API æ ¸å¿ƒ (å«æ”¿æ²»/ç„¡é—œå…§å®¹éæ¿¾)
# =========================

def search_youtube_videos_smart(
    keywords, youtube_client, movie_title,
    max_per_keyword, max_total_videos,
    start_date, end_date,
    negative_keywords_list  # æ–°å¢ï¼šè² é¢é—œéµè©åˆ—è¡¨
):
    all_video_ids = set()
    video_meta = {}
    search_cache_name = "yt_search_smart_filtered_cache"
    
    progress_text = "æ­£åœ¨æœå°‹ä¸¦éæ¿¾ç„¡é—œ/æ”¿æ²»å½±ç‰‡..."
    my_bar = st.progress(0, text=progress_text)
    
    # 1. æº–å‚™æ­£å‘é—œéµè© (é›»å½±åæ‹†åˆ†)
    # ä¾‹å¦‚ "ä¹é¾åŸå¯¨ä¹‹åœåŸ" -> ["ä¹é¾åŸå¯¨", "åœåŸ"]
    title_keywords = [k for k in re.split(r'\s+|ï¼š|:|,|ï¼Œ', movie_title) if len(k) > 1]
    
    # 2. æº–å‚™è² é¢é—œéµè© (ç¡¬ç·¨ç¢¼åŸºç¤ + ç”¨æˆ¶è¼¸å…¥)
    # é€™äº›è©å‡ºç¾åœ¨æ¨™é¡Œä¸­é€šå¸¸ä»£è¡¨æ˜¯æ™‚æ”¿æ–°èè€Œéå½±è©•
    base_negative_keywords = [
        "æ–°è", "ç›´æ’­", "æ–½æ”¿å ±å‘Š", "ç¿’è¿‘å¹³", "ä¸­å…±", "å…±ç”¢é»¨", 
        "ç‰¹é¦–", "æå®¶è¶…", "ç«‹æ³•æœƒ", "ç¤ºå¨", "æ”¿æ²»", "æ”¿ç¶“", 
        "å¤§ç´€å…ƒ", "æ–‡æ˜­", "æ±Ÿå³°", "å¤©äº®æ™‚åˆ†", "æ™‚äº‹", "è²¡ç¶“"
    ]
    # åˆä½µç”¨æˆ¶å®šç¾©çš„æ’é™¤è©
    final_negative_keywords = list(set(base_negative_keywords + negative_keywords_list))

    for idx, query in enumerate(keywords):
        if len(all_video_ids) >= max_total_videos: break
            
        cache_key = f"{query}_{start_date}_{end_date}_{max_per_keyword}_filtered"
        cached_records = _get_cached_value(search_cache_name, cache_key, CACHE_TTL_SEARCH)
        
        query_records = []
        
        if cached_records is None:
            try:
                request = youtube_client.search().list(
                    q=query, part="id,snippet", type="video", maxResults=max_per_keyword,
                    publishedAfter=f"{start_date}T00:00:00Z", publishedBefore=f"{end_date}T23:59:59Z",
                    order="relevance", safeSearch="none", relevanceLanguage="zh-Hant"
                )
                response = request.execute()
                for item in response.get("items", []):
                    vid = item["id"]["videoId"]
                    snip = item.get("snippet", {})
                    title = snip.get("title", "")
                    desc = snip.get("description", "")
                    channel_title = snip.get("channelTitle", "")
                    
                    # === æ ¸å¿ƒä¿®æ”¹ï¼šé›™é‡éæ¿¾é‚è¼¯ ===
                    
                    # A. è² é¢éæ¿¾ (Negative Filter) - å„ªå…ˆç´šæœ€é«˜
                    # å¦‚æœæ¨™é¡Œæˆ–é »é“ååŒ…å«æ”¿æ²»æ•æ„Ÿè©ï¼Œç›´æ¥è·³é
                    if any(nk in title for nk in final_negative_keywords) or \
                       any(nk in channel_title for nk in final_negative_keywords):
                        continue 

                    # B. æ­£å‘ç›¸é—œæ€§æª¢æŸ¥ (Positive Relevance)
                    is_relevant = False
                    
                    # B1. æ¨™é¡Œå¿…é ˆåŒ…å«è‡³å°‘ä¸€å€‹é›»å½±æ ¸å¿ƒè©
                    # é€™æ˜¯ç‚ºäº†é˜²æ­¢ YouTube æ¨é€å®Œå…¨ç„¡é—œçš„ "çŒœä½ å–œæ­¡"
                    if any(tk.lower() in title.lower() for tk in title_keywords):
                        is_relevant = True
                    
                    # B2. å¦‚æœæ¨™é¡Œæ²’æœ‰æ ¸å¿ƒè©ï¼Œä½†æè¿°è£¡æœ‰å®Œæ•´é›»å½±åï¼Œä¹Ÿå¯ä»¥æ¥å— (é˜²æ­¢æ¨™é¡Œé»¨)
                    elif movie_title.lower() in desc.lower():
                        is_relevant = True
                        
                    if is_relevant:
                        query_records.append({
                            "id": vid,
                            "title": title,
                            "channelTitle": channel_title,
                            "publishedAt": snip.get("publishedAt", "")
                        })
                
                _set_cached_value(search_cache_name, cache_key, query_records)
            except Exception as e:
                st.warning(f"æœå°‹ '{query}' å¤±æ•—: {e}")
        else:
            query_records = cached_records

        for rec in query_records:
            if rec["id"] not in all_video_ids:
                all_video_ids.add(rec["id"])
                video_meta[rec["id"]] = rec
                if len(all_video_ids) >= max_total_videos: break
        
        my_bar.progress((idx + 1) / len(keywords), text=f"æœå°‹ä¸­... å·²éæ¿¾æ”¿æ²»/ç„¡é—œå…§å®¹ï¼Œä¿ç•™: {len(all_video_ids)} éƒ¨")

    my_bar.empty()
    return list(all_video_ids), video_meta

# ... (get_all_comments_cached å‡½æ•¸ä¿æŒä¸è®Šï¼Œä½¿ç”¨ä¸Šä¸€ç‰ˆçš„å‹•æ…‹èª¿æ•´é‚è¼¯) ...
def get_all_comments_cached(video_ids, youtube_client, max_per_video, max_total_comments, video_meta):
    all_comments = []
    comments_cache_name = "yt_comments_cache_v4" # Update version
    
    progress_bar = st.progress(0, text="æŠ“å–ç•™è¨€ä¸­...")
    
    # å‹•æ…‹èª¿æ•´ï¼šå½±ç‰‡å°‘å‰‡æŠ“æ›´å¤šè©•è«–
    if len(video_ids) > 0 and len(video_ids) < 5:
        adjusted_max_per_video = max_per_video * 4 # æå‡å€ç‡
        st.caption(f"âš ï¸ ç¶“éæ¿¾å¾Œå½±ç‰‡ä¾†æºè¼ƒå°‘ï¼Œè‡ªå‹•å°‡å–®ä¸€å½±ç‰‡ç•™è¨€æŠ“å–ä¸Šé™å¤§å¹…æå‡è‡³ {adjusted_max_per_video} å‰‡")
    else:
        adjusted_max_per_video = max_per_video

    for i, vid in enumerate(video_ids):
        if len(all_comments) >= max_total_comments: break

        cache_key = f"comments_{vid}_{adjusted_max_per_video}"
        cached_comments = _get_cached_value(comments_cache_name, cache_key, CACHE_TTL_COMMENTS)
        
        video_comments = []
        if cached_comments is not None:
            video_comments = cached_comments
        else:
            try:
                request = youtube_client.commentThreads().list(
                    part="snippet", videoId=vid, textFormat="plainText",
                    order="relevance", maxResults=min(100, adjusted_max_per_video)
                )
                response = request.execute()
                for item in response.get("items", []):
                    if len(video_comments) >= adjusted_max_per_video: break
                    comm = item["snippet"]["topLevelComment"]["snippet"]
                    video_comments.append({
                        "comment_text": comm.get("textDisplay", ""),
                        "published_at": comm.get("publishedAt", ""),
                        "like_count": comm.get("likeCount", 0)
                    })
                _set_cached_value(comments_cache_name, cache_key, video_comments)
            except: pass
        
        title = video_meta.get(vid, {}).get("title", "")
        for c in video_comments:
            c_copy = c.copy()
            c_copy.update({"video_id": vid, "video_title": title})
            all_comments.append(c_copy)
            if len(all_comments) >= max_total_comments: break
            
        progress_bar.progress((i + 1) / len(video_ids), text=f"æŠ“å–ç•™è¨€... ({len(all_comments)}/{max_total_comments})")

    progress_bar.empty()
    return pd.DataFrame(all_comments)

# =========================
# 2. DeepSeek åˆ†æ (Prompt å†æ¬¡å¢å¼·ï¼šé˜²æ­¢æ”¿æ²»éš±å–»å¹²æ“¾)
# =========================

async def analyze_comment_deepseek_v2(row, deepseek_client, semaphore):
    text = row["comment_text"]
    video_title = row.get("video_title", "")
    
    # Prompt ç­–ç•¥ï¼š
    # å¢åŠ  Contextï¼šå‘Šè¨´ AI é€™æ¢è©•è«–ä¾†è‡ªå“ªå€‹è¦–é »æ¨™é¡Œï¼Œå¹«åŠ©åˆ¤æ–·
    # å¢åŠ è¦å‰‡ï¼šå¦‚æœè©•è«–æ˜¯åœ¨è¨è«–æ”¿æ²»è€Œéé›»å½±æœ¬èº«ï¼Œä¹Ÿè¦–ç‚º False
    
    system_prompt = (
        "You are a movie analyst focusing on the Hong Kong market. "
        f"The comment is from a video titled: '{video_title}'. "
        "Analyze the comment. "
        "Output JSON with keys: "
        "'sentiment' (Positive/Negative/Neutral), "
        "'keywords' (Extract 1-2 main keywords in Traditional Chinese), "
        "'is_target_audience' (boolean). "
        "\n\n"
        "Rules for 'is_target_audience':\n"
        "1. **TRUE** if it is a relevant movie review/reaction in Cantonese, Traditional Chinese, or mixed English.\n"
        "2. **FALSE** if it is purely about politics (e.g., discussing government policies, CCP, democracy) without relating to the movie plot.\n"
        "3. **FALSE** if it is Simplified Chinese (unless clearly HK slang).\n"
        "4. **FALSE** if it is spam or ads."
    )

    async with semaphore:
        try:
            response = await deepseek_client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": text}
                ],
                response_format={"type": "json_object"},
                temperature=0.1,
            )
            return json.loads(response.choices[0].message.content)
        except:
            return {"sentiment": "Error", "keywords": "", "is_target_audience": False}

async def run_deepseek_analysis(df, deepseek_client):
    semaphore = asyncio.Semaphore(50)
    rows = df.to_dict('records')
    tasks = [analyze_comment_deepseek_v2(row, deepseek_client, semaphore) for row in rows]
    
    progress_bar = st.progress(0, text="AI æ­£åœ¨åˆ†æ (å·²å•Ÿç”¨æ”¿æ²»å…§å®¹éæ¿¾)...")
    
    results = []
    total = len(tasks)
    for i, task in enumerate(asyncio.as_completed(tasks)):
        await task
        progress_bar.progress((i + 1) / total)
        
    results = await asyncio.gather(*tasks)
    progress_bar.empty()
    return pd.DataFrame(results)

# =========================
# 3. ä¸»æµç¨‹
# =========================

def main_process(movie_title, start_date, end_date, yt_api_key, deepseek_api_key, 
                 max_per_keyword, max_total_videos, max_per_video, max_total_comments,
                 negative_keywords):
    
    youtube = build("youtube", "v3", developerKey=yt_api_key)
    deepseek = openai.AsyncOpenAI(api_key=deepseek_api_key, base_url="https://api.deepseek.com/v1")
    
    # 1. æœå°‹ (å‚³å…¥è² é¢é—œéµè©)
    keywords = generate_search_queries(movie_title)
    video_ids, video_meta = search_youtube_videos_smart(
        keywords, youtube, movie_title,
        max_per_keyword, max_total_videos, start_date, end_date,
        negative_keywords
    )
    
    if not video_ids:
        return None, f"æ‰¾ä¸åˆ°ç›¸é—œå½±ç‰‡ã€‚è«‹æª¢æŸ¥é›»å½±åç¨±ï¼Œæˆ–å˜—è©¦æ¸›å°‘è² é¢é—œéµè©ã€‚"
    
    st.info(f"éæ¿¾æ”¿æ²»/ç„¡é—œå…§å®¹å¾Œï¼Œé–å®š {len(video_ids)} éƒ¨å½±ç‰‡ï¼Œé–‹å§‹æŠ“å–ç•™è¨€...")
    
    # 2. æŠ“å–
    df_comments = get_all_comments_cached(video_ids, youtube, max_per_video, max_total_comments, video_meta)
    
    if df_comments.empty:
        return None, "é€™äº›å½±ç‰‡ä¸‹æ²’æœ‰æ‰¾åˆ°ç•™è¨€ã€‚"
    
    # 3. AI åˆ†æ
    analysis_df = asyncio.run(run_deepseek_analysis(df_comments, deepseek))
    final_df = pd.concat([df_comments, analysis_df], axis=1)
    
    # 4. ç¯©é¸
    original_len = len(final_df)
    final_df = final_df[final_df["is_target_audience"] == True].copy()
    filtered_len = len(final_df)
    
    st.success(f"åˆ†æå®Œæˆï¼åŸå§‹æŠ“å– {original_len} å‰‡ï¼ŒAI å‰”é™¤éæ¸¯å¼/æ”¿æ²»é›¢é¡Œå…§å®¹å¾Œï¼Œå‰©é¤˜ {filtered_len} å‰‡æœ‰æ•ˆè©•è«–ã€‚")
    
    final_df["published_at"] = pd.to_datetime(final_df["published_at"])
    return final_df, None

# =========================
# 4. UI
# =========================

st.set_page_config(page_title="YouTube å½±è©•åˆ†æ (Anti-Spam)", layout="wide")
st.title("ğŸ¬ YouTube å½±è©•åˆ†æ (æ™ºèƒ½éæ¿¾ç‰ˆ)")
st.markdown("### ç‰¹é»ï¼šæ™ºèƒ½æœå°‹ | ğŸš« è‡ªå‹•éæ¿¾æ”¿æ²»/æ–°èå½±ç‰‡ | ç¹é«”/ç²µèªè­˜åˆ¥")

with st.sidebar:
    st.header("è¨­å®š")
    yt_api_key = st.text_input("YouTube API Key", type='password')
    deepseek_api_key = st.text_input("DeepSeek API Key", type='password')
    st.divider()
    max_total_videos = st.number_input("æœ€å¤§å½±ç‰‡æœå°‹æ•¸", 10, 100, 50)
    max_total_comments = st.number_input("æœ€å¤§åˆ†æç•™è¨€æ•¸", 50, 2000, 500)
    
    st.divider()
    st.subheader("ğŸš« æ’é™¤é—œéµè© (é˜²æ­¢æ”¿æ²»å¹²æ“¾)")
    default_neg = "æ–°è, ç›´æ’­, ç¿’è¿‘å¹³, ä¸­å…±, æ”¿æ²»"
    user_neg_input = st.text_area("è¼¸å…¥è¦æ’é™¤çš„è© (é€—è™Ÿåˆ†éš”)", value=default_neg, help="è‹¥æ¨™é¡ŒåŒ…å«é€™äº›è©ï¼Œå°‡ç›´æ¥å¿½ç•¥è©²å½±ç‰‡")
    negative_keywords = [x.strip() for x in user_neg_input.split(",") if x.strip()]

col1, col2, col3 = st.columns([2, 1, 1])
with col1:
    movie_title = st.text_input("é›»å½±åç¨±", value="ä¹é¾åŸå¯¨") 
with col2:
    start_date = st.date_input("é–‹å§‹", value=datetime.today() - timedelta(days=90))
with col3:
    end_date = st.date_input("çµæŸ", value=datetime.today())

if st.button("ğŸš€ é–‹å§‹åˆ†æ", type="primary"):
    if not all([movie_title, yt_api_key, deepseek_api_key]):
        st.error("è«‹å¡«å¯«æ‰€æœ‰æ¬„ä½")
    else:
        with st.spinner("æ­£åœ¨æœå°‹ä¸¦åŸ·è¡Œé›™é‡éæ¿¾ (é—œéµè© + AI)..."):
            df_result, err = main_process(
                movie_title, str(start_date), str(end_date),
                yt_api_key, deepseek_api_key,
                20, max_total_videos, 50, max_total_comments,
                negative_keywords
            )
            
        if err:
            st.error(err)
        else:
            st.divider()
            
            # ç°¡å–®å±•ç¤ºçµæœ (ä¿ç•™åŸæœ‰çš„å¯è¦–åŒ–ä»£ç¢¼çµæ§‹)
            st.subheader("ğŸ”¥ ç†±é–€è©•è«–é—œéµè©")
            # ... (æ­¤è™•å¯è¦–åŒ–ä»£ç¢¼èˆ‡ä¸Šä¸€ç‰ˆç›¸åŒï¼Œçœç•¥ä»¥ç¯€çœç¯‡å¹…) ...
            all_keywords = []
            for item in df_result['keywords']:
                if isinstance(item, str):
                    words = [w.strip() for w in re.split(r'[ï¼Œ,ã€\s]+', item) if len(w.strip()) > 1]
                    all_keywords.extend(words)
                elif isinstance(item, list):
                    all_keywords.extend([str(w).strip() for w in item if len(str(w).strip()) > 1])
            
            if all_keywords:
                kw_counts = Counter(all_keywords).most_common(15)
                kw_df = pd.DataFrame(kw_counts, columns=['Keyword', 'Count']).sort_values(by='Count')
                fig_kw = px.bar(kw_df, x='Count', y='Keyword', orientation='h', title='Top Keywords')
                st.plotly_chart(fig_kw, use_container_width=True)

            with st.expander("æŸ¥çœ‹è©³ç´°æ•¸æ“š"):
                st.dataframe(df_result[['sentiment', 'keywords', 'comment_text', 'video_title']])
