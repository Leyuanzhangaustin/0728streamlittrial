# app.py

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
from datetime import datetime, timedelta
import time
import asyncio
import openai
import re
import json
from opencc import OpenCC
from googleapiclient.discovery import build

# =========================
# 0. å¿«å–èˆ‡å·¥å…·å‡½å¼
# =========================

CACHE_TTL_SEARCH = 3600
CACHE_TTL_RELEVANCE = 86400
CACHE_TTL_CHANNEL = 86400
CACHE_TTL_COMMENTS = 900

def _get_cached_value(cache_name: str, key, ttl_seconds: int):
    if cache_name not in st.session_state:
        st.session_state[cache_name] = {}
    entry = st.session_state[cache_name].get(key)
    if entry:
        if (time.time() - entry["ts"]) <= ttl_seconds:
            return entry["value"]
        else:
            del st.session_state[cache_name][key]
    return None

def _set_cached_value(cache_name: str, key, value):
    if cache_name not in st.session_state:
        st.session_state[cache_name] = {}
    st.session_state[cache_name][key] = {
        "value": value,
        "ts": time.time()
    }

# =========================
# 1. èªè¨€èˆ‡é—œéµå­—å·¥å…· (å·²æ›´æ–°å­—å…¸)
# =========================

def generate_search_queries(movie_title: str):
    zh_terms = [
        "å½±è©•", "è©•è«–", "è©•åƒ¹", "é»è©•", "è§£æ", "åˆ†æ", "è§€å¾Œæ„Ÿ",
        "ç„¡é›·", "æœ‰é›·", "è¨è«–", "å¥½å””å¥½ç‡", "é å‘Š", "èŠ±çµ®", "ç‰‡æ®µ", "é¦–æ˜ ", "å¹•å¾Œ",
        "é¦™æ¸¯", "é¦™æ¸¯ä¸Šæ˜ ", "é¦™æ¸¯é¦–æ˜ ", "é¦™æ¸¯åæ‡‰", "æˆ²é™¢ åæ‡‰", "é™¢ç·š", "è¡—è¨ª",
        "ç²µèª", "å»£æ±è©±", "ç²µèªé…éŸ³", "ç²µé…", "æ¸¯ç‰ˆ", "æ¸¯ç”¢"
    ]
    en_terms = [
        "review", "reaction", "ending explained", "analysis", "explained",
        "behind the scenes", "bts", "premiere", "interview", "press conference",
        "hong kong", "hk reaction", "hk audience"
    ]
    loose = [f"{movie_title}"]
    loose += [f"{movie_title} {t}" for t in zh_terms]
    loose += [f"{movie_title} {t}" for t in en_terms]
    tight = [
        f"\"{movie_title}\"",
        f"\"{movie_title}\" å½±è©•",
        f"\"{movie_title}\" è©•è«–",
        f"\"{movie_title}\" è§£æ",
        f"\"{movie_title}\" review",
        f"\"{movie_title}\" reaction",
        f"\"{movie_title}\" é¦™æ¸¯",
        f"\"{movie_title}\" ç²µèª",
        f"\"{movie_title}\" å»£æ±è©±",
    ]
    seen = set()
    queries = []
    for q in loose + tight:
        if q not in seen:
            queries.append(q)
            seen.add(q)
    return queries

# æ›´æ–°ï¼šå¢åŠ æ›´å¤šå£èªè®Šé«”å’Œä¿šèª
CANTONESE_CHAR_TOKENS = {
    "å””": 1.0, "å†‡": 1.6, "å’—": 1.6, "å˜…": 1.6, "å•²": 1.2, "å—°": 1.2, "ä½¢": 1.0,
    "å–º": 1.6, "åšŸ": 1.6, "å’ª": 1.2, "å•±": 1.2, "æ‚": 1.2, "éš": 1.2, "æ›³": 1.2,
    "æ”°": 1.2, "å’": 1.0, "å™‰": 1.0, "å¾—": 0.6, "å–": 0.8, "å†§": 1.0, "æ’š": 1.2,
    "ä»†": 1.2, "å±Œ": 1.2, "å—®": 1.0, "ç•€": 0.8, "æ¸": 1.0, "è…": 0.0,
    # æ–°å¢/èª¿æ•´
    "ç³»": 0.5,  # å¾ˆå¤šäººæ‰“éŒ¯å­— "ç³»" ä»£æ›¿ "ä¿‚"ï¼Œé›–ç„¶ç°¡é«”ä¹Ÿæœ‰ï¼Œä½†åœ¨ç¹é«”ç’°å¢ƒä¸‹å‡ºç¾é€šå¸¸æ˜¯ç²µèª
    "ä¿‚": 1.5,  # æ ¸å¿ƒè©
    "9": 0.5,   # æ•¸å­—ä¿šèª (é³©/ç‹—)
    "7": 0.5,   # æ•¸å­—ä¿šèª (æŸ’)
    "6": 0.3,   # æ•¸å­—ä¿šèª (é™¸/ç¢Œ)
    "äº": 0.5,  # äºåª½, äºå“¥ (é˜¿çš„ç•°é«”)
    "é‡": 0.5,  # å˜¢çš„ç•°é«”
    "æ—¢": 0.5,  # å˜…çš„ç•°é«”
    "å·¦": 0.5,  # å’—çš„ç•°é«”
    "d": 0.8, "D": 0.8, # å•²çš„ä»£è™Ÿ
}

CANTONESE_PARTICLES = ["å•¦", "å›‰", "å–", "å’©", "å‘¢", "å‘€", "å˜›", "å–‡", "æ°", "å§", "å™ƒ"]
CANTONESE_PHRASES = {
    "å¥½å””å¥½ç‡": 2.0, "åšå’©": 1.6, "é»è§£": 1.6, "å’©æ–™": 1.6, "ç®—å•¦": 1.2,
    "å¾—å•¦": 1.2, "æ­£å–": 1.2, "å¹¾å¥½ç‡": 1.6, "å¹¾æ­£": 1.2, "å¥½æ­£": 1.0,
    "æœ‰å•²": 0.8, "å—°å•²": 1.2, "å‘¢å•²": 1.2, "è¬›çœŸ": 0.8, "å¥½ä¼¼": 0.5,
    "å¤š9ä½™": 2.0, "å¤šé¤˜": 0.5, "çœŸç³»": 1.0, "çœŸä¿‚": 1.5, "æ‰“é¢¨": 1.0
}
ROMANIZATION_RE = re.compile(r"(?i)(?<![A-Za-z])(la|lor|wor|leh|meh|mah|ga|wo|ar)(?=[\s\W]|$)")

def count_chars(text: str):
    counts = {
        "cjk": 0, "hiragana": 0, "katakana": 0, "half_katakana": 0,
        "hangul": 0, "latin": 0, "digits": 0, "other": 0
    }
    for ch in text:
        code = ord(ch)
        if 0x4E00 <= code <= 0x9FFF or 0x3400 <= code <= 0x4DBF or 0xF900 <= code <= 0xFAFF:
            counts["cjk"] += 1
        elif 0x3040 <= code <= 0x309F:
            counts["hiragana"] += 1
        elif 0x30A0 <= code <= 0x30FF or 0x31F0 <= code <= 0x31FF:
            counts["katakana"] += 1
        elif 0xFF65 <= code <= 0xFF9F:
            counts["half_katakana"] += 1
        elif 0xAC00 <= code <= 0xD7AF:
            counts["hangul"] += 1
        elif (0x0041 <= code <= 0x005A) or (0x0061 <= code <= 0x007A):
            counts["latin"] += 1
        elif 0x0030 <= code <= 0x0039:
            counts["digits"] += 1
        else:
            counts["other"] += 1
    return counts

def diff_chars(a: str, b: str) -> int:
    m = min(len(a), len(b))
    return sum(1 for i in range(m) if a[i] != b[i]) + abs(len(a) - len(b))

def classify_zh_trad_simp(text: str, cc_t2s: OpenCC, cc_s2t: OpenCC):
    if not isinstance(text, str) or len(text.strip()) < 1:
        return "other"
    counts = count_chars(text)
    
    # åˆ¤æ–·æ˜¯å¦ä¸»è¦ç‚ºè‹±æ–‡
    total_chars = len(text.strip())
    if counts["latin"] / max(1, total_chars) > 0.7:
        return "en"

    kana = counts["hiragana"] + counts["katakana"] + counts["half_katakana"]
    cjk = counts["cjk"]
    
    if kana >= 2 and kana / max(1, (cjk + kana)) >= 0.10:
        return "ja"
    if cjk < 1:
        # å¦‚æœæ²’æœ‰ä¸­æ–‡å­—ï¼Œä½†ä¹Ÿä¸æ˜¯è‹±æ–‡ï¼Œæ­¸é¡ç‚ºå…¶ä»–
        return "other" if counts["latin"] == 0 else "en"

    t2s = cc_t2s.convert(text)
    s2t = cc_s2t.convert(text)
    ct2s = diff_chars(text, t2s)
    cs2t = diff_chars(text, s2t)
    threshold = max(1, int(0.05 * cjk))
    if ct2s > cs2t + threshold:
        return "zh-Hant"
    elif cs2t > ct2s + threshold:
        return "zh-Hans"
    else:
        return "zh-unkn"

def score_cantonese(text: str) -> float:
    if not isinstance(text, str) or not text:
        return 0.0
    score = 0.0
    text_lower = text.lower() # è™•ç† d/D
    
    for phrase, w in CANTONESE_PHRASES.items():
        if phrase in text: # å€åˆ†å¤§å°å¯«çš„åŒ¹é… (ä¸­æ–‡)
            score += text.count(phrase) * w
            
    for ch, w in CANTONESE_CHAR_TOKENS.items():
        # å°æ–¼è‹±æ–‡ä»£è™Ÿ d/Dï¼Œæˆ‘å€‘ç”¨ lower æª¢æŸ¥
        if ch in ['d', 'D']:
            cnt = text_lower.count('d')
            # ç°¡å–®é˜²æ­¢å–®è©ä¸­çš„ d (å¦‚ and, good) è¢«èª¤åˆ¤ï¼Œé€™è£¡åªæ˜¯ä¸€å€‹ç²—ç•¥éæ¿¾
            # æ›´å¥½çš„æ–¹æ³•æ˜¯ç”¨ regexï¼Œä½†é€™è£¡å¾ç°¡ï¼Œå‡è¨­ d æ—é‚Šæœ‰ä¸­æ–‡æˆ–ç©ºæ ¼
            score += cnt * w * 0.5 
        else:
            cnt = text.count(ch)
            if cnt:
                score += cnt * w

    end_slice = text[-8:] if len(text) > 8 else text
    for p in CANTONESE_PARTICLES:
        if p in end_slice:
            score += 0.4
        elif p in text:
            score += 0.2 # éçµå°¾èªæ°£è©æ¬Šé‡è¼ƒä½

    roman_hits = ROMANIZATION_RE.findall(text)
    if roman_hits:
        score += len(roman_hits) * 0.8
    return score

# =========================
# 2. YouTube æœå°‹
# =========================

def search_youtube_videos(keywords, youtube_client, max_per_keyword, start_date, end_date, add_language_bias=True, region_bias=True, max_total_videos=150):
    all_video_ids = set()
    video_meta = {}
    status_text = st.empty()

    for idx, query in enumerate(keywords):
        if len(all_video_ids) >= max_total_videos:
            status_text.info(f"å·²é”åˆ°æœå°‹ä¸Šé™ ({max_total_videos} éƒ¨)ï¼Œåœæ­¢å¾ŒçºŒæœå°‹ã€‚")
            break

        cache_key = f"{query}_{start_date}_{end_date}_{max_per_keyword}_{add_language_bias}_{region_bias}"
        cached_data = _get_cached_value("search_cache", cache_key, CACHE_TTL_SEARCH)
        query_records = []

        if cached_data is not None:
            query_records = cached_data
        else:
            collected_records = []
            collected_ids_for_query = set()
            for order in ["relevance", "viewCount"]:
                if len(collected_records) >= max_per_keyword: break
                try:
                    request = youtube_client.search().list(
                        q=query, part="id,snippet", type="video", maxResults=50,
                        publishedAfter=f"{start_date}T00:00:00Z", publishedBefore=f"{end_date}T23:59:59Z",
                        order=order, safeSearch="none",
                        **({"relevanceLanguage": "zh-Hant"} if add_language_bias else {}),
                        **({"regionCode": "HK"} if region_bias else {})
                    )
                    while request and len(collected_records) < max_per_keyword:
                        response = request.execute()
                        for item in response.get("items", []):
                            vid = item["id"]["videoId"]
                            if vid in collected_ids_for_query: continue
                            collected_ids_for_query.add(vid)
                            snip = item.get("snippet", {})
                            collected_records.append({
                                "video_id": vid, "title": snip.get("title", ""),
                                "channelTitle": snip.get("channelTitle", ""),
                                "publishedAt": snip.get("publishedAt", "")
                            })
                        if len(collected_records) >= max_per_keyword: break
                        request = youtube_client.search().list_next(request, response)
                        time.sleep(0.1)
                except Exception as e:
                    st.warning(f"æœå°‹ '{query}' éŒ¯èª¤: {e}")
                    continue
            _set_cached_value("search_cache", cache_key, collected_records)
            query_records = collected_records

        for record in query_records:
            vid = record["video_id"]
            if vid not in all_video_ids:
                all_video_ids.add(vid)
                if vid not in video_meta:
                    video_meta[vid] = {
                        "title": record["title"],
                        "channelTitle": record["channelTitle"],
                        "publishedAt": record["publishedAt"]
                    }
            if len(all_video_ids) >= max_total_videos: break
    status_text.empty()
    return list(all_video_ids), video_meta

# =========================
# 3. AI ç›¸é—œæ€§éæ¿¾
# =========================

async def check_relevance_batch_async(movie_title, batch_videos, deepseek_client):
    if not batch_videos: return []
    prompt_items = [f"ID: {v['id']}\nTitle: {v['title']}\nChannel: {v['channel']}" for v in batch_videos]
    prompt_text = "\n---\n".join(prompt_items)
    system_prompt = (
        f"You are a data cleaner. The user is analyzing the movie '{movie_title}'. "
        "Identify which videos are actually discussing this specific movie. "
        "Exclude unrelated videos. Return JSON: {\"vid123\": true, \"vid456\": false}"
    )
    try:
        response = await deepseek_client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": prompt_text}],
            response_format={"type": "json_object"}, temperature=0.1,
        )
        data = json.loads(response.choices[0].message.content)
        return [vid for vid, is_rel in data.items() if is_rel is True]
    except Exception:
        return [v['id'] for v in batch_videos]

async def filter_videos_by_relevance(movie_title, video_ids, video_meta, deepseek_client):
    to_check = []
    valid_ids = set()
    for vid in video_ids:
        cached = _get_cached_value("relevance_cache", f"{movie_title}_{vid}", CACHE_TTL_RELEVANCE)
        if cached is not None:
            if cached: valid_ids.add(vid)
        else:
            meta = video_meta.get(vid, {})
            to_check.append({"id": vid, "title": meta.get("title", ""), "channel": meta.get("channelTitle", "")})
    
    if to_check:
        batch_size = 20
        tasks = [check_relevance_batch_async(movie_title, to_check[i:i+batch_size], deepseek_client) for i in range(0, len(to_check), batch_size)]
        progress = st.empty()
        progress.info(f"AI éæ¿¾ {len(to_check)} éƒ¨å½±ç‰‡ç›¸é—œæ€§...")
        results = await asyncio.gather(*tasks)
        for batch_idx, res_list in enumerate(results):
            batch_input = to_check[batch_idx*batch_size : (batch_idx+1)*batch_size]
            rel_set = set(res_list)
            for item in batch_input:
                is_rel = item["id"] in rel_set
                if is_rel: valid_ids.add(item["id"])
                _set_cached_value("relevance_cache", f"{movie_title}_{item['id']}", is_rel)
        progress.empty()
    return list(valid_ids)

# =========================
# 4. è©³æƒ…èˆ‡ç•™è¨€
# =========================

def fetch_video_and_channel_details(video_ids, youtube_client):
    video_extra = {}
    channel_ids = set()
    for i in range(0, len(video_ids), 50):
        try:
            resp = youtube_client.videos().list(part="snippet,contentDetails", id=",".join(video_ids[i:i+50])).execute()
            for item in resp.get("items", []):
                vid = item.get("id")
                snip = item.get("snippet", {})
                ch = snip.get("channelId")
                video_extra[vid] = {
                    "channelId": ch,
                    "defaultLanguage": snip.get("defaultLanguage", ""),
                    "defaultAudioLanguage": snip.get("defaultAudioLanguage", ""),
                    "tags": snip.get("tags", [])
                }
                if ch: channel_ids.add(ch)
        except Exception: pass

    channel_country = {}
    to_fetch = []
    for cid in channel_ids:
        cached = _get_cached_value("channel_cache", cid, CACHE_TTL_CHANNEL)
        if cached: channel_country[cid] = cached
        else: to_fetch.append(cid)
    
    if to_fetch:
        for i in range(0, len(to_fetch), 50):
            try:
                resp = youtube_client.channels().list(part="brandingSettings", id=",".join(to_fetch[i:i+50])).execute()
                for item in resp.get("items", []):
                    cid = item.get("id")
                    country = item.get("brandingSettings", {}).get("channel", {}).get("country")
                    channel_country[cid] = country
                    _set_cached_value("channel_cache", cid, country)
            except Exception: pass
    return video_extra, channel_country

def compute_hk_video_score(video_id, video_meta, video_extra, channel_country_map):
    meta = video_meta.get(video_id, {})
    ext = video_extra.get(video_id, {})
    title = meta.get("title", "")
    tags = " ".join(ext.get("tags", []) or [])
    ch = ext.get("channelId")
    audio = (ext.get("defaultAudioLanguage") or "").lower()
    country = channel_country_map.get(ch)

    score = 0
    if country == "HK": score += 3
    if audio in ("yue", "zh-hk", "zh-yue", "zh-hant-hk"): score += 3
    elif audio.startswith("zh"): score += 1
    
    if any(t in title for t in ["ç²µèª", "å»£æ±è©±", "ç²µé…", "ç²µèªé…éŸ³"]): score += 3
    if any(t in title for t in ["é¦™æ¸¯", "æ¸¯ç‰ˆ", "é¦™æ¸¯è§€çœ¾", "é¦™æ¸¯åæ‡‰", "é¦™æ¸¯é¦–æ˜ ", "é¦™æ¸¯ä¸Šæ˜ "]): score += 2
    if ("HK" in title) or ("Hong Kong" in title): score += 1
    if any(t in tags for t in ["ç²µèª", "å»£æ±è©±", "é¦™æ¸¯", "HK"]): score += 2
    return score

def get_all_comments(video_ids, youtube_client, max_per_video, video_meta, hk_score_map, video_extra, channel_country_map, max_total_comments):
    all_comments = []
    total_fetched = 0
    progress = st.progress(0, text="æŠ“å–ç•™è¨€...")
    
    for i, vid in enumerate(video_ids):
        if total_fetched >= max_total_comments: break
        
        cache_key = f"{vid}_{max_per_video}"
        cached = _get_cached_value("comments_cache", cache_key, CACHE_TTL_COMMENTS)
        raw_recs = []

        if cached is not None:
            raw_recs = cached
        else:
            try:
                req = youtube_client.commentThreads().list(part="snippet", videoId=vid, textFormat="plainText", order="time", maxResults=100)
                fetched_vid = 0
                while req and fetched_vid < max_per_video:
                    if total_fetched + fetched_vid >= max_total_comments: break
                    resp = req.execute()
                    for item in resp.get("items", []):
                        if fetched_vid >= max_per_video: break
                        cmt = item["snippet"]["topLevelComment"]["snippet"]
                        raw_recs.append({
                            "textDisplay": cmt.get("textDisplay", ""),
                            "publishedAt": cmt.get("publishedAt", ""),
                            "likeCount": cmt.get("likeCount", 0)
                        })
                        fetched_vid += 1
                    req = youtube_client.commentThreads().list_next(req, resp)
                    if req and fetched_vid < max_per_video: time.sleep(0.1)
                if raw_recs: _set_cached_value("comments_cache", cache_key, raw_recs)
            except Exception: pass
        
        ch_id = video_extra.get(vid, {}).get("channelId")
        for r in raw_recs:
            all_comments.append({
                "video_id": vid,
                "video_title": video_meta.get(vid, {}).get("title", ""),
                "video_hk_score": hk_score_map.get(vid, 0),
                "video_channel_country": channel_country_map.get(ch_id),
                "comment_text": r["textDisplay"],
                "published_at": r["publishedAt"],
                "like_count": r["likeCount"]
            })
        total_fetched += len(raw_recs)
        progress.progress((i+1)/len(video_ids), text=f"æŠ“å–ç•™è¨€... ({min(i+1, len(video_ids))}/{len(video_ids)})")
    progress.empty()
    return pd.DataFrame(all_comments)

# =========================
# 5. DeepSeek åˆ†æ
# =========================

async def analyze_comment_deepseek_async(comment_text, deepseek_client, semaphore, max_retries=3):
    if not isinstance(comment_text, str) or len(comment_text.strip()) < 2: # æ”¾å¯¬é•·åº¦é™åˆ¶
        return {"sentiment": "Invalid", "topic": "N/A", "summary": "Too short."}
    
    # é‡å°è‹±æ–‡æˆ–çŸ­å¥çš„ Prompt å„ªåŒ–
    system_prompt = (
        "You are a professional Hong Kong market sentiment analyst. "
        "Analyze the movie comment. Return JSON with keys: "
        "'sentiment' (Positive/Negative/Neutral), 'topic' (Plot/Acting/Action Design/Visuals/Pace/Overall/N/A), 'summary'. "
        "Treat 'Thank you' or 'Good' as Positive/Overall."
    )
    async with semaphore:
        for attempt in range(max_retries):
            try:
                response = await deepseek_client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": comment_text}],
                    response_format={"type": "json_object"}, temperature=0.1,
                )
                return json.loads(response.choices[0].message.content)
            except Exception:
                if attempt < max_retries - 1: await asyncio.sleep(2**attempt)
                else: return {"sentiment": "Error", "topic": "Error", "summary": "API Error"}

async def run_all_analyses(df, deepseek_client):
    semaphore = asyncio.Semaphore(50)
    tasks = []
    async def with_index(idx, text):
        res = await analyze_comment_deepseek_async(text, deepseek_client, semaphore)
        return idx, res
    for i, text in enumerate(df["comment_text"]):
        tasks.append(asyncio.create_task(with_index(i, text)))
    
    progress = st.progress(0, text="AI åˆ†æä¸­...")
    results = [None]*len(tasks)
    for done, coro in enumerate(asyncio.as_completed(tasks), 1):
        idx, res = await coro
        results[idx] = res
        progress.progress(done/len(tasks))
    progress.empty()
    return results

# =========================
# 6. ä¸»æµç¨‹ (æ ¸å¿ƒé‚è¼¯ä¿®æ”¹)
# =========================

def movie_comment_analysis(
    movie_title, start_date, end_date, yt_api_key, deepseek_api_key,
    max_videos_per_keyword=30, max_comments_per_video=50, sample_size=None,
    relax_trad_filter=True, cantonese_threshold=2.0, auto_relax_threshold=True,
    target_min_cantonese=300, prefer_hk_videos=True
):
    target_sample = sample_size if sample_size and sample_size > 0 else 1000
    GLOBAL_MAX_COMMENTS = max(2000, target_sample * 4)
    GLOBAL_MAX_VIDEOS = 150
    SEARCH_KEYWORDS = generate_search_queries(movie_title)

    youtube_client = build("youtube", "v3", developerKey=yt_api_key)
    deepseek_client = openai.AsyncOpenAI(api_key=deepseek_api_key, base_url="https://api.deepseek.com/v1")

    # 1. æœå°‹
    video_ids, video_meta = search_youtube_videos(
        SEARCH_KEYWORDS, youtube_client, max_videos_per_keyword, start_date, end_date,
        add_language_bias=True, region_bias=True, max_total_videos=GLOBAL_MAX_VIDEOS
    )
    if not video_ids: return None, "æ‰¾ä¸åˆ°ç›¸é—œå½±ç‰‡ã€‚"
    
    # 2. ç›¸é—œæ€§éæ¿¾
    relevant_video_ids = asyncio.run(filter_videos_by_relevance(movie_title, video_ids, video_meta, deepseek_client))
    if not relevant_video_ids: return None, "AI éæ¿¾å¾Œç„¡ç›¸é—œå½±ç‰‡ã€‚"
    
    # 3. è©³æƒ…èˆ‡åˆ†æ•¸
    video_extra, channel_country_map = fetch_video_and_channel_details(relevant_video_ids, youtube_client)
    hk_score_map = {vid: compute_hk_video_score(vid, video_meta, video_extra, channel_country_map) for vid in relevant_video_ids}
    
    # æ’åº
    sorted_ids = sorted(relevant_video_ids, key=lambda v: hk_score_map.get(v, 0), reverse=True) if prefer_hk_videos else relevant_video_ids

    # 4. æŠ“å–ç•™è¨€
    df_comments = get_all_comments(
        sorted_ids, youtube_client, max_comments_per_video,
        video_meta, hk_score_map, video_extra, channel_country_map, GLOBAL_MAX_COMMENTS
    )
    if df_comments.empty: return None, "æ‰¾ä¸åˆ°ä»»ä½•ç•™è¨€ã€‚"

    st.info(f"å·²æŠ“å– {len(df_comments)} å‰‡åŸå§‹ç•™è¨€ï¼Œé–‹å§‹é€²è¡Œã€Œæƒ…å¢ƒå¼ã€ç¯©é¸...")

    # 5. èªè¨€èˆ‡æƒ…å¢ƒç¯©é¸ (æ ¸å¿ƒä¿®æ”¹)
    cc_t2s = OpenCC("t2s")
    cc_s2t = OpenCC("s2t")
    
    # è¨ˆç®—ç‰¹å¾µ
    df_comments["lang_pred"] = df_comments["comment_text"].apply(lambda x: classify_zh_trad_simp(x, cc_t2s, cc_s2t))
    df_comments["cantonese_score"] = df_comments["comment_text"].apply(score_cantonese)
    
    # å®šç¾©ç¯©é¸é‚è¼¯
    def is_target_audience(row):
        text_score = row["cantonese_score"]
        vid_score = row["video_hk_score"]
        lang = row["lang_pred"]
        
        # æ¢ä»¶ A: æ–‡æœ¬æœ¬èº«å°±æ˜¯å¼·ç²µèª (ç„¡è«–å½±ç‰‡ä¾†æº)
        if text_score >= cantonese_threshold:
            return True
            
        # æ¢ä»¶ B: å½±ç‰‡æ˜¯å¼·é¦™æ¸¯èƒŒæ™¯ (åˆ†æ•¸ >= 3)ï¼Œä¸”ç•™è¨€æ˜¯ç¹é«”ä¸­æ–‡ã€è‹±æ–‡æˆ–æœªçŸ¥ä¸­æ–‡
        # é€™èƒ½æ•‘å› "Thanks for sharing" æˆ– "è¬è¬åˆ†äº«"
        if vid_score >= 3 and lang in ["zh-Hant", "zh-unkn", "en"]:
            return True
            
        # æ¢ä»¶ C: å½±ç‰‡æ˜¯ä¸­ç­‰é¦™æ¸¯èƒŒæ™¯ (åˆ†æ•¸ >= 1)ï¼Œä¸”ç•™è¨€æ˜¯ç¹é«”ä¸­æ–‡ (ç¨å¾®åš´æ ¼ä¸€é»ï¼Œä¸æ”¶è‹±æ–‡)
        if vid_score >= 1 and lang in ["zh-Hant", "zh-unkn"]:
            # å¦‚æœæ–‡æœ¬åˆ†æ•¸ç¨å¾®æœ‰ä¸€é» (ä¾‹å¦‚æœ‰ "ç³»" æˆ– "d")ï¼Œä¹Ÿæ”¾è¡Œ
            if text_score >= 0.5:
                return True
                
        return False

    # åˆæ­¥ç¯©é¸
    df_comments["is_target"] = df_comments.apply(is_target_audience, axis=1)
    
    # æ’é™¤ç°¡é«”ä¸­æ–‡ (é™¤éå®ƒæœ‰å¾ˆé«˜çš„ç²µèªåˆ†æ•¸ï¼Œä¾‹å¦‚å»£æ±äººæ‰“ç°¡é«”ç²µèªï¼Œä½†é€™è£¡æˆ‘å€‘å‡è¨­ç°¡é«”=éç›®æ¨™ä»¥ä¿æŒç´”æ·¨)
    # å¦‚æœæƒ³ä¿ç•™å»£æ±çœç²µèªï¼Œå¯ç§»é™¤é€™è¡Œ
    df_comments = df_comments[df_comments["lang_pred"] != "zh-Hans"].reset_index(drop=True)
    
    df_filtered = df_comments[df_comments["is_target"]].reset_index(drop=True)

    # è‡ªå‹•æ”¾å¯¬é‚è¼¯ (ç¾åœ¨ä¸»è¦èª¿æ•´çš„æ˜¯ text_score çš„æ¬Šé‡ï¼Œæˆ–è€…å¦‚æœæ¨£æœ¬å¤ªå°‘ï¼Œæˆ‘å€‘å¯ä»¥é™ä½ vid_score çš„é–€æª»)
    # é€™è£¡ç°¡åŒ–ç‚ºï¼šå¦‚æœæ¨£æœ¬ä¸å¤ ï¼Œæˆ‘å€‘é™ä½å° text_score çš„ä¾è³´ï¼Œæ›´å¤šä¾è³´ video_score
    if auto_relax_threshold and len(df_filtered) < target_min_cantonese:
        st.info(f"æ¨£æœ¬ä¸è¶³ ({len(df_filtered)})ï¼Œå˜—è©¦æ”¾å¯¬æ¢ä»¶...")
        # æ”¾å¯¬ç­–ç•¥ï¼šåªè¦å½±ç‰‡æœ‰ä¸€é»é¦™æ¸¯ç‰¹å¾µ (score >= 1) ä¸”æ˜¯ç¹é«”/è‹±æ–‡éƒ½æ”¶
        mask_relaxed = (df_comments["video_hk_score"] >= 1) & (df_comments["lang_pred"].isin(["zh-Hant", "zh-unkn", "en"]))
        df_filtered = df_comments[mask_relaxed].reset_index(drop=True)
        st.info(f"æ”¾å¯¬å¾Œæ¨£æœ¬æ•¸ï¼š{len(df_filtered)}")

    if df_filtered.empty: return None, "ç¯©é¸å¾Œç„¡ç¬¦åˆæ¢ä»¶çš„ç•™è¨€ã€‚"

    # 6. æ—¥æœŸèˆ‡å–æ¨£
    df_filtered["published_at"] = pd.to_datetime(df_filtered["published_at"], utc=True, errors="coerce")
    df_filtered["published_at_hk"] = df_filtered["published_at"].dt.tz_convert("Asia/Hong_Kong")
    start_dt = pd.to_datetime(start_date).tz_localize("Asia/Hong_Kong")
    end_dt = pd.to_datetime(end_date).tz_localize("Asia/Hong_Kong") + timedelta(days=1)
    df_filtered = df_filtered[(df_filtered["published_at_hk"] >= start_dt) & (df_filtered["published_at_hk"] < end_dt)].reset_index(drop=True)
    
    if df_filtered.empty: return None, "æ—¥æœŸç¯„åœå…§ç„¡ç•™è¨€ã€‚"
    
    df_analyze = df_filtered.sample(n=sample_size, random_state=42) if sample_size and 0 < sample_size < len(df_filtered) else df_filtered
    
    # 7. åˆ†æ
    analysis_results = asyncio.run(run_all_analyses(df_analyze, deepseek_client))
    final_df = pd.concat([df_analyze.reset_index(drop=True), pd.DataFrame(analysis_results)], axis=1)
    final_df["published_at"] = pd.to_datetime(final_df["published_at"])
    
    return final_df, None

# =========================
# 7. UI
# =========================

st.set_page_config(page_title="YouTube é›»å½±è©•è«– AI åˆ†æï¼ˆé¦™æ¸¯ç²µèªå„ªå…ˆï¼‰", layout="wide")
st.title("ğŸ¬ YouTube é›»å½±è©•è«– AI æƒ…æ„Ÿåˆ†æï¼ˆé¦™æ¸¯ç²µèªå„ªå…ˆï¼‰")

with st.expander("ä½¿ç”¨èªªæ˜"):
    st.markdown("""
    **æ›´æ–°èªªæ˜ï¼š**
    *   å·²å„ªåŒ–ç¯©é¸é‚è¼¯ï¼šç¾åœ¨æœƒä¿ç•™ **é¦™æ¸¯å½±ç‰‡** åº•ä¸‹çš„ **æ¨™æº–ç¹é«”ä¸­æ–‡** å’Œ **è‹±æ–‡** ç•™è¨€ï¼ˆä¾‹å¦‚ "Thanks for sharing" æˆ– "è¬è¬åˆ†äº«"ï¼‰ã€‚
    *   å·²å¢å¼·ç²µèªè­˜åˆ¥ï¼šæ”¯æ´ "ç³»"ã€"9"ã€"d" ç­‰å¸¸è¦‹ç¶²çµ¡ç”¨èªã€‚
    """)

movie_title = st.text_input("é›»å½±åç¨±", value="ä¹é¾åŸå¯¨ä¹‹åœåŸ")
col1, col2 = st.columns(2)
with col1: start_date = st.date_input("é–‹å§‹æ—¥æœŸ", value=datetime.today() - timedelta(days=30))
with col2: end_date = st.date_input("çµæŸæ—¥æœŸ", value=datetime.today())
yt_api_key = st.text_input("YouTube API Key", type='password')
deepseek_api_key = st.text_input("DeepSeek API Key", type='password')

st.subheader("é€²éšè¨­å®š")
max_videos = st.slider("æ¯å€‹é—œéµå­—æœå°‹æ•¸", 5, 80, 30)
max_comments = st.slider("æ¯éƒ¨å½±ç‰‡ç•™è¨€æ•¸", 10, 200, 80)
sample_size = st.number_input("åˆ†æä¸Šé™", 0, 5000, 500)
cantonese_threshold = st.slider("ç²µèªç‰¹å¾µåˆ†æ•¸é–€æª» (é‡å°éé¦™æ¸¯é »é“)", 0.5, 6.0, 2.0)

if st.button("ğŸš€ é–‹å§‹åˆ†æ"):
    if not all([movie_title, yt_api_key, deepseek_api_key]):
        st.warning("è«‹å¡«å¯«æ‰€æœ‰æ¬„ä½ã€‚")
    else:
        with st.spinner("AI åˆ†æä¸­..."):
            df_result, err = movie_comment_analysis(
                movie_title, str(start_date), str(end_date), yt_api_key, deepseek_api_key,
                max_videos, max_comments, sample_size, cantonese_threshold=cantonese_threshold
            )
        if err: st.error(err)
        else:
            st.success("å®Œæˆï¼")
            st.dataframe(df_result.head(20), use_container_width=True)
            
            # ç°¡å–®åœ–è¡¨å±•ç¤º
            c1, c2 = st.columns(2)
            with c1:
                st.subheader("æƒ…æ„Ÿåˆ†ä½ˆ")
                vc = df_result['sentiment'].value_counts()
                st.plotly_chart(px.pie(values=vc.values, names=vc.index, color=vc.index, 
                                     color_discrete_map={'Positive':'#5cb85c','Negative':'#d9534f','Neutral':'#f0ad4e'}), use_container_width=True)
            with c2:
                st.subheader("ä¸»é¡Œåˆ†ä½ˆ")
                df_topic = df_result[df_result['topic'] != 'N/A']
                if not df_topic.empty:
                    st.plotly_chart(px.bar(df_topic['topic'].value_counts(), orientation='h'), use_container_width=True)

            st.download_button("ğŸ“¥ ä¸‹è¼‰ CSV", df_result.to_csv(index=False, encoding='utf-8-sig'), f"{movie_title}_analysis.csv", "text/csv")
