# app.py

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
from datetime import datetime, timedelta
import time
import asyncio
import openai
import re
import json
from opencc import OpenCC
from googleapiclient.discovery import build

# =========================
# 0. å¿«å–èˆ‡å·¥å…·å‡½å¼
# =========================

# å¿«å–éæœŸæ™‚é–“è¨­å®š
CACHE_TTL_SEARCH = 3600          # 1 å°æ™‚ï¼šæœå°‹çµæœ
CACHE_TTL_RELEVANCE = 86400      # 24 å°æ™‚ï¼šAI ç›¸é—œæ€§åˆ¤æ–·çµæœï¼ˆå½±ç‰‡æ¨™é¡Œä¸è®Šï¼Œåˆ¤æ–·çµæœå°±ä¸è®Šï¼‰
CACHE_TTL_CHANNEL = 86400        # 24 å°æ™‚ï¼šé »é“åœ‹å®¶è³‡è¨Š
CACHE_TTL_COMMENTS = 900         # 15 åˆ†é˜ï¼šç•™è¨€æ¸…å–®

def _get_cached_value(cache_name: str, key, ttl_seconds: int):
    """å¾ st.session_state ç²å–å¿«å–ï¼Œè‹¥éæœŸå‰‡è¿”å› None"""
    if cache_name not in st.session_state:
        st.session_state[cache_name] = {}
    
    entry = st.session_state[cache_name].get(key)
    if entry:
        if (time.time() - entry["ts"]) <= ttl_seconds:
            return entry["value"]
        else:
            del st.session_state[cache_name][key]
    return None

def _set_cached_value(cache_name: str, key, value):
    """å¯«å…¥å¿«å–åˆ° st.session_state"""
    if cache_name not in st.session_state:
        st.session_state[cache_name] = {}
    st.session_state[cache_name][key] = {
        "value": value,
        "ts": time.time()
    }

# =========================
# 1. èªè¨€èˆ‡é—œéµå­—å·¥å…·
# =========================

def generate_search_queries(movie_title: str):
    zh_terms = [
        "å½±è©•", "è©•è«–", "è©•åƒ¹", "é»è©•", "è§£æ", "åˆ†æ", "è§€å¾Œæ„Ÿ",
        "ç„¡é›·", "æœ‰é›·", "è¨è«–", "å¥½å””å¥½ç‡", "é å‘Š", "èŠ±çµ®", "ç‰‡æ®µ", "é¦–æ˜ ", "å¹•å¾Œ",
        "é¦™æ¸¯", "é¦™æ¸¯ä¸Šæ˜ ", "é¦™æ¸¯é¦–æ˜ ", "é¦™æ¸¯åæ‡‰", "æˆ²é™¢ åæ‡‰", "é™¢ç·š", "è¡—è¨ª",
        "ç²µèª", "å»£æ±è©±", "ç²µèªé…éŸ³", "ç²µé…", "æ¸¯ç‰ˆ", "æ¸¯ç”¢"
    ]
    en_terms = [
        "review", "reaction", "ending explained", "analysis", "explained",
        "behind the scenes", "bts", "premiere", "interview", "press conference",
        "hong kong", "hk reaction", "hk audience"
    ]

    loose = [f"{movie_title}"]
    loose += [f"{movie_title} {t}" for t in zh_terms]
    loose += [f"{movie_title} {t}" for t in en_terms]

    tight = [
        f"\"{movie_title}\"",
        f"\"{movie_title}\" å½±è©•",
        f"\"{movie_title}\" è©•è«–",
        f"\"{movie_title}\" è§£æ",
        f"\"{movie_title}\" review",
        f"\"{movie_title}\" reaction",
        f"\"{movie_title}\" é¦™æ¸¯",
        f"\"{movie_title}\" ç²µèª",
        f"\"{movie_title}\" å»£æ±è©±",
    ]

    seen = set()
    queries = []
    for q in loose + tight:
        if q not in seen:
            queries.append(q)
            seen.add(q)
    return queries

CANTONESE_CHAR_TOKENS = {
    "å””": 1.0, "å†‡": 1.6, "å’—": 1.6, "å˜…": 1.6, "å•²": 1.2, "å—°": 1.2, "ä½¢": 1.0,
    "å–º": 1.6, "åšŸ": 1.6, "å’ª": 1.2, "å•±": 1.2, "æ‚": 1.2, "éš": 1.2, "æ›³": 1.2,
    "æ”°": 1.2, "å’": 1.0, "å™‰": 1.0, "å¾—": 0.6, "å–": 0.8, "å†§": 1.0, "æ’š": 1.2,
    "ä»†": 1.2, "å±Œ": 1.2, "å—®": 1.0, "ç•€": 0.8, "æ¸": 1.0, "è…": 0.0
}
CANTONESE_PARTICLES = ["å•¦", "å›‰", "å–", "å’©", "å‘¢", "å‘€", "å˜›", "å–‡"]
CANTONESE_PHRASES = {
    "å¥½å””å¥½ç‡": 2.0, "åšå’©": 1.6, "é»è§£": 1.2, "å’©æ–™": 1.6, "ç®—å•¦": 1.2,
    "å¾—å•¦": 1.2, "æ­£å–": 1.2, "å¹¾å¥½ç‡": 1.6, "å¹¾æ­£": 1.2, "å¥½æ­£": 1.0,
    "æœ‰å•²": 0.8, "å—°å•²": 1.2, "å‘¢å•²": 1.2, "è¬›çœŸ": 0.8, "å¥½ä¼¼": 0.5
}
ROMANIZATION_RE = re.compile(r"(?i)(?<![A-Za-z])(la|lor|wor|leh|meh|mah|ga|wo|ar)(?=[\s\W]|$)")

def count_chars(text: str):
    counts = {
        "cjk": 0, "hiragana": 0, "katakana": 0, "half_katakana": 0,
        "hangul": 0, "latin": 0, "digits": 0, "other": 0
    }
    for ch in text:
        code = ord(ch)
        if 0x4E00 <= code <= 0x9FFF or 0x3400 <= code <= 0x4DBF or 0xF900 <= code <= 0xFAFF:
            counts["cjk"] += 1
        elif 0x3040 <= code <= 0x309F:
            counts["hiragana"] += 1
        elif 0x30A0 <= code <= 0x30FF or 0x31F0 <= code <= 0x31FF:
            counts["katakana"] += 1
        elif 0xFF65 <= code <= 0xFF9F:
            counts["half_katakana"] += 1
        elif 0xAC00 <= code <= 0xD7AF:
            counts["hangul"] += 1
        elif (0x0041 <= code <= 0x005A) or (0x0061 <= code <= 0x007A):
            counts["latin"] += 1
        elif 0x0030 <= code <= 0x0039:
            counts["digits"] += 1
        else:
            counts["other"] += 1
    return counts

def diff_chars(a: str, b: str) -> int:
    m = min(len(a), len(b))
    return sum(1 for i in range(m) if a[i] != b[i]) + abs(len(a) - len(b))

def classify_zh_trad_simp(text: str, cc_t2s: OpenCC, cc_s2t: OpenCC):
    if not isinstance(text, str) or len(text.strip()) < 2:
        return "other"
    counts = count_chars(text)
    kana = counts["hiragana"] + counts["katakana"] + counts["half_katakana"]
    cjk = counts["cjk"]
    if kana >= 2 and kana / max(1, (cjk + kana)) >= 0.10:
        return "ja"
    if cjk < 1:
        return "other"
    t2s = cc_t2s.convert(text)
    s2t = cc_s2t.convert(text)
    ct2s = diff_chars(text, t2s)
    cs2t = diff_chars(text, s2t)
    threshold = max(1, int(0.05 * cjk))
    if ct2s > cs2t + threshold:
        return "zh-Hant"
    elif cs2t > ct2s + threshold:
        return "zh-Hans"
    else:
        return "zh-unkn"

def score_cantonese(text: str) -> float:
    if not isinstance(text, str) or not text:
        return 0.0
    score = 0.0
    for phrase, w in CANTONESE_PHRASES.items():
        cnt = text.count(phrase)
        if cnt:
            score += cnt * w
    for ch, w in CANTONESE_CHAR_TOKENS.items():
        cnt = text.count(ch)
        if cnt:
            score += cnt * w
    end_slice = text[-8:] if len(text) > 8 else text
    for p in CANTONESE_PARTICLES:
        cnt = text.count(p)
        if cnt:
            score += cnt * 0.6
        if p in end_slice:
            score += 0.4
    roman_hits = ROMANIZATION_RE.findall(text)
    if roman_hits:
        score += len(roman_hits) * 0.8
    return score

# =========================
# 2. YouTube æœå°‹ (å«å¿«å–èˆ‡ç¸½é‡æ§åˆ¶)
# =========================

def search_youtube_videos(
    keywords,
    youtube_client,
    max_per_keyword,
    start_date,
    end_date,
    add_language_bias=True,
    region_bias=True,
    max_total_videos=150
):
    all_video_ids = set()
    video_meta = {}
    status_text = st.empty()

    for idx, query in enumerate(keywords):
        if len(all_video_ids) >= max_total_videos:
            status_text.info(f"å·²é”åˆ°æœå°‹ä¸Šé™ ({max_total_videos} éƒ¨)ï¼Œåœæ­¢å¾ŒçºŒæœå°‹ã€‚")
            break

        cache_key = f"{query}_{start_date}_{end_date}_{max_per_keyword}_{add_language_bias}_{region_bias}"
        cached_data = _get_cached_value("search_cache", cache_key, CACHE_TTL_SEARCH)

        query_records = []

        if cached_data is not None:
            query_records = cached_data
        else:
            collected_records = []
            collected_ids_for_query = set()
            
            for order in ["relevance", "viewCount"]:
                if len(collected_records) >= max_per_keyword:
                    break

                try:
                    request = youtube_client.search().list(
                        q=query,
                        part="id,snippet",
                        type="video",
                        maxResults=50,
                        publishedAfter=f"{start_date}T00:00:00Z",
                        publishedBefore=f"{end_date}T23:59:59Z",
                        order=order,
                        safeSearch="none",
                        **({"relevanceLanguage": "zh-Hant"} if add_language_bias else {}),
                        **({"regionCode": "HK"} if region_bias else {})
                    )
                    
                    while request and len(collected_records) < max_per_keyword:
                        response = request.execute()
                        for item in response.get("items", []):
                            vid = item["id"]["videoId"]
                            if vid in collected_ids_for_query:
                                continue
                            
                            collected_ids_for_query.add(vid)
                            snip = item.get("snippet", {})
                            record = {
                                "video_id": vid,
                                "title": snip.get("title", ""),
                                "channelTitle": snip.get("channelTitle", ""),
                                "publishedAt": snip.get("publishedAt", "")
                            }
                            collected_records.append(record)
                        
                        if len(collected_records) >= max_per_keyword:
                            break
                            
                        request = youtube_client.search().list_next(request, response)
                        time.sleep(0.1)
                except Exception as e:
                    st.warning(f"æœå°‹ '{query}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    continue
            
            _set_cached_value("search_cache", cache_key, collected_records)
            query_records = collected_records

        for record in query_records:
            vid = record["video_id"]
            if vid not in all_video_ids:
                all_video_ids.add(vid)
                if vid not in video_meta:
                    video_meta[vid] = {
                        "title": record["title"],
                        "channelTitle": record["channelTitle"],
                        "publishedAt": record["publishedAt"]
                    }
            if len(all_video_ids) >= max_total_videos:
                break
    
    status_text.empty()
    return list(all_video_ids), video_meta

# =========================
# NEW: AI å½±ç‰‡ç›¸é—œæ€§éæ¿¾
# =========================

async def check_relevance_batch_async(movie_title, batch_videos, deepseek_client):
    """
    ä½¿ç”¨ DeepSeek æ‰¹é‡åˆ¤æ–·å½±ç‰‡æ˜¯å¦èˆ‡é›»å½±ç›¸é—œã€‚
    batch_videos: list of {"id": vid, "title": title, "channel": channel}
    """
    if not batch_videos:
        return []

    prompt_items = []
    for v in batch_videos:
        prompt_items.append(f"ID: {v['id']}\nTitle: {v['title']}\nChannel: {v['channel']}")
    
    prompt_text = "\n---\n".join(prompt_items)

    system_prompt = (
        f"You are a data cleaner. The user is analyzing the movie '{movie_title}'. "
        "Below is a list of YouTube video titles found by search. "
        "Identify which videos are actually discussing this specific movie (reviews, reactions, news, clips, interviews). "
        "Exclude videos that are clearly unrelated (e.g., generic news, other movies, music videos not related to the film, or completely different topics). "
        "Return a JSON object where keys are the Video IDs and values are boolean true (relevant) or false (irrelevant). "
        "Example: {\"vid123\": true, \"vid456\": false}"
    )

    try:
        response = await deepseek_client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt_text}
            ],
            response_format={"type": "json_object"},
            temperature=0.1,
        )
        data = json.loads(response.choices[0].message.content)
        # æå–ç‚º True çš„ ID
        valid_ids = [vid for vid, is_relevant in data.items() if is_relevant is True]
        return valid_ids
    except Exception as e:
        # å¦‚æœ API å¤±æ•—ï¼Œç‚ºäº†å®‰å…¨èµ·è¦‹ï¼Œé»˜èªä¿ç•™ï¼ˆFail Openï¼‰ï¼Œä»¥å…ä¸Ÿå¤±æ•¸æ“š
        # æˆ–è€…ä¹Ÿå¯ä»¥é¸æ“‡åƒ…åœ¨å¤±æ•—æ™‚ä¿ç•™åŒ…å«é›»å½±åçš„
        print(f"Relevance check failed: {e}")
        return [v['id'] for v in batch_videos]

async def filter_videos_by_relevance(movie_title, video_ids, video_meta, deepseek_client):
    """
    ä¸»å…¥å£ï¼šéæ¿¾å½±ç‰‡åˆ—è¡¨
    """
    # 1. æª¢æŸ¥å¿«å–
    to_check = []
    valid_ids = set()
    
    for vid in video_ids:
        cached_res = _get_cached_value("relevance_cache", f"{movie_title}_{vid}", CACHE_TTL_RELEVANCE)
        if cached_res is not None:
            if cached_res:
                valid_ids.add(vid)
        else:
            meta = video_meta.get(vid, {})
            to_check.append({
                "id": vid,
                "title": meta.get("title", ""),
                "channel": meta.get("channelTitle", "")
            })
    
    # 2. æ‰¹é‡é€å» AI æª¢æŸ¥
    if to_check:
        batch_size = 20
        tasks = []
        
        # åˆ†æ‰¹å»ºç«‹ç•°æ­¥ä»»å‹™
        for i in range(0, len(to_check), batch_size):
            batch = to_check[i:i+batch_size]
            tasks.append(check_relevance_batch_async(movie_title, batch, deepseek_client))
        
        # åŸ·è¡Œä»»å‹™
        progress_text = st.empty()
        progress_text.info(f"æ­£åœ¨ä½¿ç”¨ AI éæ¿¾ {len(to_check)} éƒ¨å½±ç‰‡çš„ç›¸é—œæ€§...")
        
        results = await asyncio.gather(*tasks)
        
        # æ•´åˆçµæœä¸¦å¯«å…¥å¿«å–
        for batch_idx, relevant_list in enumerate(results):
            batch_input = to_check[batch_idx*batch_size : (batch_idx+1)*batch_size]
            # å»ºç«‹ä¸€å€‹ lookup set
            rel_set = set(relevant_list)
            
            for item in batch_input:
                vid = item["id"]
                is_rel = vid in rel_set
                if is_rel:
                    valid_ids.add(vid)
                
                # å¯«å…¥å¿«å–
                _set_cached_value("relevance_cache", f"{movie_title}_{vid}", is_rel)
        
        progress_text.empty()

    return list(valid_ids)

# =========================
# 3. ç²å–è©³æƒ…èˆ‡ç•™è¨€
# =========================

def fetch_video_and_channel_details(video_ids, youtube_client):
    video_extra = {}
    channel_ids = set()

    for i in range(0, len(video_ids), 50):
        chunk = video_ids[i:i+50]
        try:
            resp = youtube_client.videos().list(
                part="snippet,contentDetails",
                id=",".join(chunk)
            ).execute()
            for item in resp.get("items", []):
                vid = item.get("id")
                snip = item.get("snippet", {}) or {}
                ch = snip.get("channelId")
                video_extra[vid] = {
                    "channelId": ch,
                    "defaultLanguage": (snip.get("defaultLanguage") or ""),
                    "defaultAudioLanguage": (snip.get("defaultAudioLanguage") or ""),
                    "tags": snip.get("tags", [])
                }
                if ch:
                    channel_ids.add(ch)
        except Exception as e:
            st.warning(f"videos.list å–è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    channel_country = {}
    channels_to_fetch = []

    for cid in channel_ids:
        cached_country = _get_cached_value("channel_cache", cid, CACHE_TTL_CHANNEL)
        if cached_country is not None:
            channel_country[cid] = cached_country
        else:
            channels_to_fetch.append(cid)
    
    if channels_to_fetch:
        for i in range(0, len(channels_to_fetch), 50):
            chunk = channels_to_fetch[i:i+50]
            try:
                resp = youtube_client.channels().list(
                    part="brandingSettings",
                    id=",".join(chunk)
                ).execute()
                for item in resp.get("items", []):
                    cid = item.get("id")
                    brand = (item.get("brandingSettings", {}) or {}).get("channel", {}) or {}
                    country = brand.get("country")
                    channel_country[cid] = country
                    _set_cached_value("channel_cache", cid, country)
            except Exception as e:
                st.warning(f"channels.list å–è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    return video_extra, channel_country

def compute_hk_video_score(video_id, video_meta, video_extra, channel_country_map):
    meta = video_meta.get(video_id, {}) or {}
    ext = video_extra.get(video_id, {}) or {}
    title = meta.get("title", "") or ""
    tags = " ".join(ext.get("tags", []) or [])
    ch = ext.get("channelId")
    default_audio = (ext.get("defaultAudioLanguage") or "").lower()
    country = channel_country_map.get(ch)

    score = 0
    if country == "HK": score += 3
    if default_audio in ("yue", "zh-hk", "zh-yue", "zh-hant-hk"): score += 3
    elif default_audio.startswith("zh"): score += 1
    
    if any(tok in title for tok in ["ç²µèª", "å»£æ±è©±", "ç²µé…", "ç²µèªé…éŸ³"]): score += 3
    if any(tok in title for tok in ["é¦™æ¸¯", "æ¸¯ç‰ˆ", "é¦™æ¸¯è§€çœ¾", "é¦™æ¸¯åæ‡‰", "é¦™æ¸¯é¦–æ˜ ", "é¦™æ¸¯ä¸Šæ˜ "]): score += 2
    if ("HK" in title) or ("Hong Kong" in title): score += 1
    if any(tok in tags for tok in ["ç²µèª", "å»£æ±è©±", "é¦™æ¸¯", "HK"]): score += 2
    return score

def get_all_comments(
    video_ids, youtube_client, max_per_video, 
    video_meta=None, hk_score_map=None, video_extra=None, channel_country_map=None,
    max_total_comments=2000
):
    video_meta = video_meta or {}
    hk_score_map = hk_score_map or {}
    video_extra = video_extra or {}
    channel_country_map = channel_country_map or {}

    all_comments = []
    total_videos = len(video_ids)
    progress_bar = st.progress(0, text="æŠ“å– YouTube ç•™è¨€ä¸­...")
    
    total_fetched_count = 0

    for i, video_id in enumerate(video_ids):
        if total_fetched_count >= max_total_comments:
            break

        cache_key = f"{video_id}_{max_per_video}"
        cached_comments = _get_cached_value("comments_cache", cache_key, CACHE_TTL_COMMENTS)
        
        current_video_comments = []

        if cached_comments is not None:
            current_video_comments = cached_comments
        else:
            try:
                request = youtube_client.commentThreads().list(
                    part="snippet",
                    videoId=video_id,
                    textFormat="plainText",
                    order="time",
                    maxResults=100
                )
                fetched_for_video = 0
                raw_records = []
                
                while request and fetched_for_video < max_per_video:
                    if total_fetched_count + fetched_for_video >= max_total_comments:
                        break

                    response = request.execute()
                    for item in response.get("items", []):
                        if fetched_for_video >= max_per_video:
                            break
                        comment = item["snippet"]["topLevelComment"]["snippet"]
                        
                        record = {
                            "textDisplay": comment.get("textDisplay", ""),
                            "publishedAt": comment.get("publishedAt", ""),
                            "likeCount": comment.get("likeCount", 0)
                        }
                        raw_records.append(record)
                        fetched_for_video += 1
                    
                    request = youtube_client.commentThreads().list_next(request, response)
                    if request and fetched_for_video < max_per_video:
                        time.sleep(0.1)
                
                if raw_records:
                    _set_cached_value("comments_cache", cache_key, raw_records)
                current_video_comments = raw_records

            except Exception:
                pass

        ch_id = (video_extra.get(video_id, {}) or {}).get("channelId")
        ch_country = channel_country_map.get(ch_id) if ch_id else None
        def_audio = (video_extra.get(video_id, {}) or {}).get("defaultAudioLanguage", "")
        v_title = video_meta.get(video_id, {}).get("title", "")
        v_score = hk_score_map.get(video_id, 0)

        for raw in current_video_comments:
            all_comments.append({
                "video_id": video_id,
                "video_title": v_title,
                "video_url": f"https://www.youtube.com/watch?v={video_id}",
                "video_hk_score": v_score,
                "video_channel_id": ch_id,
                "video_channel_country": ch_country,
                "video_default_audio_lang": def_audio,
                "comment_text": raw["textDisplay"],
                "published_at": raw["publishedAt"],
                "like_count": raw["likeCount"]
            })
        
        total_fetched_count += len(current_video_comments)

        progress_bar.progress(
            (i + 1) / max(1, total_videos),
            text=f"æŠ“å– YouTube ç•™è¨€ä¸­... ({min(i+1, total_videos)}/{total_videos} éƒ¨å½±ç‰‡, å·²æŠ“ {total_fetched_count} å‰‡)"
        )

    progress_bar.empty()
    return pd.DataFrame(all_comments)

# =========================
# 4. DeepSeek AI æƒ…æ„Ÿåˆ†æ
# =========================

async def analyze_comment_deepseek_async(comment_text, deepseek_client, semaphore, max_retries=3):
    if not isinstance(comment_text, str) or len(comment_text.strip()) < 5:
        return {"sentiment": "Invalid", "topic": "N/A", "summary": "Comment too short or invalid."}

    system_prompt = (
        "You are a professional Hong Kong market sentiment analyst. "
        "Analyze the following movie comment and strictly return the result in JSON format. "
        "The JSON object must contain three keys: "
        "1. 'sentiment': Must be either 'Positive', 'Negative', or 'Neutral'. "
        "2. 'topic': The core topic of the comment, e.g., 'Plot', 'Acting', 'Action Design', "
        "'Visuals', 'Pace', or 'Overall'. If unable to determine, use 'N/A'. "
        "3. 'summary': A concise one-sentence summary of the comment's main point. "
        "Ensure the output is only the JSON object and nothing else."
    )

    async with semaphore:
        for attempt in range(max_retries):
            try:
                response = await deepseek_client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": comment_text}
                    ],
                    response_format={"type": "json_object"},
                    temperature=0.1,
                )
                data = response.choices[0].message.content
                analysis_result = json.loads(data)
                return analysis_result
            except Exception as e:
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                else:
                    return {"sentiment": "Error", "topic": "Error", "summary": f"API Error: {e}"}

async def run_all_analyses(df, deepseek_client):
    semaphore = asyncio.Semaphore(50)
    tasks = []

    async def with_index(idx, text):
        res = await analyze_comment_deepseek_async(text, deepseek_client, semaphore)
        return idx, res

    for i, text in enumerate(df["comment_text"]):
        tasks.append(asyncio.create_task(with_index(i, text)))

    progress_bar = st.progress(0, text="AI æƒ…æ„Ÿåˆ†æä¸­...")
    results = [None] * len(tasks)
    for done_idx, coro in enumerate(asyncio.as_completed(tasks), start=1):
        idx, res = await coro
        results[idx] = res
        progress_bar.progress(done_idx / len(tasks), text=f"AI æƒ…æ„Ÿåˆ†æä¸­... ({done_idx}/{len(tasks)})")
    progress_bar.empty()
    return results

# =========================
# 5. ä¸»æµç¨‹
# =========================

def movie_comment_analysis(
    movie_title, start_date, end_date,
    yt_api_key, deepseek_api_key,
    max_videos_per_keyword=30, max_comments_per_video=50, sample_size=None,
    relax_trad_filter=True,
    cantonese_threshold=2.0,
    auto_relax_threshold=True,
    target_min_cantonese=300,
    prefer_hk_videos=True
):
    target_sample = sample_size if sample_size and sample_size > 0 else 1000
    GLOBAL_MAX_COMMENTS = max(2000, target_sample * 4) 
    GLOBAL_MAX_VIDEOS = 150

    SEARCH_KEYWORDS = generate_search_queries(movie_title)

    youtube_client = build("youtube", "v3", developerKey=yt_api_key)
    deepseek_client = openai.AsyncOpenAI(
        api_key=deepseek_api_key,
        base_url="https://api.deepseek.com/v1"
    )

    # 1) æœå°‹
    video_ids, video_meta = search_youtube_videos(
        SEARCH_KEYWORDS, youtube_client, max_videos_per_keyword, start_date, end_date,
        add_language_bias=True, region_bias=True,
        max_total_videos=GLOBAL_MAX_VIDEOS
    )
    if not video_ids:
        return None, "æ‰¾ä¸åˆ°ç›¸é—œå½±ç‰‡ã€‚"
    
    st.info(f"åˆæ­¥æœå°‹åˆ° {len(video_ids)} éƒ¨å½±ç‰‡ï¼Œæ­£åœ¨é€²è¡Œ AI ç›¸é—œæ€§éæ¿¾...")

    # 2) NEW: AI ç›¸é—œæ€§éæ¿¾ (ä½¿ç”¨ DeepSeek)
    relevant_video_ids = asyncio.run(filter_videos_by_relevance(movie_title, video_ids, video_meta, deepseek_client))
    
    removed_count = len(video_ids) - len(relevant_video_ids)
    if removed_count > 0:
        st.warning(f"AI å·²éæ¿¾æ‰ {removed_count} éƒ¨èˆ‡ã€Œ{movie_title}ã€ä¸ç›¸é—œçš„å½±ç‰‡ï¼Œä¿ç•™ {len(relevant_video_ids)} éƒ¨é€²è¡Œåˆ†æã€‚")
    else:
        st.info("æ‰€æœ‰æœå°‹åˆ°çš„å½±ç‰‡å‡è¢«åˆ¤å®šç‚ºç›¸é—œã€‚")

    if not relevant_video_ids:
        return None, "AI éæ¿¾å¾Œæ²’æœ‰å‰©é¤˜ç›¸é—œå½±ç‰‡ï¼Œè«‹å˜—è©¦æ›´æ›é—œéµå­—æˆ–æª¢æŸ¥é›»å½±åç¨±ã€‚"

    # 3) ç²å–è©³ç´°è³‡æ–™ (åªé‡å°ç›¸é—œå½±ç‰‡)
    video_extra, channel_country_map = fetch_video_and_channel_details(relevant_video_ids, youtube_client)

    # 4) å½±ç‰‡é¦™æ¸¯å‚¾å‘æ’åº
    hk_score_map = {vid: compute_hk_video_score(vid, video_meta, video_extra, channel_country_map) for vid in relevant_video_ids}
    video_ids_sorted = sorted(relevant_video_ids, key=lambda v: hk_score_map.get(v, 0), reverse=True) if prefer_hk_videos else relevant_video_ids

    # 5) æŠ“å–ç•™è¨€
    df_comments = get_all_comments(
        video_ids_sorted, youtube_client, max_comments_per_video,
        video_meta=video_meta, hk_score_map=hk_score_map,
        video_extra=video_extra, channel_country_map=channel_country_map,
        max_total_comments=GLOBAL_MAX_COMMENTS
    )
    if df_comments.empty:
        return None, "æ‰¾ä¸åˆ°ä»»ä½•ç•™è¨€ã€‚"

    # 6) èªè¨€éæ¿¾
    st.info(f"å·²æŠ“å– {len(df_comments)} å‰‡åŸå§‹ç•™è¨€ï¼Œç¾é–‹å§‹èªè¨€èˆ‡ç²µèªç¯©é¸...")

    cc_t2s = OpenCC("t2s")
    cc_s2t = OpenCC("s2t")

    def lang_pred(text):
        return classify_zh_trad_simp(text, cc_t2s, cc_s2t)

    df_comments["lang_pred"] = df_comments["comment_text"].apply(lang_pred)
    df_comments = df_comments[~df_comments["lang_pred"].isin(["ja", "other", "zh-Hans"])].reset_index(drop=True)

    if relax_trad_filter:
        df_comments = df_comments[df_comments["lang_pred"].isin(["zh-Hant", "zh-unkn"])].reset_index(drop=True)
    else:
        df_comments = df_comments[df_comments["lang_pred"] == "zh-Hant"].reset_index(drop=True)

    if df_comments.empty:
        return None, "åœ¨æŠ“å–çš„ç•™è¨€ä¸­æ²’æœ‰ç¬¦åˆåŸºæœ¬èªè¨€æ¢ä»¶çš„å…§å®¹ã€‚"

    # ç²µèªåˆ†æ•¸
    df_comments["cantonese_score"] = df_comments["comment_text"].apply(score_cantonese)

    # 7) ç²µèªé–€æª» + è‡ªå‹•æ”¾å¯¬
    thr = float(cantonese_threshold)
    def filt(t): return t >= thr
    df_filtered = df_comments[df_comments["cantonese_score"].apply(filt)].reset_index(drop=True)

    if auto_relax_threshold and len(df_filtered) < target_min_cantonese:
        new_thr = thr
        while len(df_filtered) < target_min_cantonese and new_thr > 0.5:
            new_thr = round(new_thr - 0.5, 2)
            df_filtered = df_comments[df_comments["cantonese_score"] >= new_thr].reset_index(drop=True)
        if new_thr != thr:
            st.info(f"è‡ªå‹•æ”¾å¯¬ç²µèªåˆ†æ•¸é–€æª»ï¼š{thr} âœ {new_thr}ï¼ˆç›®å‰ç¬¦åˆæ¢ä»¶ç•™è¨€ï¼š{len(df_filtered)}ï¼‰")
            thr = new_thr

    st.info(f"èªè¨€èˆ‡ç²µèªç¯©é¸å¾Œå‰©ä¸‹ {len(df_filtered)} å‰‡ç•™è¨€ï¼ˆé–€æª»={thr}ï¼‰ã€‚")
    if df_filtered.empty:
        return None, "ç²µèªç¯©é¸å¾Œæ¨£æœ¬ç‚º 0ï¼Œè«‹èª¿ä½é–€æª»æˆ–å»¶é•·æ™‚é–“ç¯„åœã€‚"

    # 8) æ™‚å€èˆ‡æ—¥æœŸç¯©é¸
    df_filtered["published_at"] = pd.to_datetime(df_filtered["published_at"], utc=True, errors="coerce")
    df_filtered["published_at_hk"] = df_filtered["published_at"].dt.tz_convert("Asia/Hong_Kong")

    start_dt = pd.to_datetime(start_date).tz_localize("Asia/Hong_Kong")
    end_dt = pd.to_datetime(end_date).tz_localize("Asia/Hong_Kong") + timedelta(days=1)
    mask_date = (df_filtered["published_at_hk"] >= start_dt) & (df_filtered["published_at_hk"] < end_dt)
    df_filtered = df_filtered.loc[mask_date].reset_index(drop=True)
    if df_filtered.empty:
        return None, "åœ¨æŒ‡å®šæ—¥æœŸç¯„åœå…§æ²’æœ‰ç¬¦åˆç²µèªæ¢ä»¶çš„ç•™è¨€ã€‚"

    # 9) å–æ¨£æ§åˆ¶
    if sample_size and 0 < sample_size < len(df_filtered):
        df_analyze = df_filtered.sample(n=sample_size, random_state=42)
    else:
        df_analyze = df_filtered

    st.info(f"æº–å‚™å° {len(df_analyze)} å‰‡ç•™è¨€é€²è¡Œé«˜é€Ÿä¸¦ç™¼åˆ†æ...")

    # 10) DeepSeek åˆ†æ
    analysis_results = asyncio.run(run_all_analyses(df_analyze, deepseek_client))
    analysis_df = pd.DataFrame(analysis_results)
    final_df = pd.concat([df_analyze.reset_index(drop=True), analysis_df], axis=1)

    final_df["published_at"] = pd.to_datetime(final_df["published_at"])
    return final_df, None

# =========================
# 6. Streamlit UI
# =========================

st.set_page_config(page_title="YouTube é›»å½±è©•è«– AI åˆ†æï¼ˆé¦™æ¸¯ç²µèªå„ªå…ˆï¼‰", layout="wide")
st.title("ğŸ¬ YouTube é›»å½±è©•è«– AI æƒ…æ„Ÿåˆ†æï¼ˆé¦™æ¸¯ç²µèªå„ªå…ˆï¼‰")

with st.expander("ä½¿ç”¨èªªæ˜"):
    st.markdown("""
    1.  è¼¸å…¥é›»å½±çš„ä¸­æ–‡å…¨åã€åˆ†ææ™‚é–“ç¯„åœåŠæ‰€éœ€çš„ API é‡‘é‘°ã€‚
    2.  æœ¬å·¥å…·æœƒåå‘æŠ“å–é¦™æ¸¯åœ°å€çš„å½±ç‰‡èˆ‡ç•™è¨€ï¼Œä¸¦ç”¨ç²µèªç‰¹å¾µæ‰“åˆ†éæ¿¾ã€‚
    3.  **å„ªåŒ–ç‰ˆ**ï¼š
        *   **æ™ºèƒ½å¿«å–**ï¼šé‡è¤‡æŸ¥è©¢ä¸æ¶ˆè€— YouTube é…é¡ã€‚
        *   **AI ç›¸é—œæ€§éæ¿¾**ï¼šä½¿ç”¨ DeepSeek è‡ªå‹•å‰”é™¤æ¨™é¡Œä¸ç›¸é—œçš„å½±ç‰‡ï¼Œç¢ºä¿åˆ†æç²¾æº–åº¦ä¸¦ç¯€çœ YouTube ç•™è¨€æŠ“å–é…é¡ã€‚
    4.  åˆ†æå®Œæˆå¾Œï¼Œæä¾›å¯è¦–åŒ–èˆ‡ CSV ä¸‹è¼‰ã€‚
    """)

movie_title = st.text_input("é›»å½±åç¨± (å»ºè­°ä½¿ç”¨é¦™æ¸¯é€šç”¨çš„ä¸­æ–‡å…¨å)", value="ä¹é¾åŸå¯¨ä¹‹åœåŸ")
col1, col2 = st.columns(2)
with col1:
    start_date = st.date_input("é–‹å§‹æ—¥æœŸ", value=datetime.today() - timedelta(days=30))
with col2:
    end_date = st.date_input("çµæŸæ—¥æœŸ", value=datetime.today())
yt_api_key = st.text_input("YouTube API Key", type='password')
deepseek_api_key = st.text_input("DeepSeek API Key", type='password')

st.subheader("é€²éšè¨­å®š")
max_videos = st.slider("æ¯å€‹é—œéµå­—çš„æœ€å¤§å½±ç‰‡æœå°‹æ•¸", 5, 80, 30, help="æé«˜å¯å¢åŠ è¦†è“‹ï¼Œä½†æœƒå¢åŠ  YouTube API é…é¡æ¶ˆè€—ã€‚")
max_comments = st.slider("æ¯éƒ¨å½±ç‰‡çš„æœ€å¤§ç•™è¨€æŠ“å–æ•¸", 10, 200, 80, help="æ•¸é‡è¶Šå¤šï¼Œåˆ†æçµæœè¶Šå…¨é¢ï¼Œä½† DeepSeek API æˆæœ¬è¶Šé«˜ã€‚")
sample_size = st.number_input("åˆ†æç•™è¨€æ•¸é‡ä¸Šé™ (0=å…¨é‡)", 0, 5000, 500)

relax_trad_filter = st.checkbox("æ”¾å¯¬ç¹é«”åˆ¤å®šï¼ˆå…è¨±é›£åˆ†çš„ä¸­æ–‡ç•™è¨€ï¼‰", value=True)
prefer_hk_videos = st.checkbox("å„ªå…ˆæŠ“å–æ›´å¯èƒ½ä¾†è‡ªé¦™æ¸¯/ç²µèªçš„å½±ç‰‡ï¼ˆæ’åºåŠ æ¬Šï¼‰", value=True)

cantonese_threshold = st.slider("ç²µèªåˆ†æ•¸é–€æª»", 0.5, 6.0, 2.0, 0.5, help="åˆ†æ•¸è¶Šé«˜è¶Šåš´æ ¼ï¼Œ2.0 æ˜¯è¼ƒç©©å¥çš„é–€æª»ã€‚")
auto_relax_threshold = st.checkbox("è‡ªå‹•æ”¾å¯¬é–€æª»ä»¥é”åˆ°ç›®æ¨™æ¨£æœ¬é‡", value=True)
target_min_cantonese = st.number_input("ç›®æ¨™æœ€å°‘ç²µèªè©•è«–æ•¸ï¼ˆå•Ÿç”¨è‡ªå‹•æ”¾å¯¬æ™‚ç”Ÿæ•ˆï¼‰", 50, 5000, 300)

if st.button("ğŸš€ é–‹å§‹åˆ†æ"):
    if not all([movie_title, yt_api_key, deepseek_api_key]):
        st.warning("è«‹å¡«å¯«é›»å½±åç¨±å’Œå…©å€‹ API é‡‘é‘°ã€‚")
    else:
        with st.spinner("AI é«˜é€Ÿåˆ†æä¸­... è«‹ç¨å€™..."):
            df_result, err = movie_comment_analysis(
                movie_title, str(start_date), str(end_date),
                yt_api_key, deepseek_api_key,
                max_videos, max_comments, sample_size,
                relax_trad_filter=relax_trad_filter,
                cantonese_threshold=cantonese_threshold,
                auto_relax_threshold=auto_relax_threshold,
                target_min_cantonese=target_min_cantonese,
                prefer_hk_videos=prefer_hk_videos
            )

        if err:
            st.error(err)
        else:
            st.success("åˆ†æå®Œæˆï¼")
            st.dataframe(df_result.head(20), use_container_width=True)

            st.header("ğŸ“Š å¯è¦–åŒ–åˆ†æçµæœ")

            sentiments_order = ['Positive', 'Negative', 'Neutral', 'Invalid', 'Error']
            colors_map = {
                'Positive': '#5cb85c', 'Negative': '#d9534f', 'Neutral': '#f0ad4e',
                'Invalid': '#cccccc', 'Error': '#888888'
            }

            # 1. æƒ…æ„Ÿåˆ†ä½ˆ
            st.subheader("1. Sentiment Distribution (Pie)")
            sentiment_series = df_result['sentiment'].dropna().astype(str)
            sentiment_counts = sentiment_series.value_counts()
            ordered_labels = [label for label in sentiments_order if label in sentiment_counts.index]
            if not sentiment_counts.empty:
                fig1 = px.pie(
                    values=sentiment_counts[ordered_labels].values,
                    names=ordered_labels,
                    title='Overall Sentiment Distribution',
                    color=ordered_labels,
                    color_discrete_map=colors_map
                )
                st.plotly_chart(fig1, use_container_width=True)
            else:
                st.info("No sentiment data available for pie chart.")

            # 2. æ¯æ—¥æƒ…æ„Ÿè¶¨å‹¢
            st.subheader("2. Daily Sentiment Trend")
            if 'published_at_hk' in df_result.columns:
                df_result['date'] = df_result['published_at_hk'].dt.date
            else:
                df_result['date'] = pd.to_datetime(df_result['published_at'], utc=True).dt.tz_convert('Asia/Hong_Kong').dt.date

            daily = df_result.groupby(['date', 'sentiment']).size().unstack().fillna(0)
            daily = daily.reindex(columns=sentiments_order).dropna(axis=1, how='all')
            if not daily.empty:
                daily_long = daily.reset_index().melt(id_vars='date', var_name='sentiment', value_name='count')
                fig_line = px.line(
                    daily_long, x='date', y='count', color='sentiment',
                    title='Daily Comment Volume Trend by Sentiment',
                    labels={'date': 'Date', 'count': 'Number of Comments', 'sentiment': 'Sentiment'},
                    color_discrete_map=colors_map
                )
                st.plotly_chart(fig_line, use_container_width=True)

                fig_bar = px.bar(
                    daily_long, x='date', y='count', color='sentiment',
                    title='Daily Comment Volume by Sentiment (Stacked)',
                    labels={'date': 'Date', 'count': 'Number of Comments', 'sentiment': 'Sentiment'},
                    color_discrete_map=colors_map,
                    barmode='stack'
                )
                st.plotly_chart(fig_bar, use_container_width=True)
            else:
                st.info("Not enough daily sentiment data to display the trend charts.")

            # 3. å„ä¸»é¡Œæƒ…æ„Ÿä½”æ¯”
            st.subheader("3. Sentiment Share by Topic")
            topic_sentiment = df_result.groupby(['topic', 'sentiment']).size().unstack().fillna(0)
            topic_sentiment = topic_sentiment.reindex(columns=sentiments_order).dropna(axis=1, how='all')
            if not topic_sentiment.empty:
                topic_sentiment = topic_sentiment[topic_sentiment.sum(axis=1) > 0]
                if not topic_sentiment.empty:
                    topic_sentiment_percent = topic_sentiment.div(topic_sentiment.sum(axis=1), axis=0).fillna(0) * 100
                    fig3 = px.bar(
                        topic_sentiment_percent.reset_index().melt(id_vars='topic', var_name='sentiment', value_name='pct'),
                        x='topic', y='pct', color='sentiment',
                        title='Sentiment Share by Topic',
                        labels={'topic': 'Topic', 'pct': 'Percentage (%)', 'sentiment': 'Sentiment'},
                        color_discrete_map=colors_map
                    )
                    st.plotly_chart(fig3, use_container_width=True)
                else:
                    st.info("No topic data with comments to display the chart.")
            else:
                st.info("Not enough topic sentiment data to display the stacked bar chart.")

            # 4. ä¸‹è¼‰åˆ†ææ˜ç´°
            st.subheader("4. ä¸‹è¼‰åˆ†ææ˜ç´°")
            csv = df_result.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                "ğŸ“¥ ä¸‹è¼‰å…¨éƒ¨åˆ†ææ˜ç´° (CSV)",
                csv,
                file_name=f"{movie_title}_hk_cantonese_analysis.csv",
                mime='text/csv'
            )
