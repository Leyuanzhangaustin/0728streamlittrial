import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import time
import asyncio
import openai
import re
import json
from googleapiclient.discovery import build
from collections import Counter

# =========================
# 0. ç·©å­˜èˆ‡å·¥å…·è¨­å®š
# =========================

CACHE_TTL_SEARCH = 3600          # 1 å°æ™‚
CACHE_TTL_COMMENTS = 900         # 15 åˆ†é˜

def _get_cached_value(cache_name: str, key, ttl_seconds: int):
    cache = st.session_state.setdefault(cache_name, {})
    entry = cache.get(key)
    if entry and (time.time() - entry["ts"] <= ttl_seconds):
        return entry["value"]
    return None

def _set_cached_value(cache_name: str, key, value):
    cache = st.session_state.setdefault(cache_name, {})
    cache[key] = {"value": value, "ts": time.time()}

def generate_search_queries(movie_title: str):
    # ç‚ºäº†ç¢ºä¿èƒ½æœåˆ°ï¼Œæˆ‘å€‘ä½¿ç”¨ç²¾ç¢ºåŒ¹é…çš„é‚è¼¯ï¼Œä½†åœ¨ API æŸ¥è©¢æ™‚é‚„æ˜¯è¦çµ¦ä¸€é»å»£åº¦
    # åš´æ ¼éæ¿¾æœƒåœ¨ä»£ç¢¼å±¤é¢åš
    return [
        f"{movie_title}",
        f"{movie_title} å½±è©•",
        f"{movie_title} è©•åƒ¹",
        f"{movie_title} é¦™æ¸¯",
        f"{movie_title} ç²µèª"
    ]

# =========================
# 1. YouTube API æ ¸å¿ƒ (å«åš´æ ¼æ¨™é¡Œéæ¿¾)
# =========================

def search_youtube_videos_strict(
    keywords, youtube_client, movie_title,
    max_per_keyword, max_total_videos,
    start_date, end_date
):
    all_video_ids = set()
    video_meta = {}
    search_cache_name = "yt_search_strict_cache"
    
    progress_text = "æ­£åœ¨æœå°‹ä¸¦åš´æ ¼éæ¿¾å½±ç‰‡..."
    my_bar = st.progress(0, text=progress_text)
    
    # é è™•ç†é›»å½±æ¨™é¡Œï¼Œè½‰å°å¯«ä»¥é€²è¡Œä¸å€åˆ†å¤§å°å¯«çš„åŒ¹é…
    target_title_lower = movie_title.strip().lower()
    
    for idx, query in enumerate(keywords):
        if len(all_video_ids) >= max_total_videos: break
            
        cache_key = f"{query}_{start_date}_{end_date}_{max_per_keyword}_strict"
        cached_records = _get_cached_value(search_cache_name, cache_key, CACHE_TTL_SEARCH)
        
        query_records = []
        
        if cached_records is None:
            try:
                request = youtube_client.search().list(
                    q=query, part="id,snippet", type="video", maxResults=max_per_keyword,
                    publishedAfter=f"{start_date}T00:00:00Z", publishedBefore=f"{end_date}T23:59:59Z",
                    order="relevance", safeSearch="none", relevanceLanguage="zh-Hant", regionCode="HK"
                )
                response = request.execute()
                for item in response.get("items", []):
                    vid = item["id"]["videoId"]
                    snip = item.get("snippet", {})
                    title = snip.get("title", "")
                    
                    # === æ ¸å¿ƒä¿®æ”¹ï¼š100% åš´æ ¼æ¨™é¡ŒåŒ¹é… ===
                    # åªæœ‰ç•¶é›»å½±åç¨±å®Œæ•´å‡ºç¾åœ¨æ¨™é¡Œä¸­æ‰ä¿ç•™
                    if target_title_lower in title.lower():
                        query_records.append({
                            "id": vid,
                            "title": title,
                            "channelTitle": snip.get("channelTitle", ""),
                            "publishedAt": snip.get("publishedAt", "")
                        })
                
                _set_cached_value(search_cache_name, cache_key, query_records)
            except Exception as e:
                st.warning(f"æœå°‹ '{query}' å¤±æ•—: {e}")
        else:
            query_records = cached_records

        for rec in query_records:
            if rec["id"] not in all_video_ids:
                all_video_ids.add(rec["id"])
                video_meta[rec["id"]] = rec
                if len(all_video_ids) >= max_total_videos: break
        
        my_bar.progress((idx + 1) / len(keywords), text=f"æœå°‹ä¸­... ç¬¦åˆåš´æ ¼æ¨™é¡Œæ¢ä»¶çš„å½±ç‰‡: {len(all_video_ids)} éƒ¨")

    my_bar.empty()
    return list(all_video_ids), video_meta

def get_all_comments_cached(video_ids, youtube_client, max_per_video, max_total_comments, video_meta):
    all_comments = []
    comments_cache_name = "yt_comments_cache_v2"
    
    progress_bar = st.progress(0, text="æŠ“å–ç•™è¨€ä¸­...")
    
    for i, vid in enumerate(video_ids):
        if len(all_comments) >= max_total_comments: break

        cache_key = f"comments_{vid}_{max_per_video}"
        cached_comments = _get_cached_value(comments_cache_name, cache_key, CACHE_TTL_COMMENTS)
        
        video_comments = []
        if cached_comments is not None:
            video_comments = cached_comments
        else:
            try:
                request = youtube_client.commentThreads().list(
                    part="snippet", videoId=vid, textFormat="plainText",
                    order="relevance", maxResults=min(100, max_per_video)
                )
                response = request.execute()
                for item in response.get("items", []):
                    if len(video_comments) >= max_per_video: break
                    comm = item["snippet"]["topLevelComment"]["snippet"]
                    video_comments.append({
                        "comment_text": comm.get("textDisplay", ""),
                        "published_at": comm.get("publishedAt", ""),
                        "like_count": comm.get("likeCount", 0)
                    })
                _set_cached_value(comments_cache_name, cache_key, video_comments)
            except: pass
        
        title = video_meta.get(vid, {}).get("title", "")
        for c in video_comments:
            c_copy = c.copy()
            c_copy.update({"video_id": vid, "video_title": title})
            all_comments.append(c_copy)
            if len(all_comments) >= max_total_comments: break
            
        progress_bar.progress((i + 1) / len(video_ids), text=f"æŠ“å–ç•™è¨€... ({len(all_comments)}/{max_total_comments})")

    progress_bar.empty()
    return pd.DataFrame(all_comments)

# =========================
# 2. DeepSeek åˆ†æ (èªè¨€ç¯©é¸ + é—œéµè©æå–)
# =========================

async def analyze_comment_deepseek_v2(row, deepseek_client, semaphore):
    text = row["comment_text"]
    
    # Prompt ç­–ç•¥ï¼š
    # 1. åš´æ ¼æ‹’çµ•è‹±æ–‡ (Reject English)
    # 2. æ¥å—ç²µèªã€ç¹é«”ä¸­æ–‡ (Accept Cantonese/Traditional)
    # 3. æå–é—œéµè© (Extract Keywords)
    
    system_prompt = (
        "You are a Hong Kong movie analyst. Analyze the comment. "
        "Output JSON with keys: "
        "'sentiment' (Positive/Negative/Neutral), "
        "'keywords' (Extract 1-2 main keywords/short phrases in Traditional Chinese, e.g. 'åŠ‡æƒ…', 'å¤å¤©æ¨‚', 'æ‰“é¬¥'), "
        "'is_cantonese_target' (boolean). "
        "\n\n"
        "Rules for 'is_cantonese_target':\n"
        "1. **Strictly Set False** if the comment is in English (even if positive).\n"
        "2. Set True if the comment is in Cantonese (contains slang like å””, ä¿‚, å˜…, ä½¢) or Traditional Chinese.\n"
        "3. If the comment is ambiguous (short Chinese phrases), Set True (give benefit of doubt).\n"
        "4. Set False for spam or unrelated content."
    )

    async with semaphore:
        try:
            response = await deepseek_client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": text}
                ],
                response_format={"type": "json_object"},
                temperature=0.1,
            )
            return json.loads(response.choices[0].message.content)
        except:
            return {"sentiment": "Error", "keywords": "", "is_cantonese_target": False}

async def run_deepseek_analysis(df, deepseek_client):
    semaphore = asyncio.Semaphore(50)
    rows = df.to_dict('records')
    
    # ä½¿ç”¨ gather ç¢ºä¿é †åºä¸€è‡´
    tasks = [analyze_comment_deepseek_v2(row, deepseek_client, semaphore) for row in rows]
    
    progress_bar = st.progress(0, text="AI æ­£åœ¨é€²è¡Œæƒ…æ„Ÿåˆ†æèˆ‡é—œéµè©æå–...")
    
    # ç‚ºäº†é¡¯ç¤ºé€²åº¦ï¼Œæˆ‘å€‘ç¨å¾®åŒ…è£ä¸€ä¸‹
    results = []
    total = len(tasks)
    for i, task in enumerate(asyncio.as_completed(tasks)):
        await task # é€™è£¡åªæ˜¯ç‚ºäº†è§¸ç™¼é€²åº¦æ¢ï¼Œå¯¦éš›çµæœé †åºç”± gather æ±ºå®š
        progress_bar.progress((i + 1) / total)
    
    # é‡æ–°æŒ‰é †åºç²å–çµæœ
    results = await asyncio.gather(*[analyze_comment_deepseek_v2(row, deepseek_client, semaphore) for row in rows])
    progress_bar.empty()
    
    return pd.DataFrame(results)

# =========================
# 3. ä¸»æµç¨‹
# =========================

def main_process(movie_title, start_date, end_date, yt_api_key, deepseek_api_key, 
                 max_per_keyword, max_total_videos, max_per_video, max_total_comments):
    
    youtube = build("youtube", "v3", developerKey=yt_api_key)
    deepseek = openai.AsyncOpenAI(api_key=deepseek_api_key, base_url="https://api.deepseek.com/v1")
    
    # 1. æœå°‹ (åš´æ ¼æ¨™é¡Œ)
    keywords = generate_search_queries(movie_title)
    video_ids, video_meta = search_youtube_videos_strict(
        keywords, youtube, movie_title,
        max_per_keyword, max_total_videos, start_date, end_date
    )
    
    if not video_ids:
        return None, f"æ‰¾ä¸åˆ°æ¨™é¡ŒåŒ…å«ã€Œ{movie_title}ã€çš„å½±ç‰‡ã€‚"
    
    st.info(f"å·²é–å®š {len(video_ids)} éƒ¨æ¨™é¡Œå®Œå…¨åŒ¹é…çš„å½±ç‰‡ï¼Œé–‹å§‹æŠ“å–ç•™è¨€...")
    
    # 2. æŠ“å–ç•™è¨€
    df_comments = get_all_comments_cached(video_ids, youtube, max_per_video, max_total_comments, video_meta)
    
    if df_comments.empty:
        return None, "é€™äº›å½±ç‰‡ä¸‹æ²’æœ‰æ‰¾åˆ°ç•™è¨€ã€‚"
    
    # 3. AI åˆ†æ
    analysis_df = asyncio.run(run_deepseek_analysis(df_comments, deepseek))
    final_df = pd.concat([df_comments, analysis_df], axis=1)
    
    # 4. éæ¿¾éç²µèª/è‹±æ–‡
    original_len = len(final_df)
    final_df = final_df[final_df["is_cantonese_target"] == True].copy()
    filtered_len = len(final_df)
    
    st.success(f"åˆ†æå®Œæˆï¼å…±æŠ“å– {original_len} å‰‡ç•™è¨€ï¼ŒAI å‰”é™¤éç²µèª/ç´”è‹±æ–‡ç•™è¨€å¾Œï¼Œå‰©é¤˜ {filtered_len} å‰‡æœ‰æ•ˆæ•¸æ“šã€‚")
    
    final_df["published_at"] = pd.to_datetime(final_df["published_at"])
    return final_df, None

# =========================
# 4. Streamlit UI & Visualization
# =========================

st.set_page_config(page_title="YouTube ç²µèªå½±è©•åˆ†æ", layout="wide")
st.title("ğŸ¬ YouTube ç²µèªå½±è©•ç²¾æº–åˆ†æ")
st.markdown("### ç‰¹é»ï¼š100% æ¨™é¡ŒåŒ¹é… | å‰”é™¤è‹±æ–‡ | ç²µèªå„ªå…ˆ | æ·±åº¦å¯è¦–åŒ–")

with st.sidebar:
    st.header("è¨­å®š")
    yt_api_key = st.text_input("YouTube API Key", type='password')
    deepseek_api_key = st.text_input("DeepSeek API Key", type='password')
    st.divider()
    max_total_videos = st.number_input("æœ€å¤§å½±ç‰‡æ•¸", 10, 100, 30)
    max_total_comments = st.number_input("æœ€å¤§åˆ†æç•™è¨€æ•¸", 50, 1000, 300)

col1, col2, col3 = st.columns([2, 1, 1])
with col1:
    movie_title = st.text_input("é›»å½±å…¨å (å¿…é ˆå®Œå…¨åŒ¹é…)", value="éå¸¸ç›œ3") # æ¸¬è©¦ç”¨ä¾‹
with col2:
    start_date = st.date_input("é–‹å§‹", value=datetime.today() - timedelta(days=60))
with col3:
    end_date = st.date_input("çµæŸ", value=datetime.today())

if st.button("ğŸš€ é–‹å§‹åˆ†æ", type="primary"):
    if not all([movie_title, yt_api_key, deepseek_api_key]):
        st.error("è«‹å¡«å¯«æ‰€æœ‰æ¬„ä½")
    else:
        with st.spinner("AI æ­£åœ¨å…¨åŠ›é‹ç®—ä¸­..."):
            df_result, err = main_process(
                movie_title, str(start_date), str(end_date),
                yt_api_key, deepseek_api_key,
                20, max_total_videos, 50, max_total_comments
            )
            
        if err:
            st.error(err)
        else:
            # ==========================================
            # Visualization å„ªåŒ–éƒ¨åˆ†
            # ==========================================
            st.divider()
            
            # 1. é—œéµè©åˆ†æ (Horizontal Bar Chart)
            st.subheader("ğŸ”¥ ç†±é–€è©•è«–é—œéµè© (Top Keywords)")
            
            # è™•ç†é—œéµè©ï¼šDeepSeek å¯èƒ½è¿”å› list æˆ– stringï¼Œéœ€æ¨™æº–åŒ–
            all_keywords = []
            for item in df_result['keywords']:
                if isinstance(item, str):
                    # å‡è¨­é€—è™Ÿåˆ†éš”
                    words = [w.strip() for w in re.split(r'[ï¼Œ,ã€\s]+', item) if len(w.strip()) > 1]
                    all_keywords.extend(words)
                elif isinstance(item, list):
                    all_keywords.extend([str(w).strip() for w in item if len(str(w).strip()) > 1])
            
            if all_keywords:
                kw_counts = Counter(all_keywords).most_common(15)
                kw_df = pd.DataFrame(kw_counts, columns=['Keyword', 'Count'])
                kw_df = kw_df.sort_values(by='Count', ascending=True) # ç‚ºäº†è®“ Bar Chart æœ€é«˜åœ¨æœ€ä¸Šé¢
                
                fig_kw = px.bar(
                    kw_df, x='Count', y='Keyword', orientation='h',
                    title='Top 15 Most Mentioned Keywords',
                    text='Count',
                    color='Count',
                    color_continuous_scale='Blues'
                )
                fig_kw.update_layout(yaxis={'categoryorder':'total ascending'})
                st.plotly_chart(fig_kw, use_container_width=True)
            else:
                st.info("ç„¡æ³•æå–è¶³å¤ çš„é—œéµè©æ•¸æ“šã€‚")

            # 2. æƒ…æ„Ÿèµ°å‹¢åˆ†æ (Line + Stacked Bar)
            st.subheader("ğŸ“ˆ æƒ…æ„Ÿè¶¨å‹¢åˆ†æ (Sentiment Trend)")
            
            # æ•¸æ“šé è™•ç†
            df_result['date'] = df_result['published_at'].dt.date
            sentiments = ['Positive', 'Negative', 'Neutral']
            colors = {'Positive': '#28a745', 'Negative': '#dc3545', 'Neutral': '#ffc107'}
            
            # èšåˆæ•¸æ“š
            daily_sentiment = df_result.groupby(['date', 'sentiment']).size().reset_index(name='count')
            
            # ç¢ºä¿æ—¥æœŸé€£çºŒæ€§ (å¯é¸ï¼Œç‚ºäº†åœ–è¡¨å¥½çœ‹)
            if not daily_sentiment.empty:
                min_date = daily_sentiment['date'].min()
                max_date = daily_sentiment['date'].max()
                all_dates = pd.date_range(min_date, max_date).date
                
                # å»ºç«‹å®Œæ•´ç´¢å¼•
                full_idx = pd.MultiIndex.from_product([all_dates, sentiments], names=['date', 'sentiment'])
                daily_sentiment = daily_sentiment.set_index(['date', 'sentiment']).reindex(full_idx, fill_value=0).reset_index()

                # A. æŠ˜ç·šåœ– (Line Chart) - é¡¯ç¤ºèµ°å‹¢
                fig_line = px.line(
                    daily_sentiment, x='date', y='count', color='sentiment',
                    title='æ¯æ—¥æƒ…æ„Ÿè®ŠåŒ–è¶¨å‹¢ (Line Chart)',
                    color_discrete_map=colors,
                    markers=True
                )
                st.plotly_chart(fig_line, use_container_width=True)
                
                # B. å †ç–ŠæŸ±ç‹€åœ– (Stacked Bar Chart) - é¡¯ç¤ºç¸½é‡èˆ‡æ§‹æˆ
                fig_stack = px.bar(
                    daily_sentiment, x='date', y='count', color='sentiment',
                    title='æ¯æ—¥è©•è«–ç¸½é‡èˆ‡æƒ…æ„Ÿæ§‹æˆ (Stacked Bar)',
                    color_discrete_map=colors,
                    barmode='stack'
                )
                st.plotly_chart(fig_stack, use_container_width=True)
            else:
                st.warning("æ•¸æ“šä¸è¶³ä»¥ç”Ÿæˆè¶¨å‹¢åœ–ã€‚")

            # 3. æ•¸æ“šæ˜ç´°
            with st.expander("æŸ¥çœ‹è©³ç´°æ•¸æ“š (CSV ä¸‹è¼‰)"):
                st.dataframe(df_result[['sentiment', 'keywords', 'comment_text', 'video_title', 'published_at']])
                csv = df_result.to_csv(index=False).encode('utf-8-sig')
                st.download_button("ğŸ“¥ ä¸‹è¼‰å®Œæ•´ CSV", csv, "cantonese_analysis.csv", "text/csv")
