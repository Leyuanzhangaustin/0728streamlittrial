# app.py (Final Visualization Version - Revised for Language Filtering)
pip install opencc-python-reimplemented
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import plotly.express as px
from datetime import datetime, timedelta
import time
import asyncio
import openai
from opencc import OpenCC  # ### NEW: å¼•å…¥ OpenCC åº«

# ========== 1. YouTube Search (MODIFIED) ==========
# ç§»é™¤äº† 'relevanceLanguage' å’Œ 'regionCode' åƒæ•¸ï¼Œå› ç‚ºå®ƒå€‘ä¸æ˜¯åš´æ ¼çš„éæ¿¾å™¨ã€‚
# æˆ‘å€‘å°‡åœ¨æŠ“å–ç•™è¨€å¾Œé€²è¡Œæ›´å¯é çš„èªè¨€ç¯©é¸ã€‚
def search_youtube_videos(keywords, youtube_client, max_per_keyword, start_date, end_date):
    all_video_ids = set()
    for query in keywords:
        try:
            search_response = youtube_client.search().list(
                q=query,
                part='id,snippet',
                type='video',
                maxResults=max_per_keyword,
                publishedAfter=f"{start_date}T00:00:00Z",
                publishedBefore=f"{end_date}T23:59:59Z"
                # ### MODIFIED: ç§»é™¤ä»¥ä¸‹å…©è¡Œ ###
                # relevanceLanguage='zh-Hant',
                # regionCode='HK'
            ).execute()
            video_ids = [item['id']['videoId'] for item in search_response.get('items', [])]
            all_video_ids.update(video_ids)
            time.sleep(0.5)
        except Exception as e:
            st.warning(f"æœå°‹é—œéµå­— '{query}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            continue
    return list(all_video_ids)

# ========== 2. Batch Fetch Comments (No changes) ==========
def get_all_comments(video_ids, youtube_client, max_per_video):
    all_comments = []
    progress_bar = st.progress(0, text="æŠ“å– YouTube ç•™è¨€ä¸­...")
    total_videos = len(video_ids)
    for i, video_id in enumerate(video_ids):
        try:
            request = youtube_client.commentThreads().list(
                part='snippet', videoId=video_id, textFormat='plainText', maxResults=100
            )
            comments_fetched = 0
            while request and comments_fetched < max_per_video:
                response = request.execute()
                for item in response['items']:
                    if comments_fetched >= max_per_video:
                        break
                    comment = item['snippet']['topLevelComment']['snippet']
                    all_comments.append({
                        'video_id': video_id,
                        'comment_text': comment['textDisplay'],
                        'published_at': comment['publishedAt'],
                        'like_count': comment['likeCount']
                    })
                    comments_fetched += 1
                if comments_fetched >= max_per_video:
                    break
                request = youtube_client.commentThreads().list_next(request, response)
        except Exception:
            continue
        finally:
            # æ›´æ–°é€²åº¦æ¢
            progress_bar.progress((i + 1) / total_videos, text=f"æŠ“å– YouTube ç•™è¨€ä¸­... ({i+1}/{total_videos} éƒ¨å½±ç‰‡)")
    progress_bar.empty() # å®Œæˆå¾Œç§»é™¤é€²åº¦æ¢
    return pd.DataFrame(all_comments)

# ========== 3. DeepSeek AI Sentiment Analysis (No changes) ==========
async def analyze_comment_deepseek_async(comment_text, deepseek_client, semaphore, max_retries=3):
    import json
    if not isinstance(comment_text, str) or len(comment_text.strip()) < 5:
        return {"sentiment": "Invalid", "topic": "N/A", "summary": "Comment too short or invalid."}

    system_prompt = (
        "You are a professional Hong Kong market sentiment analyst. "
        "Analyze the following movie comment and strictly return the result in JSON format. "
        "The JSON object must contain three keys: "
        "1. 'sentiment': Must be either 'Positive', 'Negative', or 'Neutral'. "
        "2. 'topic': The core topic of the comment, e.g., 'Plot', 'Acting', 'Action Design', "
        "'Visuals', 'Pace', or 'Overall'. If unable to determine, use 'N/A'. "
        "3. 'summary': A concise one-sentence summary of the comment's main point. "
        "Ensure the output is only the JSON object and nothing else."
    )

    async with semaphore:
        for attempt in range(max_retries):
            try:
                response = await deepseek_client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": comment_text}
                    ],
                    response_format={"type": "json_object"},
                    temperature=0.1,
                )
                analysis_result = json.loads(response.choices[0].message.content)
                return analysis_result
            except Exception as e:
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                else:
                    return {"sentiment": "Error", "topic": "Error", "summary": f"API Error: {e}"}

# ========== 4. Main Process (MODIFIED) ==========
# å¢åŠ äº†ç¹é«”ä¸­æ–‡ç•™è¨€çš„ç¯©é¸é‚è¼¯
async def run_all_analyses(df, deepseek_client):
    semaphore = asyncio.Semaphore(50)
    tasks = [
        analyze_comment_deepseek_async(comment_text, deepseek_client, semaphore)
        for comment_text in df['comment_text']
    ]

    # ä½¿ç”¨ Streamlit çš„é€²åº¦æ¢ä¾†é¡¯ç¤º AI åˆ†æé€²åº¦
    progress_bar = st.progress(0, text="AI æƒ…æ„Ÿåˆ†æä¸­...")
    
    results = []
    for i, f in enumerate(asyncio.as_completed(tasks)):
        results.append(await f)
        progress_bar.progress((i + 1) / len(tasks), text=f"AI æƒ…æ„Ÿåˆ†æä¸­... ({i+1}/{len(tasks)})")
        
    progress_bar.empty()
    # ç”±æ–¼ as_completed ä¸ä¿è­‰é †åºï¼Œæˆ‘å€‘éœ€è¦ä¸€ç¨®æ–¹æ³•ä¾†é‡æ–°å°é½Šçµæœã€‚
    # é€™è£¡æˆ‘å€‘æš«æ™‚å‡è¨­é †åºå•é¡Œä¸å¤§ï¼Œæˆ–è€…åœ¨æ›´è¤‡é›œçš„å ´æ™¯ä¸­éœ€è¦å‚³éç´¢å¼•ã€‚
    # ç‚ºäº†ç°¡å–®èµ·è¦‹ï¼Œæˆ‘å€‘ç›´æ¥è¿”å›çµæœåˆ—è¡¨ã€‚
    return results

def movie_comment_analysis(
    movie_title, start_date, end_date,
    yt_api_key, deepseek_api_key,
    max_videos_per_keyword=30, max_comments_per_video=50, sample_size=None
):
    SEARCH_KEYWORDS = [
        f'"{movie_title}" é å‘Š', f'"{movie_title}" review', f'"{movie_title}" å½±è©•',
        f'"{movie_title}" åˆ†æ', f'"{movie_title}" å¥½å””å¥½ç‡', f'"{movie_title}" è¨è«–',
        f'"{movie_title}" reaction'
    ]

    from googleapiclient.discovery import build
    youtube_client = build('youtube', 'v3', developerKey=yt_api_key)

    deepseek_client = openai.AsyncOpenAI(
        api_key=deepseek_api_key,
        base_url="https://api.deepseek.com/v1"
    )

    video_ids = search_youtube_videos(
        SEARCH_KEYWORDS, youtube_client, max_videos_per_keyword, start_date, end_date
    )
    if not video_ids:
        return None, "æ‰¾ä¸åˆ°ç›¸é—œå½±ç‰‡ã€‚"

    df_comments = get_all_comments(video_ids, youtube_client, max_comments_per_video)
    if df_comments.empty:
        return None, "æ‰¾ä¸åˆ°ä»»ä½•ç•™è¨€ã€‚"

    # ### NEW: ç¹é«”ä¸­æ–‡ç•™è¨€ç¯©é¸é‚è¼¯ ###
    st.info(f"å·²æŠ“å– {len(df_comments)} å‰‡åŸå§‹ç•™è¨€ï¼Œç¾é–‹å§‹ç¯©é¸ç¹é«”ä¸­æ–‡å…§å®¹...")
    
    # åˆå§‹åŒ– OpenCCï¼Œ't2s' è¡¨ç¤ºå¾ç¹é«” (Traditional) åˆ°ç°¡é«” (Simplified)
    cc = OpenCC('t2s')
    
    def is_traditional_chinese(text):
        if not isinstance(text, str) or len(text.strip()) < 2:
            return False
        # åˆ¤æ–·é‚è¼¯ï¼šå¦‚æœå°‡æ–‡æœ¬å¾ç¹é«”è½‰æ›ç‚ºç°¡é«”å¾Œï¼Œèˆ‡åŸæ–‡ä¸åŒï¼Œ
        # å°±æ„å‘³è‘—åŸæ–‡ä¸­è‡³å°‘åŒ…å«ä¸€å€‹å¯è¢«è½‰æ›çš„ç¹é«”å­—ã€‚
        return cc.convert(text) != text

    mask_trad = df_comments['comment_text'].apply(is_traditional_chinese)
    df_comments_filtered = df_comments[mask_trad].reset_index(drop=True)
    
    st.info(f"ç¯©é¸å¾Œå‰©ä¸‹ {len(df_comments_filtered)} å‰‡ç¹é«”ä¸­æ–‡ç•™è¨€ã€‚")
    
    if df_comments_filtered.empty:
        return None, "åœ¨æŠ“å–çš„ç•™è¨€ä¸­æ‰¾ä¸åˆ°ç¹é«”ä¸­æ–‡å…§å®¹ã€‚"
    
    # å¾ŒçºŒæµç¨‹ä½¿ç”¨ç¯©é¸å¾Œçš„ DataFrame
    df_comments = df_comments_filtered
    # ### END OF NEW BLOCK ###

    df_comments['published_at'] = pd.to_datetime(df_comments['published_at'], utc=True)
    df_comments['published_at_hk'] = df_comments['published_at'].dt.tz_convert('Asia/Hong_Kong')

    start_dt = pd.to_datetime(start_date).tz_localize('Asia/Hong_Kong')
    end_dt = pd.to_datetime(end_date).tz_localize('Asia/Hong_Kong') + timedelta(days=1)
    mask_date = (df_comments['published_at_hk'] >= start_dt) & (df_comments['published_at_hk'] < end_dt)
    df_comments = df_comments.loc[mask_date].reset_index(drop=True)
    if df_comments.empty:
        return None, "åœ¨æŒ‡å®šæ—¥æœŸç¯„åœå…§æ²’æœ‰ç¬¦åˆèªè¨€æ¢ä»¶çš„ç•™è¨€ã€‚"

    if sample_size and sample_size > 0 and sample_size < len(df_comments):
        df_analyze = df_comments.sample(n=sample_size, random_state=42)
    else:
        df_analyze = df_comments

    st.info(f"æº–å‚™å° {len(df_analyze)} å‰‡ç•™è¨€é€²è¡Œé«˜é€Ÿä¸¦ç™¼åˆ†æ...")
    
    # é‹è¡Œç•°æ­¥åˆ†æ
    analysis_results_unordered = asyncio.run(run_all_analyses(df_analyze, deepseek_client))
    
    # å› ç‚º asyncio.as_completed çš„çµæœæ˜¯ç„¡åºçš„ï¼Œæˆ‘å€‘éœ€è¦å°‡å…¶èˆ‡åŸå§‹æ•¸æ“šå®‰å…¨åœ°åˆä½µã€‚
    # æœ€å®‰å…¨çš„æ–¹æ³•æ˜¯å°‡åˆ†æçµæœè½‰æ›ç‚º DataFrameï¼Œä¸¦ç¢ºä¿å…¶ç´¢å¼•èˆ‡ df_analyze ä¸€è‡´ã€‚
    analysis_df = pd.DataFrame(analysis_results_unordered)
    
    # æª¢æŸ¥è¡Œæ•¸æ˜¯å¦åŒ¹é…
    if len(df_analyze) != len(analysis_df):
        st.warning("AI åˆ†æè¿”å›çš„çµæœæ•¸é‡èˆ‡è«‹æ±‚æ•¸é‡ä¸åŒ¹é…ï¼Œæ•¸æ“šå¯èƒ½æœªå®Œå…¨å°é½Šã€‚")
        # æ¡å–ä¸€ç¨®ä¿å®ˆçš„åˆä½µç­–ç•¥
        min_len = min(len(df_analyze), len(analysis_df))
        final_df = pd.concat([df_analyze.head(min_len).reset_index(drop=True), analysis_df.head(min_len)], axis=1)
    else:
        final_df = pd.concat([df_analyze.reset_index(drop=True), analysis_df], axis=1)

    final_df['published_at'] = pd.to_datetime(final_df['published_at'])
    return final_df, None

# ========== 5. Streamlit UI (No changes in this part) ==========
st.set_page_config(page_title="YouTube é›»å½±è©•è«– AI åˆ†æ", layout="wide")
st.title("ğŸ¬ YouTube é›»å½±è©•è«– AI æƒ…æ„Ÿåˆ†æ")

with st.expander("ä½¿ç”¨èªªæ˜"):
    st.markdown("""
    1.  è¼¸å…¥é›»å½±çš„**ä¸­æ–‡å…¨å**ã€åˆ†ææ™‚é–“ç¯„åœåŠæ‰€éœ€çš„ API é‡‘é‘°ã€‚
    2.  è‡ªè¨‚æ¯å€‹é—œéµå­—æœå°‹çš„å½±ç‰‡æ•¸é‡ä¸Šé™ï¼ŒåŠæ¯éƒ¨å½±ç‰‡æŠ“å–çš„ç•™è¨€æ•¸é‡ä¸Šé™ã€‚
    3.  é»æ“Šã€Œé–‹å§‹åˆ†æã€ï¼Œç³»çµ±å°‡è‡ªå‹•æŠ“å– YouTube ç•™è¨€ï¼Œ**ç¯©é¸å‡ºç¹é«”ä¸­æ–‡å…§å®¹**ï¼Œä¸¦é€²è¡Œ AI é«˜é€Ÿæƒ…æ„Ÿåˆ†æã€‚
    4.  åˆ†æå®Œæˆå¾Œï¼Œä¸‹æ–¹æœƒé¡¯ç¤ºæ•¸æ“šåœ–è¡¨åŠè©³ç´°çµæœçš„ä¸‹è¼‰æŒ‰éˆ•ã€‚
    """)

movie_title = st.text_input("é›»å½±åç¨± (å»ºè­°ä½¿ç”¨é¦™æ¸¯é€šç”¨çš„ä¸­æ–‡å…¨å)", value="ä¹é¾åŸå¯¨ä¹‹åœåŸ")
col1, col2 = st.columns(2)
with col1:
    start_date = st.date_input("é–‹å§‹æ—¥æœŸ", value=datetime.today() - timedelta(days=30))
with col2:
    end_date = st.date_input("çµæŸæ—¥æœŸ", value=datetime.today())
yt_api_key = st.text_input("YouTube API Key", type='password')
deepseek_api_key = st.text_input("DeepSeek API Key", type='password')

st.subheader("é€²éšè¨­å®š")
max_videos = st.slider("æ¯å€‹é—œéµå­—çš„æœ€å¤§å½±ç‰‡æœå°‹æ•¸", 5, 50, 10, help="å¢åŠ æ­¤æ•¸å€¼æœƒæ‰¾åˆ°æ›´å¤šå½±ç‰‡ï¼Œä½†æœƒå¢åŠ  YouTube API çš„é…é¡æ¶ˆè€—ã€‚")
max_comments = st.slider("æ¯éƒ¨å½±ç‰‡çš„æœ€å¤§ç•™è¨€æŠ“å–æ•¸", 10, 200, 50, help="åˆ†æçš„ä¸»è¦ä¾†æºï¼Œæ•¸é‡è¶Šå¤šï¼Œåˆ†æçµæœè¶Šå…¨é¢ï¼Œä½† DeepSeek API æˆæœ¬è¶Šé«˜ã€‚")
sample_size = st.number_input("åˆ†æç•™è¨€æ•¸é‡ä¸Šé™ (0 ä»£è¡¨åˆ†æå…¨éƒ¨å·²æŠ“å–çš„ç•™è¨€)", 0, 5000, 500, help="è¨­å®šä¸€å€‹ä¸Šé™ä»¥æ§åˆ¶åˆ†ææ™‚é–“å’Œæˆæœ¬ã€‚ä¾‹å¦‚ï¼Œå³ä½¿æŠ“å–äº† 2000 å‰‡ç•™è¨€ï¼Œé€™è£¡è¨­ 500 å°±åªæœƒåˆ†æå…¶ä¸­çš„ 500 å‰‡ã€‚")

if st.button("ğŸš€ é–‹å§‹åˆ†æ"):
    if not all([movie_title, yt_api_key, deepseek_api_key]):
        st.warning("è«‹å¡«å¯«é›»å½±åç¨±å’Œå…©å€‹ API é‡‘é‘°ã€‚")
    else:
        # ä½¿ç”¨ä¸€å€‹å®¹å™¨ä¾†åŒ…è£¹æ•´å€‹åˆ†æéç¨‹ï¼Œæ–¹ä¾¿æœ€å¾Œçµ±ä¸€è™•ç†
        result_container = st.container()
        
        with st.spinner("AI é«˜é€Ÿåˆ†æä¸­... è«‹ç¨å€™..."):
            df_result, err = movie_comment_analysis(
                movie_title, str(start_date), str(end_date),
                yt_api_key, deepseek_api_key,
                max_videos, max_comments, sample_size
            )

        if err:
            st.error(err)
        else:
            st.success("åˆ†æå®Œæˆï¼")
            st.dataframe(df_result.head(20))
            
            st.header("ğŸ“Š å¯è¦–åŒ–åˆ†æçµæœ")

            # --- å…±ç”¨è¨­å®š ---
            sentiments_order = ['Positive', 'Negative', 'Neutral', 'Invalid', 'Error']
            colors_map = {
                'Positive': '#5cb85c', 'Negative': '#d9534f', 'Neutral': '#f0ad4e',
                'Invalid': '#cccccc', 'Error': '#888888'
            }

            # --- 1. æƒ…æ„Ÿåˆ†ä½ˆåœ“é¤…åœ– ---
            st.subheader("1. Sentiment Distribution (Pie)")
            sentiment_series = df_result['sentiment'].dropna().astype(str)
            sentiment_counts = sentiment_series.value_counts()
            ordered_labels = [label for label in sentiments_order if label in sentiment_counts.index]

            if not sentiment_counts.empty:
                fig1, ax1 = plt.subplots(figsize=(5, 4))
                ax1.pie(
                    sentiment_counts[ordered_labels],
                    labels=ordered_labels,
                    autopct='%.1f%%',
                    colors=[colors_map[label] for label in ordered_labels],
                    wedgeprops={'linewidth': 1.0, 'edgecolor': 'white'}
                )
                ax1.set_title('Overall Sentiment Distribution', fontsize=16)
                st.pyplot(fig1, use_container_width=False)
            else:
                st.info("No sentiment data available for pie chart.")

            # --- 2. æ¯æ—¥æƒ…æ„Ÿè¶¨å‹¢åœ– ---
            st.subheader("2. Daily Sentiment Trend")
            
            if 'published_at_hk' in df_result.columns:
                df_result['date'] = df_result['published_at_hk'].dt.date
            else:
                df_result['date'] = df_result['published_at'].dt.date
            
            daily = df_result.groupby(['date', 'sentiment']).size().unstack().fillna(0)
            daily = daily.reindex(columns=sentiments_order).dropna(axis=1, how='all')

            if not daily.empty:
                daily_long = daily.reset_index().melt(id_vars='date', var_name='sentiment', value_name='count')
                
                st.markdown("#### æ¯æ—¥æƒ…æ„Ÿè¶¨å‹¢ (æŠ˜ç·šåœ–)")
                fig_line = px.line(
                    daily_long, x='date', y='count', color='sentiment',
                    title='Daily Comment Volume Trend by Sentiment',
                    labels={'date': 'Date', 'count': 'Number of Comments', 'sentiment': 'Sentiment'},
                    color_discrete_map=colors_map,
                    category_orders={'sentiment': [col for col in sentiments_order if col in daily.columns]}
                )
                st.plotly_chart(fig_line, use_container_width=True)

                st.markdown("#### æ¯æ—¥ç•™è¨€ç¸½é‡åŠæƒ…æ„Ÿåˆ†ä½ˆ (å †ç–Šé•·æ¢åœ–)")
                fig_bar = px.bar(
                    daily_long, x='date', y='count', color='sentiment',
                    title='Daily Comment Volume by Sentiment (Stacked)',
                    labels={'date': 'Date', 'count': 'Number of Comments', 'sentiment': 'Sentiment'},
                    color_discrete_map=colors_map,
                    category_orders={'sentiment': [col for col in sentiments_order if col in daily.columns]},
                    barmode='stack'
                )
                st.plotly_chart(fig_bar, use_container_width=True)
            else:
                st.info("Not enough daily sentiment data to display the trend charts.")

            # --- 3. å„ä¸»é¡Œæƒ…æ„Ÿä½”æ¯” ---
            st.subheader("3. Sentiment Share by Topic")
            topic_sentiment = df_result.groupby(['topic', 'sentiment']).size().unstack().fillna(0)
            topic_sentiment = topic_sentiment.reindex(columns=sentiments_order).dropna(axis=1, how='all')
            
            if not topic_sentiment.empty:
                topic_sentiment = topic_sentiment[topic_sentiment.sum(axis=1) > 0]
                
                if not topic_sentiment.empty:
                    topic_sentiment_percent = topic_sentiment.div(topic_sentiment.sum(axis=1), axis=0).fillna(0) * 100

                    fig3, ax3 = plt.subplots(figsize=(10, 5))
                    topic_sentiment_percent.plot(
                        kind='bar', stacked=True, ax=ax3,
                        color=[colors_map[col] for col in topic_sentiment_percent.columns]
                    )
                    ax3.set_title('Sentiment Share by Topic', fontsize=16)
                    ax3.set_xlabel('Topic')
                    ax3.set_ylabel('Percentage (%)')
                    ax3.yaxis.set_major_formatter(plt.FuncFormatter('{:.0f}%'.format))
                    plt.xticks(rotation=45, ha='right')
                    ax3.legend(title='Sentiment')
                    plt.tight_layout()
                    st.pyplot(fig3, use_container_width=True)
                else:
                    st.info("No topic data with comments to display the chart.")
            else:
                st.info("Not enough topic sentiment data to display the stacked bar chart.")

            # --- 4. ä¸‹è¼‰åˆ†ææ˜ç´° ---
            st.subheader("4. ä¸‹è¼‰åˆ†ææ˜ç´°")
            csv = df_result.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                "ğŸ“¥ ä¸‹è¼‰å…¨éƒ¨åˆ†ææ˜ç´° (CSV)",
                csv,
                file_name=f"{movie_title}_analysis_details.csv",
                mime='text/csv'
            )
