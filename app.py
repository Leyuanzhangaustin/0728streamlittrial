# app.py (Final Visualization Version)

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import plotly.express as px
from datetime import datetime, timedelta
import time
import asyncio
import openai

# ========== 1. YouTube Search (No changes) ==========
def search_youtube_videos(keywords, youtube_client, max_per_keyword, start_date, end_date):
    all_video_ids = set()
    for query in keywords:
        try:
            search_response = youtube_client.search().list(
                q=query,
                part='id,snippet',
                type='video',
                maxResults=max_per_keyword,
                publishedAfter=f"{start_date}T00:00:00Z",
                publishedBefore=f"{end_date}T23:59:59Z",
                relevanceLanguage='zh-Hant',
                regionCode='HK'
            ).execute()
            video_ids = [item['id']['videoId'] for item in search_response.get('items', [])]
            all_video_ids.update(video_ids)
            time.sleep(0.5)
        except Exception as e:
            st.warning(f"æœå°‹é—œéµå­— '{query}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            continue
    return list(all_video_ids)

# ========== 2. Batch Fetch Comments (No changes) ==========
def get_all_comments(video_ids, youtube_client, max_per_video):
    all_comments = []
    for video_id in video_ids:
        try:
            request = youtube_client.commentThreads().list(
                part='snippet', videoId=video_id, textFormat='plainText', maxResults=100
            )
            comments_fetched = 0
            while request and comments_fetched < max_per_video:
                response = request.execute()
                for item in response['items']:
                    if comments_fetched >= max_per_video:
                        break
                    comment = item['snippet']['topLevelComment']['snippet']
                    all_comments.append({
                        'video_id': video_id,
                        'comment_text': comment['textDisplay'],
                        'published_at': comment['publishedAt'],
                        'like_count': comment['likeCount']
                    })
                    comments_fetched += 1
                if comments_fetched >= max_per_video:
                    break
                request = youtube_client.commentThreads().list_next(request, response)
        except Exception:
            continue
    return pd.DataFrame(all_comments)

# ========== 3. DeepSeek AI Sentiment Analysis (No changes) ==========
async def analyze_comment_deepseek_async(comment_text, deepseek_client, semaphore, max_retries=3):
    import json
    if not isinstance(comment_text, str) or len(comment_text.strip()) < 5:
        return {"sentiment": "Invalid", "topic": "N/A", "summary": "Comment too short or invalid."}

    system_prompt = (
        "You are a professional Hong Kong market sentiment analyst. "
        "Analyze the following movie comment and strictly return the result in JSON format. "
        "The JSON object must contain three keys: "
        "1. 'sentiment': Must be either 'Positive', 'Negative', or 'Neutral'. "
        "2. 'topic': The core topic of the comment, e.g., 'Plot', 'Acting', 'Action Design', "
        "'Visuals', 'Pace', or 'Overall'. If unable to determine, use 'N/A'. "
        "3. 'summary': A concise one-sentence summary of the comment's main point. "
        "Ensure the output is only the JSON object and nothing else."
    )

    async with semaphore:
        for attempt in range(max_retries):
            try:
                response = await deepseek_client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": comment_text}
                    ],
                    response_format={"type": "json_object"},
                    temperature=0.1,
                )
                analysis_result = json.loads(response.choices[0].message.content)
                return analysis_result
            except Exception as e:
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                else:
                    return {"sentiment": "Error", "topic": "Error", "summary": f"API Error: {e}"}

# ========== 4. Main Process (No changes) ==========
async def run_all_analyses(df, deepseek_client):
    semaphore = asyncio.Semaphore(50)
    tasks = [
        analyze_comment_deepseek_async(comment_text, deepseek_client, semaphore)
        for comment_text in df['comment_text']
    ]

    from tqdm.asyncio import tqdm_asyncio
    results = await tqdm_asyncio.gather(*tasks, desc="AI Sentiment Analysis (Concurrent)")
    return results

def movie_comment_analysis(
    movie_title, start_date, end_date,
    yt_api_key, deepseek_api_key,
    max_videos_per_keyword=30, max_comments_per_video=50, sample_size=None
):
    SEARCH_KEYWORDS = [
        f'"{movie_title}" é å‘Š', f'"{movie_title}" review', f'"{movie_title}" å½±è©•',
        f'"{movie_title}" åˆ†æ', f'"{movie_title}" å¥½å””å¥½ç‡', f'"{movie_title}" è¨è«–',
        f'"{movie_title}" reaction'
    ]

    from googleapiclient.discovery import build
    youtube_client = build('youtube', 'v3', developerKey=yt_api_key)

    deepseek_client = openai.AsyncOpenAI(
        api_key=deepseek_api_key,
        base_url="https://api.deepseek.com/v1"
    )

    video_ids = search_youtube_videos(
        SEARCH_KEYWORDS, youtube_client, max_videos_per_keyword, start_date, end_date
    )
    if not video_ids:
        return None, "æ‰¾ä¸åˆ°ç›¸é—œå½±ç‰‡ã€‚"

    df_comments = get_all_comments(video_ids, youtube_client, max_comments_per_video)
    if df_comments.empty:
        return None, "æ‰¾ä¸åˆ°ä»»ä½•ç•™è¨€ã€‚"

    df_comments['published_at'] = pd.to_datetime(df_comments['published_at'], utc=True)
    df_comments['published_at_hk'] = df_comments['published_at'].dt.tz_convert('Asia/Hong_Kong')

    start = pd.to_datetime(start_date).tz_localize('Asia/Hong_Kong')
    end = pd.to_datetime(end_date).tz_localize('Asia/Hong_Kong') + timedelta(days=1)
    mask = (df_comments['published_at_hk'] >= start) & (df_comments['published_at_hk'] <= end)
    df_comments = df_comments.loc[mask].reset_index(drop=True)
    if df_comments.empty:
        return None, "åœ¨æŒ‡å®šæ—¥æœŸç¯„åœå…§æ²’æœ‰ç•™è¨€ã€‚"

    if sample_size and sample_size > 0 and sample_size < len(df_comments):
        df_analyze = df_comments.sample(n=sample_size, random_state=42)
    else:
        df_analyze = df_comments

    st.info(f"æº–å‚™å° {len(df_analyze)} å‰‡ç•™è¨€é€²è¡Œé«˜é€Ÿä¸¦ç™¼åˆ†æ...")
    analysis_results = asyncio.run(run_all_analyses(df_analyze, deepseek_client))

    analysis_df = pd.json_normalize(analysis_results)
    final_df = pd.concat([df_analyze.reset_index(drop=True), analysis_df], axis=1)
    final_df['published_at'] = pd.to_datetime(final_df['published_at'])
    return final_df, None

# ========== 5. Streamlit UI (No changes in this part) ==========
st.set_page_config(page_title="YouTube é›»å½±è©•è«– AI åˆ†æ", layout="wide")
st.title("ğŸ¬ YouTube é›»å½±è©•è«– AI æƒ…æ„Ÿåˆ†æ")

with st.expander("ä½¿ç”¨èªªæ˜"):
    st.markdown("""
    1.  è¼¸å…¥é›»å½±çš„**ä¸­æ–‡å…¨å**ã€åˆ†ææ™‚é–“ç¯„åœåŠæ‰€éœ€çš„ API é‡‘é‘°ã€‚
    2.  è‡ªè¨‚æ¯å€‹é—œéµå­—æœå°‹çš„å½±ç‰‡æ•¸é‡ä¸Šé™ï¼ŒåŠæ¯éƒ¨å½±ç‰‡æŠ“å–çš„ç•™è¨€æ•¸é‡ä¸Šé™ã€‚
    3.  é»æ“Šã€Œé–‹å§‹åˆ†æã€ï¼Œç³»çµ±å°‡è‡ªå‹•æŠ“å– YouTube ç•™è¨€ä¸¦é€²è¡Œ AI é«˜é€Ÿæƒ…æ„Ÿåˆ†æã€‚
    4.  åˆ†æå®Œæˆå¾Œï¼Œä¸‹æ–¹æœƒé¡¯ç¤ºæ•¸æ“šåœ–è¡¨åŠè©³ç´°çµæœçš„ä¸‹è¼‰æŒ‰éˆ•ã€‚
    """)

movie_title = st.text_input("é›»å½±åç¨± (å»ºè­°ä½¿ç”¨é¦™æ¸¯é€šç”¨çš„ä¸­æ–‡å…¨å)", value="ä¹é¾åŸå¯¨ä¹‹åœåŸ")
col1, col2 = st.columns(2)
with col1:
    start_date = st.date_input("é–‹å§‹æ—¥æœŸ", value=datetime.today() - timedelta(days=30))
with col2:
    end_date = st.date_input("çµæŸæ—¥æœŸ", value=datetime.today())
yt_api_key = st.text_input("YouTube API Key", type='password')
deepseek_api_key = st.text_input("DeepSeek API Key", type='password')

st.subheader("é€²éšè¨­å®š")
max_videos = st.slider("æ¯å€‹é—œéµå­—çš„æœ€å¤§å½±ç‰‡æœå°‹æ•¸", 5, 50, 10, help="å¢åŠ æ­¤æ•¸å€¼æœƒæ‰¾åˆ°æ›´å¤šå½±ç‰‡ï¼Œä½†æœƒå¢åŠ  YouTube API çš„é…é¡æ¶ˆè€—ã€‚")
max_comments = st.slider("æ¯éƒ¨å½±ç‰‡çš„æœ€å¤§ç•™è¨€æŠ“å–æ•¸", 10, 200, 50, help="åˆ†æçš„ä¸»è¦ä¾†æºï¼Œæ•¸é‡è¶Šå¤šï¼Œåˆ†æçµæœè¶Šå…¨é¢ï¼Œä½† DeepSeek API æˆæœ¬è¶Šé«˜ã€‚")
sample_size = st.number_input("åˆ†æç•™è¨€æ•¸é‡ä¸Šé™ (0 ä»£è¡¨åˆ†æå…¨éƒ¨å·²æŠ“å–çš„ç•™è¨€)", 0, 5000, 500, help="è¨­å®šä¸€å€‹ä¸Šé™ä»¥æ§åˆ¶åˆ†ææ™‚é–“å’Œæˆæœ¬ã€‚ä¾‹å¦‚ï¼Œå³ä½¿æŠ“å–äº† 2000 å‰‡ç•™è¨€ï¼Œé€™è£¡è¨­ 500 å°±åªæœƒåˆ†æå…¶ä¸­çš„ 500 å‰‡ã€‚")

if st.button("ğŸš€ é–‹å§‹åˆ†æ"):
    if not all([movie_title, yt_api_key, deepseek_api_key]):
        st.warning("è«‹å¡«å¯«é›»å½±åç¨±å’Œå…©å€‹ API é‡‘é‘°ã€‚")
    else:
        with st.spinner("AI é«˜é€Ÿåˆ†æä¸­... (è™•ç† 500 å‰‡ç•™è¨€ç´„éœ€ 1-2 åˆ†é˜)"):
            df_result, err = movie_comment_analysis(
                movie_title, str(start_date), str(end_date),
                yt_api_key, deepseek_api_key,
                max_videos, max_comments, sample_size
            )

        if err:
            st.error(err)
        else:
            st.success("åˆ†æå®Œæˆï¼")
            st.dataframe(df_result.head(20))
            
            st.header("ğŸ“Š å¯è¦–åŒ–åˆ†æçµæœ")

            # --- å…±ç”¨è¨­å®š ---
            sentiments_order = ['Positive', 'Negative', 'Neutral', 'Invalid', 'Error']
            colors_map = {
                'Positive': '#5cb85c', 'Negative': '#d9534f', 'Neutral': '#f0ad4e',
                'Invalid': '#cccccc', 'Error': '#888888'
            }

            # --- 1. æƒ…æ„Ÿåˆ†ä½ˆåœ“é¤…åœ– (No changes) ---
            st.subheader("1. Sentiment Distribution (Pie)")
            sentiment_series = df_result['sentiment'].dropna().astype(str)
            sentiment_counts = sentiment_series.value_counts()
            ordered_labels = [label for label in sentiments_order if label in sentiment_counts.index]

            if not sentiment_counts.empty:
                fig1, ax1 = plt.subplots(figsize=(5, 4))
                ax1.pie(
                    sentiment_counts[ordered_labels],
                    labels=ordered_labels,
                    autopct='%.1f%%',
                    colors=[colors_map[label] for label in ordered_labels],
                    wedgeprops={'linewidth': 1.0, 'edgecolor': 'white'}
                )
                ax1.set_title('Overall Sentiment Distribution', fontsize=16)
                st.pyplot(fig1, use_container_width=False)
            else:
                st.info("No sentiment data available for pie chart.")

            # <<< MODIFIED BLOCK START: å¯¦ç¾å…©å¼µç¨ç«‹çš„æ¯æ—¥è¶¨å‹¢åœ– >>>
            
            st.subheader("2. Daily Sentiment Trend")
            
            # --- æ•¸æ“šæº–å‚™ (å…±ç”¨) ---
            if 'published_at_hk' in df_result.columns:
                df_result['date'] = df_result['published_at_hk'].dt.date
            else:
                df_result['date'] = df_result['published_at'].dt.date
            
            daily = df_result.groupby(['date', 'sentiment']).size().unstack().fillna(0)
            daily = daily.reindex(columns=sentiments_order).dropna(axis=1, how='all')

            if not daily.empty:
                # å°‡æ•¸æ“šå¾ "wide" è½‰ç‚º "long" æ ¼å¼ï¼Œæ–¹ä¾¿ Plotly ä½¿ç”¨
                daily_long = daily.reset_index().melt(id_vars='date', var_name='sentiment', value_name='count')
                
                # --- åœ–è¡¨ 2a: æ¯æ—¥æƒ…æ„Ÿè¶¨å‹¢ (æŠ˜ç·šåœ–) ---
                st.markdown("#### æ¯æ—¥æƒ…æ„Ÿè¶¨å‹¢ (æŠ˜ç·šåœ–)")
                st.markdown("æ­¤åœ–è¡¨å±•ç¤ºå„æƒ…æ„Ÿé¡åˆ¥æ¯æ—¥çš„ç•™è¨€æ•¸é‡è®ŠåŒ–ï¼Œé©åˆæ¯”è¼ƒä¸åŒæƒ…æ„Ÿçš„ç†±åº¦è¶¨å‹¢ã€‚")
                
                fig_line = px.line(
                    daily_long,
                    x='date',
                    y='count',
                    color='sentiment',
                    title='Daily Comment Volume Trend by Sentiment',
                    labels={'date': 'Date', 'count': 'Number of Comments', 'sentiment': 'Sentiment'},
                    color_discrete_map=colors_map,
                    category_orders={'sentiment': [col for col in sentiments_order if col in daily.columns]}
                )
                fig_line.update_layout(legend_title_text='Sentiment')
                st.plotly_chart(fig_line, use_container_width=True)

                # --- åœ–è¡¨ 2b: æ¯æ—¥ç•™è¨€ç¸½é‡ (å †ç–Šé•·æ¢åœ–) ---
                st.markdown("#### æ¯æ—¥ç•™è¨€ç¸½é‡åŠæƒ…æ„Ÿåˆ†ä½ˆ (å †ç–Šé•·æ¢åœ–)")
                st.markdown("æ­¤åœ–è¡¨å±•ç¤ºæ¯æ—¥çš„ç¸½ç•™è¨€é‡ï¼Œä¸¦ä»¥é¡è‰²å€åˆ†å…¶ä¸­å„ç¨®æƒ…æ„Ÿçš„ä½”æ¯”ã€‚")

                fig_bar = px.bar(
                    daily_long,
                    x='date',
                    y='count',
                    color='sentiment',
                    title='Daily Comment Volume by Sentiment (Stacked)',
                    labels={'date': 'Date', 'count': 'Number of Comments', 'sentiment': 'Sentiment'},
                    color_discrete_map=colors_map,
                    category_orders={'sentiment': [col for col in sentiments_order if col in daily.columns]}
                )
                fig_bar.update_layout(legend_title_text='Sentiment', barmode='stack')
                st.plotly_chart(fig_bar, use_container_width=True)

            else:
                st.info("Not enough daily sentiment data to display the trend charts.")

            # <<< MODIFIED BLOCK END >>>

            # --- 3. å„ä¸»é¡Œæƒ…æ„Ÿä½”æ¯” (No changes) ---
            st.subheader("3. Sentiment Share by Topic")
            topic_sentiment = df_result.groupby(['topic', 'sentiment']).size().unstack().fillna(0)
            topic_sentiment = topic_sentiment.reindex(columns=sentiments_order).dropna(axis=1, how='all')
            
            if not topic_sentiment.empty:
                topic_sentiment = topic_sentiment[topic_sentiment.sum(axis=1) > 0]
                
                if not topic_sentiment.empty:
                    topic_sentiment_percent = topic_sentiment.div(topic_sentiment.sum(axis=1), axis=0).fillna(0) * 100

                    fig3, ax3 = plt.subplots(figsize=(10, 5))
                    topic_sentiment_percent.plot(
                        kind='bar',
                        stacked=True,
                        ax=ax3,
                        color=[colors_map[col] for col in topic_sentiment_percent.columns]
                    )
                    ax3.set_title('Sentiment Share by Topic', fontsize=16)
                    ax3.set_xlabel('Topic')
                    ax3.set_ylabel('Percentage (%)')
                    ax3.yaxis.set_major_formatter(plt.FuncFormatter('{:.0f}%'.format))
                    plt.xticks(rotation=45, ha='right')
                    ax3.legend(title='Sentiment')
                    plt.tight_layout()
                    st.pyplot(fig3, use_container_width=True)
                else:
                    st.info("No topic data with comments to display the chart.")
            else:
                st.info("Not enough topic sentiment data to display the stacked bar chart.")

            # --- 4. ä¸‹è¼‰åˆ†ææ˜ç´° (No changes) ---
            st.subheader("4. ä¸‹è¼‰åˆ†ææ˜ç´°")
            csv = df_result.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                "ğŸ“¥ ä¸‹è¼‰å…¨éƒ¨åˆ†ææ˜ç´° (CSV)",
                csv,
                file_name=f"{movie_title}_analysis_details.csv",
                mime='text/csv'
            )
